% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}

% Packages 
\usepackage{graphicx} % Add assets folder to path for images. 
\graphicspath{{assets/}}
\usepackage{amsmath,amssymb,float,url} % Hide red underlines for URLs. 
\usepackage[hidelinks]{hyperref} % Stack two figures on top of each other.
\usepackage[normalem]{ulem} % Add strikethrough text.
\usepackage{orcidlink} % Render oorcid ID properly. 
\usepackage{xcolor} % Color needed for hyperref setup.

% Hide the ugly green box around the oorcid id.
\hypersetup{
    colorlinks,
    linkcolor={black},
    citecolor={black},
    urlcolor={black}
}

\begin{document}

\title{AJCAI 2022 Paper 3476: Response Letter}
\titlerunning{Reponse Letter}

\author{Jesse Wood\inst{1}\orcidlink{0000-0003-3756-2122} \and
  Bach Hoai Nguyen\inst{1}\orcidlink{0000-0002-6930-6863} \and
  Bing Xue\inst{1}\orcidlink{0000-0002-4865-8026} \and 
  Mengjie Zhang\inst{1}\orcidlink{0000-0003-4463-9538} \and 
  Daniel Killeen\inst{2}\orcidlink{0000-0002-4898-6724}
}

\authorrunning{J. Wood, B. Nguyen, et al.}

\institute{ Victoria University of Wellington, Te Herenga Waka,  Wellington, New Zealand\\
  \email{ \{jesse.wood, hoai.bach.nguyen, bing.xue, mengjie.zhang\}@ecs.vuw.ac.nz}\\
  \and 
  New Zealand Institute for Plant and Food Research Limited, Nelson, New Zealand\\
  \email{daniel.killeen@plantandfood.co.nz}\\
}

\maketitle

\section{Review I}

\textbf{SCORE:} SCORE: 1 (weak accept)
\\\\
The first reviewer had these general comments for the paper. 

\begin{quote}
  This paper automates processing of raw Gas Chromatography data to classify biomass that include fish species and fish body parts. First, this paper proposes a preprocessing imputation method to align timestamps in the training data. Then, it uses various machine learning methods to develop models for the classification tasks. Experimental results show that the SVM approach performs the best, however, visitation shows not all the features are needed. So this paper also uses four existing feature selection methods.

  This paper is well-written and easy to read. It explains the motivations behind the work and explain why a machine learning method is needed. The reason is the manual approach needs human experts and is expensive and time-consuming. This paper is mainly about using existing machine learning algorithms in an application. The experimental setup for both classification and then feature selection is good. The experimental results are good as well.
\end{quote}

\subsection{Literature Review}

\textbf{Comment:}

\begin{quote}
  What I found missing is a literature review on the problem or similar problem. If this is the first of such work, the authors could explicitly claim that. Otherwise, discuss existing methods and perhaps compare with those as well.
\end{quote}

\noindent\textbf{Fixes:}

\begin{enumerate}
  \item Introduction adds references to similar existing methods for classification of GC data in food science, and the limitations of this work. This motivates the interpretable Linear SVM proposed in this work.
  \begin{quote}
    [Introduction] \\ 
    Previous works using CNNs, \emph{\cite{bi2020gc,matyushin2020gas}}, showed high classification accuracy on gas-chromatograph data. 
    However, these black-box models do not produce interpretable models, making it difficult to verify/troubleshoot for fish processing in a factory setting.
  \end{quote}
  \item Section 3 on preprocessing explicitly adds references to similar existing methods.
  \begin{quote}
    [3 Preprocessing] \\ 
    The authors are aware that there are more complex methods for imputing missing values, \emph{\cite{tomasi2004correlation,zhang2008two}}, but they are not the focus of the paper and will be left for future work [...].
  \end{quote}
\end{enumerate}

\subsection{Imputation Contribution}

\textbf{Comment:}

\begin{quote}
  Although data imputation is an important part in the pipeline, however just 0 filling, while that makes sense, is not really a contribution. The time alignment in the data appears to be trivial as well,
\end{quote}

\noindent\textbf{Fixes:}

\begin{enumerate}
  \item The abstract removes reference to the filling 0 imputation. 
  \begin{quote}
    [Abstract] \\ 
    \sout{Firstly, the paper proposes a preprocessing imputation method for aligning timestamps in Gas Chromatography data.}
  \end{quote}
  \item The introduction lightens the impact of the imputation contribution, 
  \begin{quote}
    [Introduction] \\ 
    The paper \sout{proposes} \emph{finds} an effective method to detect and fill the missing packets/features which \sout{significantly} improves the classification performance over using the raw data.
  \end{quote}
  \item Section 3 on preprocessing explicitly adds references to more advanced techniques to highlight the trivial nature of filling 0 for aligning GC data.
  \begin{quote}
    [3 Preprocessing] \\ 
    The authors are aware that there are more complex methods for imputing missing values, \emph{\cite{tomasi2004correlation,zhang2008two}}, but they are not the focus of the paper and will be left for future work [...].
  \end{quote}
\end{enumerate}

\subsection{Figure Formatting}

\textbf{Comment:}

\begin{quote}
  This figures need to be larger and visible.
\end{quote} 

\noindent\textbf{Fixes:}

\begin{enumerate}
  \item Figure 1, the gas chromatograph, was enlarged to 0.8 of the linewidth. 
  \item Figure 2, the hyperplane coefficients, were changed to be two figures stacked vertically, both 0.8 of the linewidth. 
  \item Figures 3,4, the feature selection results, cannot be made any larger without exceeding the page limit (unless there is some black magic I am unaware of).
\end{enumerate}

\section{Reivew II}

\textbf{SCORE:} -1 (weak reject)
\\\\
The second reviewer gave the paper these general comments. 

\begin{quote}
  This paper provides an interesting application of ML for fish classification using fatty acid Chromatographic data. It proposes a pre-processing imputation method for aligning timestamps in Gas Chromatography data, it demonstrates SVM could classify compositionally diverse marine biomass based on raw chromatographic fatty acid data, which can highlight important features for classification, and it also demonstrates that feature selection reduces dimensionality and improves classification performance by accelerating the classification system by four times.
\end{quote}

\subsection{Preprocessing Experimental Results}

\textbf{Comment:}

\begin{quote}
  However, the motivation and research problem is not clear; 
  for example, you need to demonstrate your pre-processing works using experimental results. 
\end{quote}

\noindent\textbf{Fixes:}

\begin{enumerate}
  \item The model intends to be deployed in a factory setting for fish processing. Therefore an interpretable and accurate model is required. The paper had a section added to the introduction to clarify the constraints of this application. This concretizes the real-world applicability and scope of the research problem and motivates the need for an interpretable model. 
  \begin{quote}
    [Introduction] \\ 
    However, fatty acid data must be carefully processed and interpreted by domain experts (i.e. chemists), which is very expensive and time-consuming.
    \emph{Previous works using CNNs, \cite{bi2020gc,matyushin2020gas}, showed high classification accuracy on gas-chromatograph data. 
    However, these black-box models do not produce interpretable models, making it difficult to verify/troubleshoot these models for fish processing in a factory setting.}
  \end{quote}
  \item \textbf{TODO:} KNN classification results for imputation methods. 
\end{enumerate}

\subsection{Contributions}

\textbf{Comment:}

\begin{quote}
  Also, no innovative techniques have been developed, so the contribution is not enough; you should provide a new method to compare with the methods in Tables 3 and 4.
\end{quote}

\noindent\textbf{Fixes:}

\begin{enumerate}
  \item Future work will likely extend this conference paper into a journal paper, where the authors will propose new methods for imputation, classification and feature selection. The work would compare techniques from evolutionary computation to these existing results as suggested by the reviewer.
  \item The paper does not propose a new method, but rather uses existing methods to solve a new problem. The novelty of the paper is its application. The model intends to be deployed in a factory setting for fish processing. Therefore an interpretable and accurate model is required. The paper had a passage added to the introduction to clarify the constraints of this application: 
  \begin{quote}
    [Introduction] \\ 
    However, fatty acid data must be carefully processed and interpreted by domain experts (i.e. chemists), which is very expensive and time-consuming.
    \emph{Previous works using CNNs, \cite{bi2020gc,matyushin2020gas}, showed high classification accuracy on gas-chromatograph data. 
    However, these black-box models do not produce interpretable models, making it difficult to verify/troubleshoot for fish processing in a factory setting.}
  \end{quote}
\end{enumerate}

\section{Review III}

\textbf{SCORE:} SCORE: 0 (borderline paper)
\\\\
\noindent\textbf{Comment:}

\begin{quote}
  Page 6: "The hyperplane is represented by a weight vector in which each weight is associated with a feature. The larger the weight, the more important the corresponding feature. After an SVM classification algorithm is trained on the training set, an SVM classifier containing a learned weight vector is obtained. This section analyses the learned weight vector to examine the contribution of each packet/feature."

  SVMs implement kernel methods to transform original data items into a high dimensional feature space where the input samples become linearly or mostly linearly separable. SVMs can learn the hyperplane in the feature space, which separates the training data with the widest margin.

  The hyperplane is constructed in the feature space that is nonlinearly related to input space. The weight vector representing the hyperplane in the feature space wouldn't match the features of original data items in input space neither in dimensions nor in physical significance. The hyperplane, when being mapped to input space, becomes irregular contours outlined by support vectors. The important features (with larger weight) in feature space can hardly have their corresponding features in input space.

  How to use weight vector of the hyperplane in feature space to examine the contribution of each packet/feature in input space?
\end{quote}

\noindent\textbf{Response:}
\\\\
(Usually) a conventional SVM uses a non-linear kernel, and the sklearn library defaults to the radial basis function (RBF) \cite{sklearn2021feature}. 
However, the paper states that experiments use a linear SVM model,

\begin{enumerate}
  \item 
  \begin{quote}
    [Introduction]
    Experiments find that kernel-based classifiers, particularly \emph{linear SVM}, achieve high classification accuracy on the fish data [...]
  \end{quote}
  \item 
  \begin{quote}
    [4.1 Experiment Settings]
    These experiments compare five well-known classifications: K Nearest Neighbours (KNN where K is set to 3), Naive Bayes (NB), Random Forest (RF), Decision Trees (DT), and \emph{Linear Support Vector Machines} (SVM) [...]
  \end{quote}
  \item 
  \begin{quote}
    [5.4 Feature Selection Methods]
    In this work, a \emph{linear SVM} is used as the wrapped classification algorithm since it achieves good classification performance [...]
  \end{quote}
  \item \begin{quote}
    [5.3 Experimental Settings]
    For each method, the balanced classification accuracy is measured with a \emph{linear SVM} classification algorithm \cite{sklearn2021feature} [...]
  \end{quote}
  \item \begin{quote}
    [Conclusion]
    Among the considered classification algorithms, \emph{linear SVM} achieves the best classification performance since it is suited to high-dimensional problems [...]
  \end{quote}
\end{enumerate}

A linear kernel performs a linear transformation, which preserves the distance between points, mapping each instance to a 4800-dimensional vector in the feature space, then creates a hyperplane to linearly separate the classes in that feature space. Therefore the hyperplane coefficients, given in section 4.3, correspond to the original features.
\\\\
\noindent\textbf{Fixes:}

\begin{enumerate}
  \item The paper now clearly states in section 4.3 that an SVM with linear kernel was used.
    \begin{quote} 
    [4.3 Interpret SVM models] \\
    This subsection analyzes the \emph{Linear} SVM model\sout{s} built to classify the fish species. 
  \end{quote}
\end{enumerate}

\subsection{Time Complexity}

\noindent\textbf{Comment:}

\begin{quote}
  Page 9: Meanwhile, PSO can remove 75\% features, which means the classification system can be four times faster given the number of required packets/features is reduced by four times.

  Considering the dimension of features of input data, does the classification speed linearly vary with the reduction amount of features?
\end{quote}

\noindent\textbf{Response:}
\\\\
Again, the reviewer has assumed the SVM used a non-linear kernel, in which case the complexity for non-linear SVM is expected to be $O(|\mathbb{X}|^2)$, where $|\mathbb{X}|$ is the number of training instances \cite{chang2011libsvm}. On page 9 the paper claims a linear speed up inversely proportional to the feature reduction, 

\begin{quote}
  Meanwhile, PSO can remove 75\% features, which means the classification system can be four times faster given the number of required packets/features is reduced by four times. 
\end{quote}

The paper uses SVM with a linear kernel, with time complexity $O(|\mathbb{X}| \times f_n)$, where $f_n$ is the number of features. Therefore a 75\% feature reduction would speed up the classification by a factor of 4. 
\\\\
\noindent\textbf{Fixes:}

\begin{enumerate}
  \item (\emph{Potential?}) Include complexity analysis of the Linear SVM in the introduction to the feature selection. 
  \begin{quote}
    [5.4 Feature Selection Performance on Fish Species Classification]
    \emph{The linear SVM has time complexity $O(|\mathbb{X}| \times f_n)$, where $f_n$ is the number of features.}
    Meanwhile, PSO can remove 75\% features, which means the classification system can be four times faster given the number of required packets/features is reduced by four times. 
  \end{quote}
\end{enumerate}

\bibliographystyle{splncs04}
% \bibliography{mybibliography}
\bibliography{refs}

\end{document}
