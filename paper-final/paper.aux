\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{galanakis2019saving}
\citation{eder1995gas}
\citation{restek2018high}
\citation{bi2020gc,matyushin2020gas}
\citation{bi2020gc}
\citation{matyushin2020gas}
\@writefile{toc}{\contentsline {title}{Machine Learning for Fish Oil Anaylsis}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{No Author Given}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1.1}\protected@file@percent }
\citation{robinson2020genetic}
\citation{eder1995gas,restek2018high,khan2013gas}
\citation{restek2018high}
\citation{restek2018high}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related Work}{2}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Goals and Objectives}{2}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Gas Chromatorgraphy}{2}{subsection.1.2.1}\protected@file@percent }
\citation{khan2013gas}
\citation{restek2018high}
\citation{eder1995gas,restek2018high}
\citation{musk2020battery}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Gas chromatograph: the artifact of the GC method \cite  {restek2018high}. The detection is used to visualize intensity (y) and time (x) on a chromotrogaph.\relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gas-chromatography}{{1}{3}{Gas chromatograph: the artifact of the GC method \cite {restek2018high}. The detection is used to visualize intensity (y) and time (x) on a chromotrogaph.\relax }{figure.caption.2}{}}
\citation{cortes1995support}
\citation{sklearn2021feature}
\citation{sklearn2021feature}
\citation{scholkopf2000new}
\citation{aizerman1964theoretical}
\citation{boser1992training}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Classification Algorithms}{4}{subsection.1.2.2}\protected@file@percent }
\newlabel{sec:background-svm}{{2.2}{4}{Support Vector Machines}{section*.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Support Vector Machines}{4}{section*.3}\protected@file@percent }
\newlabel{sec:background-svm-model}{{2.2}{4}{SVM Model}{section*.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{SVM Model}{4}{section*.4}\protected@file@percent }
\newlabel{sec:background-svm-kernel}{{2.2}{4}{SVM Kernel}{section*.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{SVM Kernel}{4}{section*.5}\protected@file@percent }
\newlabel{sec:background-svm-hyperplane}{{2.2}{4}{SVM Hyperplane Coefficients}{section*.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{SVM Hyperplane Coefficients}{4}{section*.7}\protected@file@percent }
\newlabel{eq:hyperplane}{{1}{4}{SVM Hyperplane Coefficients}{equation.1.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  SVM kernels shapes are shown. Specifically, linear, polynomial, radial basis function (rbf) and sigmoidal kernal are shown.\relax }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{fig:kernels}{{2}{5}{SVM kernels shapes are shown. Specifically, linear, polynomial, radial basis function (rbf) and sigmoidal kernal are shown.\relax }{figure.caption.6}{}}
\newlabel{fig:fish-hyperplane-coeffcients}{{3a}{5}{Fish Species: Hyperplane Coefficients\relax }{figure.caption.8}{}}
\newlabel{sub@fig:fish-hyperplane-coeffcients}{{a}{5}{Fish Species: Hyperplane Coefficients\relax }{figure.caption.8}{}}
\newlabel{fig:part-hyperplane-coeffcients}{{3b}{5}{Fish Part: Hyperplane Coefficients\relax }{figure.caption.8}{}}
\newlabel{sub@fig:part-hyperplane-coeffcients}{{b}{5}{Fish Part: Hyperplane Coefficients\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Two numerical solutions}}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig:hyperplane-coefficients}{{3}{5}{Two numerical solutions}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Visualisation}{5}{subsection.1.2.3}\protected@file@percent }
\newlabel{sec:background-visualisation}{{2.3}{5}{Visualisation}{subsection.1.2.3}{}}
\citation{koppen2000curse}
\citation{fix1989discriminatory}
\citation{ho1995random}
\citation{hand2001idiot}
\citation{loh2011classification}
\citation{cortes1995support}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Feature Selection}{6}{subsection.1.2.4}\protected@file@percent }
\newlabel{sec:background-feature-selection}{{2.4}{6}{Feature Selection}{subsection.1.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Data processing}{6}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Classification}{6}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Classification Algorithms}{6}{subsection.1.4.1}\protected@file@percent }
\citation{sklearn2021feature}
\citation{cortes1995support}
\citation{scholkopf2000new}
\citation{sklearn2021feature}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Accuracy for different classification techniques. Accuracy is given as the stratified k-fold cross validation over 30 independent runs. We compare K-nearest neighbours (KNN), random forest (RF), decision tree (DT), naive bayes (NB) and support vector machines (SVM).\relax }}{7}{table.caption.9}\protected@file@percent }
\newlabel{t:classification}{{1}{7}{Accuracy for different classification techniques. Accuracy is given as the stratified k-fold cross validation over 30 independent runs. We compare K-nearest neighbours (KNN), random forest (RF), decision tree (DT), naive bayes (NB) and support vector machines (SVM).\relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}SVM Model}{7}{subsection.1.4.2}\protected@file@percent }
\newlabel{sec:results-classification-svm}{{4.2}{7}{SVM Model}{subsection.1.4.2}{}}
\citation{aizerman1964theoretical}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Accuracy for different SVM models. Accuracy is given as the stratified k-fold cross validation over 30 independent runs. We compare Support-Vector Classification (SVC), Nu-Support Vector Classification (Nu-SVC) and Linear Support-Vector Classification (LSVC).\relax }}{8}{table.caption.10}\protected@file@percent }
\newlabel{t:svm-models}{{2}{8}{Accuracy for different SVM models. Accuracy is given as the stratified k-fold cross validation over 30 independent runs. We compare Support-Vector Classification (SVC), Nu-Support Vector Classification (Nu-SVC) and Linear Support-Vector Classification (LSVC).\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}SVM Kernel}{8}{subsection.1.4.3}\protected@file@percent }
\newlabel{sec:results-classification-svm-kernel}{{4.3}{8}{SVM Kernel}{subsection.1.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Accuracy for different SVM kernals. Accuracy is given as the stratified k-fold cross validation over 30 independent runs. We compare polynomial (poly), radial basis function (rbf), sigmoidal (sigmoid) and linear.\relax }}{8}{table.caption.11}\protected@file@percent }
\newlabel{t:svm-kernels}{{3}{8}{Accuracy for different SVM kernals. Accuracy is given as the stratified k-fold cross validation over 30 independent runs. We compare polynomial (poly), radial basis function (rbf), sigmoidal (sigmoid) and linear.\relax }{table.caption.11}{}}
\citation{scholkopf2000new}
\citation{sklearn2021feature}
\citation{robnik2003theoretical}
\citation{chappers2015skfeature}
\citation{liu1995chi2}
\citation{ding2005minimum}
\citation{robnik2003theoretical}
\citation{kennedy1995particle,kennedy1997discrete}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Discussion}{9}{subsection.1.4.4}\protected@file@percent }
\newlabel{sec:results-classification-discussion}{{4.4}{9}{Discussion}{subsection.1.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Feature Selection}{9}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Classification Accuracy $k = 500$}{9}{subsection.1.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces  Accuracy for different feature selection methods. Accuracy is given as the stratified k-fold cross validation over 30 independent runs. We compare chi$^2$ (chi), maximum relevance - minimum redundancy (MRMR), reliefF, particle swarm optimisation (PSO).\relax }}{10}{table.caption.12}\protected@file@percent }
\newlabel{t:feature-selection}{{4}{10}{Accuracy for different feature selection methods. Accuracy is given as the stratified k-fold cross validation over 30 independent runs. We compare chi$^2$ (chi), maximum relevance - minimum redundancy (MRMR), reliefF, particle swarm optimisation (PSO).\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Classification Accuracy (all $k$)}{10}{subsection.1.5.2}\protected@file@percent }
\newlabel{fig:accuracy-features-fish-train}{{4a}{11}{Species: Training set\relax }{figure.caption.13}{}}
\newlabel{sub@fig:accuracy-features-fish-train}{{a}{11}{Species: Training set\relax }{figure.caption.13}{}}
\newlabel{fig:accuracy-features-fish-test}{{4b}{11}{Species: Test set\relax }{figure.caption.13}{}}
\newlabel{sub@fig:accuracy-features-fish-test}{{b}{11}{Species: Test set\relax }{figure.caption.13}{}}
\newlabel{fig:accuracy-features-part-train}{{4c}{11}{Part: Training set\relax }{figure.caption.13}{}}
\newlabel{sub@fig:accuracy-features-part-train}{{c}{11}{Part: Training set\relax }{figure.caption.13}{}}
\newlabel{fig:accuracy-features-part-test}{{4d}{11}{Part: Test set\relax }{figure.caption.13}{}}
\newlabel{sub@fig:accuracy-features-part-test}{{d}{11}{Part: Test set\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Two numerical solutions}}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig:animals}{{4}{11}{Two numerical solutions}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Disucssion}{11}{subsection.1.5.3}\protected@file@percent }
\newlabel{sec:results-feature-selection-discussion}{{5.3}{11}{Disucssion}{subsection.1.5.3}{}}
\citation{clarke2013profiles}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions and Future Work}{12}{section.1.6}\protected@file@percent }
\bibstyle{splncs04}
\bibdata{refs}
\bibcite{khan2013gas}{1}
\bibcite{aizerman1964theoretical}{2}
\bibcite{bi2020gc}{3}
\bibcite{boser1992training}{4}
\bibcite{chappers2015skfeature}{5}
\bibcite{clarke2013profiles}{6}
\bibcite{cortes1995support}{7}
\bibcite{robinson2020genetic}{8}
\bibcite{ding2005minimum}{9}
\bibcite{eder1995gas}{10}
\bibcite{fix1989discriminatory}{11}
\bibcite{galanakis2019saving}{12}
\bibcite{hand2001idiot}{13}
\bibcite{ho1995random}{14}
\bibcite{kennedy1995particle}{15}
\bibcite{kennedy1997discrete}{16}
\bibcite{koppen2000curse}{17}
\bibcite{liu1995chi2}{18}
\bibcite{loh2011classification}{19}
\bibcite{matyushin2020gas}{20}
\bibcite{musk2020battery}{21}
\bibcite{restek2018high}{22}
\bibcite{robnik2003theoretical}{23}
\bibcite{scholkopf2000new}{24}
\bibcite{sklearn2021feature}{25}
\gdef \@abspage@last{14}
