@article{aizerman1964theoretical,
  title   = {Theoretical foundations of the potential function method in pattern recognition learning},
  author  = {Aizerman, Mark A},
  journal = {Automation and remote control},
  volume  = {25},
  pages   = {821--837},
  year    = {1964},
  url     = {https://ci.nii.ac.jp/naid/10021200712/}
}

@article{bi2020gc,
  title     = {GC-MS Fingerprints Profiling Using Machine Learning Models for Food Flavor Prediction},
  author    = {Bi, Kexin and Zhang, Dong and Qiu, Tong and Huang, Yizhen},
  journal   = {Processes},
  volume    = {8},
  number    = {1},
  pages     = {23},
  year      = {2020},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url       = {https://www.mdpi.com/2227-9717/8/1/23},
  abstract  = {Food flavor quality evaluation is attracting continuous attention, but a suitable evaluation system is severely lacking. Gas chromatography-mass spectrometry/olfactometry (GC-MS/O) is widely used to solve the food flavor evaluation problem, but the olfactometry evaluation is unfeasible to be carried out in large batches and is unreliable due to potential issue of an operator or systematic laboratory effect. Thus, a novel fingerprint modeling and profiling process was proposed based on several machine learning models including convolutional neural network (CNN). The fingerprint template was created by the data analysis of existing GC-MS spectrum dataset. Then the fingerprint image generation program was applied for structuring the complex instrumental data. Food olfactometry result was obtained by a machine learning method based on CNN using fingerprint image as the input. The case study on peanut oil samples demonstrated the model accuracy of around 93%. By structure optimization and further dataset expansion, the whole process has the potential to be utilized by sensory laboratories for aroma analysis instead of humans.}
}


@inproceedings{boser1992training,
  title     = {A training algorithm for optimal margin classifiers},
  author    = {Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
  booktitle = {Proceedings of the fifth annual workshop on Computational learning theory},
  pages     = {144--152},
  year      = {1992},
  url       = {https://dl.acm.org/doi/abs/10.1145/130385.130401},
  abstract  = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning}
}

@misc{chappers2015skfeature,
  title        = {charliec443/scikit-feature},
  author       = {Charlie Chappers},
  howpublished = {\url{https://github.com/charliec443/scikit-feature}},
  journal      = {GitHub}
}

@book{clarke2013profiles,
  title     = {Profiles of the Future},
  author    = {Clarke, Arthur C},
  year      = {1962},
  publisher = {Hachette UK}
}

@article{cortes1995support,
  title     = {Support-vector networks},
  author    = {Cortes, Corinna and Vapnik, Vladimir},
  journal   = {Machine learning},
  volume    = {20},
  number    = {3},
  pages     = {273--297},
  year      = {1995},
  publisher = {Springer},
  url       = {https://link.springer.com/article/10.1007/BF00994018},
  abstract  = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.
               
               High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.}
}

@article{ding2005minimum,
  title     = {Minimum redundancy feature selection from microarray gene expression data},
  author    = {Ding, Chris and Peng, Hanchuan},
  journal   = {Journal of bioinformatics and computational biology},
  volume    = {3},
  number    = {02},
  pages     = {185--205},
  year      = {2005},
  publisher = {World Scientific},
  url       = {https://www.worldscientific.com/doi/abs/10.1142/s0219720005001004},
  abstract  = {How to selecting a small subset out of the thousands of genes in microarray data is important for accurate classification of phenotypes. Widely used methods typically rank genes according to their differential expressions among phenotypes and pick the top-ranked genes. We observe that feature sets so obtained have certain redundancy and study methods to minimize it. We propose a minimum redundancy — maximum relevance (MRMR) feature selection framework. Genes selected via MRMR provide a more balanced coverage of the space and capture broader characteristics of phenotypes. They lead to significantly improved class predictions in extensive experiments on 6 gene expression data sets: NCI, Lymphoma, Lung, Child Leukemia, Leukemia, and Colon. Improvements are observed consistently among 4 classification methods: Naïve Bayes, Linear discriminant analysis, Logistic regression, and Support vector machines.
               
               Supplimentary: The top 60 MRMR genes for each of the datasets are listed in . More information related to MRMR methods can be found at.}
}

@article{eder1995gas,
  title     = {Gas chromatographic analysis of fatty acid methyl esters},
  author    = {Eder, K},
  journal   = {Journal of Chromatography B: Biomedical Sciences and Applications},
  volume    = {671},
  number    = {1-2},
  pages     = {113--131},
  year      = {1995},
  publisher = {Elsevier},
  url       = {https://www.sciencedirect.com/science/article/pii/0378434795001426},
  abtract   = {The full process of fatty acid methyl ester (FAME) analysis consists of esterification of lipids, and of injection, separation, identification and quantitation of the FAMEs. In order for the required accuracy and precision to be attained, each of these steps has to be optimized.
               
               Esterification of lipids can be carried out with several reagents based on acid-catalysed or base-catalysed reactions. The advantages and disadvantages of these reagents are discussed. The most critical step in the gas chromatographic analysis of FAMEs is sample introduction. The classical split injection technique, which is the most widely used technique in the analysis of FAMEs, has the potential disadvantage of boiling-point-dependent sample discrimination. Cold injection of the sample, either on-column or by programmed-temperature vaporization, does not present this problem and should therefore be preferred.
               
               Modern, commercially available fused-silica capillary columns offer excellent separation of FAMEs from biological samples. Very polar stationary phases give excellent separation of all FAMEs but have relatively low thermal stability, resulting in long retention times. Non-polar phases have a much greater thermal stability but inferior selectivity. For many analyses, phases of intermediate polarity, which combine the advantages of a relatively high resolution capability with relatively high thermal stability, are the most suitable.
               
               FAMEs can be identified by comparison of their retention times with those of individual purified standards or secondary standards based on lipids that have been well characterized in literature. Relative retention times and equivalent chain-length values also provide useful information. FAMEs can be quantitated by peak areas via calibration factors, and absolute concentrations can be determined by adding an internal standard.
               
               Among numerous applications in biomedical research, the analysis of fatty acids from body tissues may contribute to the understanding of the link between the dietary intake of fatty acids and the diseases with which these acids are associated.}
}

@article{fix1989discriminatory,
  title     = {Discriminatory analysis. Nonparametric discrimination: Consistency properties},
  author    = {Fix, Evelyn and Hodges, Joseph Lawson},
  journal   = {International Statistical Review/Revue Internationale de Statistique},
  volume    = {57},
  number    = {3},
  pages     = {238--247},
  year      = {1989},
  publisher = {JSTOR},
  url       = {https://www.jstor.org/stable/1403797},
  abstract  = {The discrimination problem (two population case) may be defined as follows: a random variable Z, of observed value z, is distributed over some space (say, p-dimensional) either according to distribution F, or according to distribution G. The problem is to decide, on the basis of z, which of the two distributions Z has.}
}

@article{hand2001idiot,
  title     = {Idiot's Bayes—not so stupid after all?},
  author    = {Hand, David J and Yu, Keming},
  journal   = {International statistical review},
  volume    = {69},
  number    = {3},
  pages     = {385--398},
  year      = {2001},
  publisher = {Wiley Online Library},
  url       = {https://doi.org/10.1111/j.1751-5823.2001.tb00465.x},
  abstract  = {Folklore has it that a very simple supervised classification rule, based on the typically false assumption that the predictor variables are independent, can be highly effective, and often more effective than sophisticated rules. We examine the evidence for this, both empirical, as observed in real data applications, and theoretical, summarising explanations for why this simple rule might be effective.}
}

@inproceedings{ho1995random,
  title        = {Random decision forests},
  author       = {Ho, Tin Kam},
  booktitle    = {Proceedings of 3rd international conference on document analysis and recognition},
  volume       = {1},
  pages        = {278--282},
  year         = {1995},
  organization = {IEEE},
  url          = {https://ieeexplore.ieee.org/abstract/document/598994},
  abstract     = {Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits.}
}

@inproceedings{kennedy1997discrete,
  title        = {A discrete binary version of the particle swarm algorithm},
  author       = {Kennedy, James and Eberhart, Russell C},
  booktitle    = {1997 IEEE International conference on systems, man, and cybernetics. Computational cybernetics and simulation},
  volume       = {5},
  pages        = {4104--4108},
  year         = {1997},
  organization = {IEEE},
  url          = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=637339},
  abstract     = {The particle swarm algorithm adjusts the trajectories of a population of "particles" through a problem space on the basis of information about each particle's previous best performance and the best previous performance of its neighbors. Previous versions of the particle swarm have operated in continuous space, where trajectories are defined as changes in position on some number of dimensions. The paper reports a reworking of the algorithm to operate on discrete binary variables. In the binary version, trajectories are changes in the probability that a coordinate will take on a zero or one value. Examples, applications, and issues are discussed.}
}

@inproceedings{kennedy1995particle,
  title        = {Particle swarm optimization},
  author       = {Kennedy, James and Eberhart, Russell C},
  booktitle    = {Proceedings of ICNN'95-international conference on neural networks},
  volume       = {4},
  pages        = {1942--1948},
  year         = {1995},
  organization = {IEEE},
  url          = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=488968},
  abtract      = {A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described, }
}

@inproceedings{koppen2000curse,
  title     = {The curse of dimensionality},
  author    = {K{\"o}ppen, Mario},
  booktitle = {5th Online World Conference on Soft Computing in Industrial Applications (WSC5)},
  volume    = {1},
  pages     = {4--8},
  year      = {2000}
}

@inproceedings{liu1995chi2,
  title        = {Chi2: Feature selection and discretization of numeric attributes},
  author       = {Liu, Huan and Setiono, Rudy},
  booktitle    = {Proceedings of 7th IEEE International Conference on Tools with Artificial Intelligence},
  pages        = {388--391},
  year         = {1995},
  organization = {IEEE},
  url          = {https://ieeexplore.ieee.org/abstract/document/479783},
  abstract     = {Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant attributes. This paper describes Chi2 a simple and general algorithm that uses the /spl chi//sup 2/ statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data, and achieves feature selection via discretization. The empirical results demonstrate that Chi/sup 2/ is effective in feature selection and discretization of numeric and ordinal attributes.}
}

@article{loh2011classification,
  title     = {Classification and regression trees},
  author    = {Loh, Wei-Yin},
  journal   = {Wiley interdisciplinary reviews: data mining and knowledge discovery},
  volume    = {1},
  number    = {1},
  pages     = {14--23},
  year      = {2011},
  publisher = {Wiley Online Library},
  url       = {https://doi.org/10.1002/widm.8},
  abstract  = {Classification and regression trees are machine-learning methods for constructing prediction models from data. The models are obtained by recursively partitioning the data space and fitting a simple prediction model within each partition. As a result, the partitioning can be represented graphically as a decision tree. Classification trees are designed for dependent variables that take a finite number of unordered values, with prediction error measured in terms of misclassification cost. Regression trees are for dependent variables that take continuous or ordered discrete values, with prediction error typically measured by the squared difference between the observed and predicted values. This article gives an introduction to the subject by reviewing some widely available algorithms and comparing their capabilities, strengths, and weakness in two examples.}
}

@article{matyushin2020gas,
  title     = {Gas Chromatographic Retention Index Prediction Using Multimodal Machine Learning},
  author    = {Matyushin, Dmitriy D and Buryak, Aleksey K},
  journal   = {Ieee Access},
  volume    = {8},
  pages     = {223140--223155},
  year      = {2020},
  publisher = {IEEE},
  url       = {https://ieeexplore.ieee.org/abstract/document/9294096},
  abstract  = {Gas chromatography is a widely used method in analytical chemistry and metabolomics. Using gas chromatography, vaporizable compounds can be separated for their further identification. Retention indices are standardized values that depend only on a chemical structure of a compound and on a stationary phase and characterize the retention of a compound in a chromatographic system. Retention index prediction is an important task because databases contain experimental values for a small fraction of all possible molecules, while this information is usable for untargeted analysis. In this work, we consider four machine learning models for retention index prediction: 1D and 2D convolutional neural networks, deep residual multilayer perceptron, and gradient boosting. String representation of the molecule, 2D representation of the chemical structure, molecular descriptors and fingerprints, and molecular descriptors are used as inputs of these four models, respectively, along with information about the stationary phase. The first and third models show the best performance, while the other two perform slightly worse. The models predict retention index values for various standard and semi-standard non-polar stationary phases. Further improvement in performance was achieved using a linear model that uses the results of four previous models as inputs (model stacking). The models were tested using various diverse data sets: flavor compounds, essential oils, metabolomics-related compounds. Achieved accuracy: median absolute and percentage errors - 6-40 units and 0.8-2.2%. Accuracy depends on a test data set. The stacking model outperforms previously reported approaches for all test data sets. Parameters of a pre-trained model and some source code are provided.}
}

@misc{musk2020battery,
  title    = {2020 annual meeting of stockholders and Battery Day: Tesla},
  journal  = {Tesla},
  author   = {Musk, Elon},
  year     = {2020},
  month    = {Sep},
  url      = {https://www.tesla.com/en_nz/2020shareholdermeeting},
  abstract = {Tesla’s 2020 Annual Meeting of Stockholders will be held on Tuesday, September 22, 2020, at 1:30 PM Pacific Time. Immediately following the conclusion of the 2020 Annual Meeting, we will hold our separate Battery Day event. Live video webcasts of both events will be accessible to the general public below.}
} 

@misc{restek2018high,
  title        = {High-Resolution GC Analyses of Fatty Acid Methyl Esters (FAMEs)},
  author       = {Restek},
  journal      = {Restek},
  howpublished = {\url{https://www.restek.com/en/technical-literature-library/articles/high-resolution-GC-analyses-of-fatty-acid-methyl-esters-fames/}},
  abstract     = {Fatty acid methyl esters (FAMEs) analysis is an important tool both for characterizing fats and oils and for determining the total fat content in foods. Fats can be extracted from a matrix using a nonpolar solvent and saponified to produce salts of the free fatty acids. After derivatizing the free acids to form methyl esters, the mixture can readily be analyzed by gas chromatography (GC) due to the volatility and thermal stability of the FAMEs. Gas chromatography has become an important technique in fats and oils analysis because accurate results can be obtained for complex as well as simple sample matrices.
                  
                  FAMEs analyses were among the first applications for gas chromatography, so many of the GC methods originally written for analysis of fats and oils described packed column technology. However, capillary columns offer significant advantages, including more efficient separations. When analyzing fats and oils with complex fatty acid profiles such as the cis and trans forms of polyunsaturated fatty acids, higher efficiencies are needed to resolve the individual components. Capillary columns with Carbowax-type (polyethylene glycol) stationary phases are typically used for analyses of saturated and unsaturated fatty acid methyl esters (Figures 1 and 2), and biscyanopropyl phases are used to resolve cis and trans isomers of polyunsaturated components (Figure 3).}
}

@article{robnik2003theoretical,
  title     = {Theoretical and empirical analysis of ReliefF and RReliefF},
  author    = {Robnik-{\v{S}}ikonja, Marko and Kononenko, Igor},
  journal   = {Machine learning},
  volume    = {53},
  number    = {1},
  pages     = {23--69},
  year      = {2003},
  publisher = {Springer},
  url       = {https://link.springer.com/content/pdf/10.1023/A:1025667309714.pdf},
  abstract  = {
               Relief algorithms are general and successful attribute estimators. They are able to detect conditional dependencies between attributes and provide a unified view on the attribute estimation in regression and classification. In addition, their quality estimates have a natural interpretation. While they have commonly been viewed as feature subset selection methods that are applied in prepossessing step before a model is learned, they have actually been used successfully in a variety of settings, e.g., to select splits or to guide constructive induction in the building phase of decision or regression tree learning, as the attribute weighting method and also in the inductive logic programming.
               
               A broad spectrum of successful uses calls for especially careful investigation of various features Relief algorithms have. In this paper we theoretically and empirically investigate and discuss how and why they work, their theoretical and practical properties, their parameters, what kind of dependencies they detect, how do they scale up to large number of examples and features, how to sample data for them, how robust are they regarding the noise, how irrelevant and redundant attributes influence their output and how different metrics influences them.}
}

@article{scholkopf2000new,
  title     = {New support vector algorithms},
  author    = {Sch{\"o}lkopf, Bernhard and Smola, Alex J and Williamson, Robert C and Bartlett, Peter L},
  journal   = {Neural computation},
  volume    = {12},
  number    = {5},
  pages     = {1207--1245},
  year      = {2000},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
  url       = {https://doi.org/10.1162/089976600300015565},
  abstract  = {We propose a new class of support vector algorithms for regression and classification. In these algorithms, a parameter ν lets one effectively control the number of support vectors. While this can be useful in its own right, the parameterization has the additional benefit of enabling us to eliminate one of the other free parameters of the algorithm: the accuracy parameter ε in the regression case, and the regularization constant C in the classification case. We describe the algorithms, give some theoretical results concerning the meaning and the choice of ν, and report experimental results.}
}

@misc{sklearn2021feature,
  title        = {1.13. Feature selection},
  author       = {Sklearn},
  howpublished = {\url{https://sklearn.org/modules/feature_selection.html}},
  journal      = {1.13. Feature selection - scikit-learn 0.19.1 documentation}
}

@article{tibshirani1996regression,
  title     = {Regression shrinkage and selection via the lasso},
  author    = {Tibshirani, Robert},
  journal   = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume    = {58},
  number    = {1},
  pages     = {267--288},
  year      = {1996},
  publisher = {Wiley Online Library},
  url       = {https://doi.org/10.1111/j.2517-6161.1996.tb02080.x},
  abstract  = {We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.}
}


@article{wood2021feature,
  title   = {Feature Selection Methods on Fish Oil Data},
  author  = {Jesse Wood},
  year    = {2022},
  journal = {Victoria University of Wellington},
  note    = {DIS Assignment 1}
}

@misc{wood2021classification,
  title        = {Fish 1 - Google Colab Notebook},
  author       = {Jesse Wood},
  howpublished = {\url{https://colab.research.google.com/drive/1h303x3Z7kVyJwggfvhq8WN7YFvIH5zt_?usp=sharing}},
  journal      = {Google Colab}
}

@misc{wood2021colab,
  title        = {Fish 2 - Google Colab Notebook},
  author       = {Jesse Wood},
  howpublished = {\url{https://colab.research.google.com/drive/1VqnHEXm0rtDsy4ewVJnPkiwqQcVyGMY-?usp=sharing}},
  journal      = {Google Colab}
}

@misc{wood2021visualisation,
  title        = {Fish 3 - Google Colab Notebook},
  author       = {Jesse Wood},
  howpublished = {\url{https://colab.research.google.com/drive/16mpV3eKdjTeFM0Hp4o5-N2LAy1h4gpqo?usp=sharing}},
  journal      = {Google Colab}
}

@misc{wood2021gitlab,
  title        = {Fish data anaylsis - GitLab Repository},
  author       = {Jesse Wood},
  howpublished = {\url{https://gitlab.ecs.vuw.ac.nz/course-work/project489/2021/woodjess3/fish-data-analysis}},
  journal      = {GitLab}
}

% These are copied directly from my ENGR489 project

@article{akkaya2019solving,
  title    = {Solving rubik's cube with a robot hand},
  author   = {Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal  = {arXiv preprint arXiv:1910.07113},
  year     = {2019},
  url      = {https://arxiv.org/abs/1910.07113},
  abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available:}
}

@misc{case2014google,
  title     = {Google Spain SL, Google Inc. v. Agencia Espa{\~n}ola de Protecci{\'o}n de Datos (AEPD), Mario Costeja Gonz{\'a}lez},
  booktitle = {Case C},
  volume    = {2014},
  pages     = {131},
  year      = {2014}
}

@misc{chollet2015keras,
  title     = {Keras},
  author    = {Chollet, Francois and others},
  year      = {2015},
  publisher = {GitHub},
  url       = {https://github.com/fchollet/keras}
}

@article{christiano2017deep,
  title   = {Deep reinforcement learning from human preferences},
  author  = {Christiano, Paul and Leike, Jan and Brown, Tom B and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal = {arXiv preprint arXiv:1706.03741},
  year    = {2017},
  url     = {https://arxiv.org/abs/1706.03741},
  astract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.}
}

@book{domingos2015master,
  title     = {The master algorithm: How the quest for the ultimate learning machine will remake our world},
  author    = {Domingos, Pedro},
  year      = {2015},
  publisher = {Basic Books}
}

@misc{ENZ2021code,
  title   = {Code of Ethical Conduct},
  author  = {ENZ},
  url     = {https://www.engineeringnz.org/engineer-tools/ethics-rules-standards/code-ethical-conduct/},
  journal = {Engineering New Zealand}
}

@incollection{fukushima1982neocognitron,
  title     = {Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition},
  author    = {Fukushima, Kunihiko and Miyake, Sei},
  booktitle = {Competition and cooperation in neural nets},
  pages     = {267--285},
  year      = {1982},
  publisher = {Springer},
  url       = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6313076},
  abstract  = {A recognition with a large-scale network is simulated on a PDP-11/34 minicomputer and is shown to have a great capability for visual pattern recognition. The model consists of nine layers of cells. The authors demonstrate that the model can be trained to recognize handwritten Arabic numerals even with considerable deformations in shape. A learning-with-a-teacher process is used for the reinforcement of the modifiable synapses in the new large-scale model, instead of the learning-without-a-teacher process applied to a previous model. The authors focus on the mechanism for pattern recognition rather than that for self-organization.},
  note      = {ReLU activation function}
}

@book{goodfellow2016deep,
  title     = {Deep learning},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year      = {2016},
  publisher = {MIT press}
}

@article{goodfellow2014generative,
  title    = {Generative adversarial nets},
  author   = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal  = {Advances in neural information processing systems},
  volume   = {27},
  year     = {2014},
  url      = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G
  that captures the data distribution, and a discriminative model D that estimates
  the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This
  framework corresponds to a minimax two-player game. In the space of arbitrary
  functions G and D, a unique solution exists, with G recovering the training data
  distribution and D equal to $\frac{1}{2}$ everywhere. In the case where G and D are defined
  by multilayer perceptrons, the entire system can be trained with backpropagation.
  There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate
  the potential of the framework through qualitative and quantitative evaluation of
  the generated samples.}
}

@article{goyal2021self,
  title   = {Self-supervised pretraining of visual features in the wild},
  author  = {Goyal, Priya and Caron, Mathilde and Lefaudeux, Benjamin and Xu, Min and Wang, Pengchao and Pai, Vivek and Singh, Mannat and Liptchinsky, Vitaliy and Misra, Ishan and Joulin, Armand and others},
  journal = {arXiv preprint arXiv:2103.01988},
  year    = {2021},
  url     = {https://arxiv.org/abs/2103.01988},
  astract = {Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by 1% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9% top-1 with access to only 10% of ImageNet.}
}

@inproceedings{karras2020analyzing,
  title     = {Analyzing and improving the image quality of stylegan},
  author    = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {8110--8119},
  year      = {2020},
  url       = {https://arxiv.org/abs/1912.04958v2},
  abstract  = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.}
}

@book{kelleher2018data,
  title     = {Data science},
  author    = {Kelleher, John D and Tierney, Brendan},
  year      = {2018},
  publisher = {MIT Press}
}

@misc{khan2013gas,
  title     = {Gas chromatography},
  author    = {Khan Academy},
  url       = {https://www.khanacademy.org/science/class-11-chemistry-india/xfbb6cb8fc2bd00c8:in-in-organic-chemistry-some-basic-principles-and-techniques/xfbb6cb8fc2bd00c8:in-in-methods-of-purification-of-organic-compounds/v/gas-chromatography},
  journal   = {Khan Academy},
  publisher = {Khan Academy},
  year      = {2013}
}

@misc{khan2019mass,
  title     = {Mass spectrometry},
  author    = {Khan Academy},
  url       = {https://www.khanacademy.org/science/ap-chemistry-beta/x2eef969c74e0d802:atomic-structure-and-properties/x2eef969c74e0d802:mass-spectrometry-of-elements/v/mass-spectrometry},
  journal   = {Khan Academy},
  publisher = {Khan Academy},
  year      = {2019}
}

@misc{khan2018pvalue,
  title     = {P-values and significance tests},
  author    = {Khan Academy},
  url       = {https://www.youtube.com/watch?v=KS6KEWaoOOE},
  journal   = {Khan Academy},
  publisher = {Khan Academy},
  year      = {2018},
  month     = {Jan}
}

@article{kingma2014adam,
  title    = {Adam: A method for stochastic optimization},
  author   = {Kingma, Diederik P and Ba, Jimmy},
  journal  = {arXiv preprint arXiv:1412.6980},
  year     = {2014},
  url      = {https://arxiv.org/abs/1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}
}

@misc{krulwich2021nick,
  title     = {Nick Cage Movies Vs. Drownings, and More Strange (but Spurious) Correlations},
  url       = {https://www.nationalgeographic.com/science/article/nick-cage-movies-vs-drownings-and-more-strange-but-spurious-correlations},
  journal   = {Science},
  publisher = {National Geographic},
  author    = {Krulwich, Robert},
  year      = {2021},
  month     = {Feb}
}

@article{kullback1951information,
  title     = {On information and sufficiency},
  author    = {Kullback, Solomon and Leibler, Richard A},
  journal   = {The annals of mathematical statistics},
  volume    = {22},
  number    = {1},
  pages     = {79--86},
  year      = {1951},
  publisher = {JSTOR},
  url       = {https://www.jstor.org/stable/2236703}
}

@article{lecun1989generalization,
  title     = {Generalization and network design strategies},
  author    = {LeCun, Yann and others},
  journal   = {Connectionism in perspective},
  volume    = {19},
  pages     = {143--155},
  year      = {1989},
  publisher = {Elsevier Zurich, Switzerland},
  url       = {https://d1wqtxts1xzle7.cloudfront.net/30766382/lecun-with-cover-page-v2.pdf},
  abstract  = {An interestmg property of connectiomst systems is their ability to
  learn from examples. Although most recent work in the field concentrates
  on reducing learning times, the most important feature of a learning machine is its generalization performance. It is usually accepted that good
  generalization performance on real-world problems cannot be achieved
  unless some a pnon knowledge about the task is butlt Into the system.
  Back-propagation networks provide a way of specifymg such knowledge
  by imposing constraints both on the architecture of the network and on
  its weights. In general, such constramts can be considered as particular
  transformations of the parameter space
  Building a constramed network for image recogmtton appears to be a
  feasible task. We descnbe a small handwritten digit recogmtion problem
  and show that, even though the problem is linearly separable, single layer
  networks exhibit poor generalizatton performance. Multtlayer constrained
  networks perform very well on this task when orgamzed in a hierarchical
  structure with shift invariant feature detectors.
  These results confirm the idea that minimizing the number of free
  parameters in the network enhances generalization.},
  note      = {Original CNN paper}
}

@article{lehman2020surprising,
  title     = {The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities},
  author    = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J and Bernard, Samuel and Beslon, Guillaume and Bryson, David M and others},
  journal   = {Artificial life},
  volume    = {26},
  number    = {2},
  pages     = {274--306},
  year      = {2020},
  publisher = {MIT Press},
  url       = {https://direct.mit.edu/artl/article/26/2/274/93255/The-Surprising-Creativity-of-Digital-Evolution-A},
  abstract  = {Evolution provides a creative fount of complex and subtle adaptations that often surprise the scientists who discover them. However, the creativity of evolution is not limited to the natural world: Artificial organisms evolving in computational environments have also elicited surprise and wonder from the researchers studying them. The process of evolution is an algorithmic process that transcends the substrate in which it occurs. Indeed, many researchers in the field of digital evolution can provide examples of how their evolving algorithms and organisms have creatively subverted their expectations or intentions, exposed unrecognized bugs in their code, produced unexpectedly adaptations, or engaged in behaviors and outcomes, uncannily convergent with ones found in nature. Such stories routinely reveal surprise and creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. Bugs are fixed, experiments are refocused, and one-off surprises are collapsed into a single data point. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This article is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.}
}

@article{li2018feature,
  title     = {Feature selection: A data perspective},
  author    = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P and Tang, Jiliang and Liu, Huan},
  journal   = {ACM Computing Surveys (CSUR)},
  volume    = {50},
  number    = {6},
  pages     = {94},
  year      = {2018},
  publisher = {ACM},
  url       = {https://dl.acm.org/doi/pdf/10.1145/3136625},
  abstract  = {Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing data (especially high-dimensional data) for various data-mining and machine-learning problems. The objectives of feature selection include building simpler and more comprehensible models, improving data-mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities to feature selection. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the era of big data, we revisit feature selection research from a data perspective and review representative feature selection algorithms for conventional data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for conventional data, we categorize them into four main groups: similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based methods. To facilitate and promote the research in this community, we also present an open source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/). Also, we use it as an example to show how to evaluate feature selection algorithms. At the end of the survey, we present a discussion about some open problems and challenges that require more attention in future.}
}

@article{liu2021machine,
  title     = {Machine Learning Conservation Laws from Trajectories},
  author    = {Liu, Ziming and Tegmark, Max},
  journal   = {Physical Review Letters},
  volume    = {126},
  number    = {18},
  pages     = {180604},
  year      = {2021},
  publisher = {APS},
  url       = {https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.126.180604},
  abstract  = {We present AI Poincaré, a machine learning algorithm for autodiscovering conserved quantities using trajectory data from unknown dynamical systems. We test it on five Hamiltonian systems, including the gravitational three-body problem, and find that it discovers not only all exactly conserved quantities, but also periodic orbits, phase transitions, and breakdown timescales for approximate conservation laws.}
}

@article{maini2017machine,
  title   = {Machine learning for humans},
  author  = {Maini, Vishal and Sabri, S},
  journal = {Noudettu osoitteesta https://medium.com/machine-learning-for-humans/why-machine-learningmatters-6164faf1df12},
  year    = {2017}
}

@inproceedings{mikolov2013linguistic,
  title     = {Linguistic regularities in continuous space word representations},
  author    = {Mikolov, Tom{\'a}{\v{s}} and Yih, Wen-tau and Zweig, Geoffrey},
  booktitle = {Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies},
  pages     = {746--751},
  year      = {2013},
  url       = {https://aclanthology.org/N13-1090.pdf},
  abstract  = {Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations  that are implicitly learned by the input-layer  weights. We find that these representations  are surprisingly good at capturing syntactic  and semantic regularities in language, and  that each relationship is characterized by a  relation-specific vector offset. This allows  vector-oriented reasoning based on the offsets  between words. For example, the male/female  relationship is automatically learned, and with  the induced vector representations, “King -  Man + Woman” results in a vector very close  to “Queen.” We demonstrate that the word  vectors capture syntactic regularities by means  of syntactic analogy questions (provided with  this paper), and are able to correctly answer  almost 40% of the questions. We demonstrate  that the word vectors capture semantic regularities by using the vector offset method to  answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best  previous systems.}
}

@inproceedings{nguyen2014filter,
  title        = {Filter based backward elimination in wrapper based PSO for feature selection in classification},
  author       = {Nguyen, Hoai Bach and Xue, Bing and Liu, Ivy and Zhang, Mengjie},
  booktitle    = {2014 IEEE congress on evolutionary computation (CEC)},
  pages        = {3111--3118},
  year         = {2014},
  organization = {IEEE},
  url          = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6900657},
  abstract     = {The advances in data collection increase the dimensionality of the data (i.e. the total number of features) in many fields, which arises a challenge to many existing feature selection approaches. This paper develops a new feature selection approach based on particle swarm optimisation (PSO) and a local search that mimics the typical backward elimination feature selection method. The proposed algorithm uses a wrapper based fitness function, i.e. the classification error rate. The local search is performed only on the global best and uses a filter based measure, which aims to take the advantages of both filter and wrapper approaches. The proposed approach is tested and compared with three recent PSO based feature selection algorithms and two typical traditional feature selection methods. Experiments on eight benchmark datasets show that the proposed algorithm can be successfully used to select a significantly smaller number of features and simultaneously improve the classification performance over using all features. The proposed approach outperforms the three PSO based algorithms and the two traditional methods.}
}

@inproceedings{nguyen2017particle,
  title        = {Particle swarm optimisation with genetic operators for feature selection},
  author       = {Nguyen, Hoai Bach and Xue, Bing and Andreae, Peter and Zhang, Mengjie},
  booktitle    = {2017 IEEE Congress on Evolutionary Computation (CEC)},
  pages        = {286--293},
  year         = {2017},
  organization = {IEEE},
  url          = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7969325},
  abstract     = {Feature selection is an important task in machine learning, which aims to reduce the dataset dimensionality while at least maintaining the classification performance. Particle Swarm Optimisation (PSO) has been widely applied to feature selection because of its effectiveness and efficiency. However, since feature selection is a challenging task with a complex search space, PSO easily gets stuck at local optima. This paper aims to improve the PSO's searching ability by applying genetic operators such as crossover and mutation to assist the swarm to explore the search space better. The proposed genetic operators are specifically designed for feature selection, which not only improve the quality of current feature subsets but also make the search smoother. The proposed algorithm, called CMPSO, is tested and compared with three recent PSO based feature selection algorithms. Experimental results on eight datasets show that CMPSO can adapt with different numbers of features to evolve small feature subsets, which achieve similar or better classification performance than using all features and the three PSO based algorithms. The analysis on evolutionary processes shows that genetic operators assist CMPSO to evolve better solutions than the original PSO.}
}

@article{olah2018building,
  title    = {The building blocks of interpretability},
  author   = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  journal  = {Distill},
  volume   = {3},
  number   = {3},
  pages    = {e10},
  year     = {2018},
  url      = {https://distill.pub/2018/building-blocks},
  abstract = {With the growing success of neural networks, there is a corresponding need to be able to explain their decisions — including building confidence about how they will behave in the real-world, detecting model bias, and for scientific curiosity. In order to do so, we need to both construct deep abstractions and reify (or instantiate) them in rich interfaces . With a few exceptions, existing work on interpretability fails to do these in concert.
  
  The machine learning community has primarily focused on developing powerful methods, such as feature visualization, attribution, and dimensionality reduction , for reasoning about neural networks. However, these techniques have been studied as isolated threads of research, and the corresponding work of reifying them has been neglected. On the other hand, the human-computer interaction community has begun to explore rich user interfaces for neural networks, but they have not yet engaged deeply with these abstractions. To the extent these abstractions have been used, it has been in fairly standard ways. As a result, we have been left with impoverished interfaces (e.g., saliency maps or correlating abstract neurons) that leave a lot of value on the table. Worse, many interpretability techniques have not been fully actualized into abstractions because there has not been pressure to make them generalizable or composable.
  
  In this article, we treat existing interpretability methods as fundamental and composable building blocks for rich user interfaces. We find that these disparate techniques now come together in a unified grammar, fulfilling complementary roles in the resulting interfaces. Moreover, this grammar allows us to systematically explore the space of interpretability interfaces, enabling us to evaluate whether they meet particular goals. We will present interfaces that show what the network detects and explain how it develops its understanding, while keeping the amount of information human-scale. For example, we will see how a network looking at a labrador retriever detects floppy ears and how that influences its classification.
  
  Our interfaces are speculative and one might wonder how reliable they are. Rather than address this point piecemeal, we dedicate a section to it at the end of the article.
  In this article, we use GoogLeNet, an image classification model, to demonstrate our interface ideas because its neurons seem unusually semantically meaningful. 1 Although here we’ve made a specific choice of task and network, the basic abstractions and patterns for combining them that we present can be applied to neural networks in other domains.
  }
}

@misc{oracle2014,
  title     = {Oracle America, Inc. v. Google Inc.},
  booktitle = {F. 3d},
  volume    = {750},
  pages     = {1339},
  number    = {No. 2013-1021},
  year      = {2014},
  publisher = {Court of Appeals, Federal Circuit}
}

@inproceedings{peng2018sim,
  title        = {Sim-to-real transfer of robotic control with dynamics randomization},
  author       = {Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle    = {2018 IEEE international conference on robotics and automation (ICRA)},
  pages        = {3803--3810},
  year         = {2018},
  organization = {IEEE},
  url          = {https://ieeexplore.ieee.org/document/8460528/},
  abstract     = {Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this “reality gap”. By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.}
}

@article{robinson2020genetic,
  title    = {Genetic Algorithm for Feature and Latent Variable Selection for Nutrient Assessment in Horticultural Products},
  author   = {Demelza, Robinson, Bing and Zhang, Mengjie and et al},
  pages    = {1--8},
  year     = {2020},
  url      = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9504794},
  abstract = {Vibrational spectroscopy can be used for rapid determination of chemical quality markers in horticultural produce to improve quality control, optimize harvest times and maximize profits. Most commonly, spectral data are calibrated against chemical reference data (acquired using traditional, slower analytical methods) using partial least squares regression (PLSR). However, predictive performance of PLSR can be limited by the small number of instances, high dimensionality and collinearity of spectroscopic data. Here, a new genetic algorithm (GA) for PLSR feature and latent variable selection is proposed to predict concentrations of 18 important bioactive components across three New Zealand horticultural products from infrared, near-infrared and Raman spectral data sets. Models generated using the GA-enhanced PLSR method have notably better generalization and are less complex than the standard PLSR method. GA-enhanced PLSR models are produced from each spectroscopic data set individually, and from a data set that combines all three techniques.}
}

@article{russell2002artificial,
  title  = {Artificial intelligence: a modern approach},
  author = {Russell, Stuart and Norvig, Peter},
  year   = {2002}
}

@article{sculley2014machine,
  title   = {Machine learning: The high interest credit card of technical debt},
  author  = {Sculley, David and Holt, Gary zand Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
  year    = {2014},
  url     = {https://research.google/pubs/pub43146/},
  astract = {Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.}
}

@article{shannon2001mathematical,
  title     = {A mathematical theory of communication},
  author    = {Shannon, Claude Elwood},
  journal   = {ACM SIGMOBILE mobile computing and communications review},
  volume    = {5},
  number    = {1},
  pages     = {3--55},
  year      = {2001},
  publisher = {ACM New York, NY, USA},
  url       = {https://dl.acm.org/doi/pdf/10.1145/584091.584093},
  abtract   = {THE recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist 1 and Hartley 2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.
  The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design.
  If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
  note      = {Entropy}
}

@article{simonyan2014very,
  title    = {Very deep convolutional networks for large-scale image recognition},
  author   = {Simonyan, Karen and Zisserman, Andrew},
  journal  = {arXiv preprint arXiv:1409.1556},
  year     = {2014},
  url      = {https://arxiv.org/abs/1409.1556},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  note     = {VGG-16 model}
}

@book{spiegelhalter2019art,
  title     = {The art of statistics: learning from data},
  author    = {Spiegelhalter, David},
  year      = {2019},
  publisher = {Penguin UK}
}

@misc{titanic,
  title  = {Machine Learning from Disaster | Kaggle},
  author = {Kaggle},
  url    = {https://www.kaggle.com/c/titanic}
}

@book{tegmark2017life,
  title     = {Life 3.0: Being human in the age of artificial intelligence},
  author    = {Tegmark, Max},
  year      = {2017},
  publisher = {Knopf}
}

@article{tomasi2004correlation,
  title     = {Correlation optimized warping and dynamic time warping as preprocessing methods for chromatographic data},
  author    = {Tomasi, Giorgio and Van Den Berg, Frans and Andersson, Claus},
  journal   = {Journal of Chemometrics: A Journal of the Chemometrics Society},
  volume    = {18},
  number    = {5},
  pages     = {231--241},
  year      = {2004},
  publisher = {Wiley Online Library},
  url       = {https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/epdf/10.1002/cem.859},
  abstract  = {Two different algorithms for time-alignment as a preprocessing step in linear factor models are studied. Correlation optimized warping and dynamic time warping are both presented in the literature as methods that can eliminate shift-related artifacts from measurements by correcting a sample vector towards a reference. In this study both the theoretical properties and the practical implications of using signal warping as preprocessing for chromatographic data are investigated. The connection between the two algorithms is also discussed. The findings are illustrated by means of a case study of principal component analysis on a real data set, including manifest retention time artifacts, of extracts from coffee samples stored under different packaging conditions for varying storage times. We concluded that for the data presented here dynamic time warping with rigid slope constraints and correlation optimized warping are superior to unconstrained dynamic time warping; both considerably simplify interpretation of the factor model results. Unconstrained dynamic time warping was found to be too flexible for this chromatographic data set, resulting in an overcompensation of the observed shifts and suggesting the unsuitability of this preprocessing method for this type of signals. Copyright © 2004 John Wiley & Sons, Ltd.}
}

@article{tran2018variable,
  title     = {Variable-length particle swarm optimization for feature selection on high-dimensional classification},
  author    = {Tran, Binh and Xue, Bing and Zhang, Mengjie},
  journal   = {IEEE Transactions on Evolutionary Computation},
  volume    = {23},
  number    = {3},
  pages     = {473--487},
  year      = {2018},
  publisher = {IEEE},
  url       = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8458226},
  abstract  = {With a global search mechanism, particle swarm optimization (PSO) has shown promise in feature selection (FS). However, most of the current PSO-based FS methods use a fix-length representation, which is inflexible and limits the performance of PSO for FS. When applying these methods to high-dimensional data, it not only consumes a significant amount of memory but also requires a high computational cost. Overcoming this limitation enables PSO to work on data with much higher dimensionality which has become more and more popular with the advance of data collection technologies. In this paper, we propose the first variable-length PSO representation for FS, enabling particles to have different and shorter lengths, which defines smaller search space and therefore, improves the performance of PSO. By rearranging features in a descending order of their relevance, we facilitate particles with shorter lengths to achieve better classification performance. Furthermore, using the proposed length changing mechanism, PSO can jump out of local optima, further narrow the search space and focus its search on smaller and more fruitful area. These strategies enable PSO to reach better solutions in a shorter time. Results on ten high-dimensional datasets with varying difficulties show that the proposed variable-length PSO can achieve much smaller feature subsets with significantly higher classification performance in much shorter time than the fixed-length PSO methods. The proposed method also outperformed the compared non-PSO FS methods in most cases.}
}

@book{tyson2017astrophysics,
  title     = {Astrophysics for People in a Hurry},
  author    = {Tyson, Neil deGrasse},
  year      = {2017},
  publisher = {WW Norton \& Company}
}

@article{udrescu2020ai,
  title     = {AI Feynman: A physics-inspired method for symbolic regression},
  author    = {Udrescu, Silviu-Marian and Tegmark, Max},
  journal   = {Science Advances},
  volume    = {6},
  number    = {16},
  pages     = {eaay2631},
  year      = {2020},
  url       = {https://www.science.org/doi/10.1126/sciadv.aay2631},
  publisher = {American Association for the Advancement of Science},
  abstract  = {A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90%.}
}

@article{xue2014particle,
  title     = {Particle swarm optimisation for feature selection in classification: Novel initialisation and updating mechanisms},
  author    = {Xue, Bing and Zhang, Mengjie and Browne, Will N},
  journal   = {Applied soft computing},
  volume    = {18},
  pages     = {261--276},
  year      = {2014},
  publisher = {Elsevier},
  url       = {https://www.sciencedirect.com/science/article/pii/S1568494613003128},
  abstract  = {In classification, feature selection is an important data pre-processing technique, but it is a difficult problem due mainly to the large search space. Particle swarm optimisation (PSO) is an efficient evolutionary computation technique. However, the traditional personal best and global best updating mechanism in PSO limits its performance for feature selection and the potential of PSO for feature selection has not been fully investigated. This paper proposes three new initialisation strategies and three new personal best and global best updating mechanisms in PSO to develop novel feature selection approaches with the goals of maximising the classification performance, minimising the number of features and reducing the computational time. The proposed initialisation strategies and updating mechanisms are compared with the traditional initialisation and the traditional updating mechanism. Meanwhile, the most promising initialisation strategy and updating mechanism are combined to form a new approach (PSO(4-2)) to address feature selection problems and it is compared with two traditional feature selection methods and two PSO based methods. Experiments on twenty benchmark datasets show that PSO with the new initialisation strategies and/or the new updating mechanisms can automatically evolve a feature subset with a smaller number of features and higher classification performance than using all features. PSO(4-2) outperforms the two traditional methods and two PSO based algorithm in terms of the computational time, the number of features and the classification performance. The superior performance of this algorithm is due mainly to both the proposed initialisation strategy, which aims to take the advantages of both the forward selection and backward selection to decrease the number of features and the computational time, and the new updating mechanism, which can overcome the limitations of traditional updating mechanisms by taking the number of features into account, which reduces the number of features and the computational time.}
}

@article{zhang2008two,
  title     = {Two-dimensional correlation optimized warping algorithm for aligning GC$\times$ GC- MS data},
  author    = {Zhang, Dabao and Huang, Xiaodong and Regnier, Fred E and Zhang, Min},
  journal   = {Analytical Chemistry},
  volume    = {80},
  number    = {8},
  pages     = {2664--2671},
  year      = {2008},
  publisher = {ACS Publications},
  url       = {https://pubs.acs.org/doi/pdf/10.1021/ac7024317},
  abstract  = {A two-dimensional (2-D) correlation optimized warping (COW) algorithm has been developed to align 2-D gas chromatography coupled with time-of-flight mass spectrometry (GC×GC/TOF-MS) data. By partitioning raw chromatographic profiles and warping the grid points simultaneously along the first and second dimensions on the basis of applying a one-dimensional COW algorithm to characteristic vectors, nongrid points can be interpolatively warped. This 2-D algorithm was directly applied to total ion counts (TIC) chromatographic profiles of homogeneous chemical samples, i.e., samples including mostly identical compounds. For heterogeneous chemical samples, the 2-D algorithm is first applied to certain selected ion counts chromatographic profiles, and the resultant warping parameters are then used to warp the corresponding TIC chromatographic profiles. The developed 2-D COW algorithm can also be applied to align other 2-D separation images, e.g., LC×LC data, LC×GC data, GC×GC data, LC×CE data, and CE×CE data.}
}

@article{zhou1988image,
  title     = {Image restoration using a neural network},
  author    = {Zhou, Y-T and Chellappa, Rama and Vaid, Aseem and Jenkins, B Keith},
  journal   = {IEEE transactions on acoustics, speech, and signal processing},
  volume    = {36},
  number    = {7},
  pages     = {1141--1151},
  year      = {1988},
  publisher = {IEEE},
  url       = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1641},
  abstract  = {An approach for restoration of gray level images degraded by a known shift invariant blur function and additive noise is presented using a neural computational network. A neural network model is used to represent a possibly nonstationary image whose gray level function is the simple sum of the neuron state variables. The restoration procedure consists of two stages: estimation of the parameters of the neural network model and reconstruction of images. Owing to the model's fault-tolerant nature and computation capability, a high-quality image is obtained using this approach. A practical algorithm with reduced computational complexity is also presented. A procedure for learning the blur parameters from prototypes of original and degraded images is outlined.},
  note      = {Max pooling layer}
}
