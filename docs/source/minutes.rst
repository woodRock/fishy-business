Minutes
=======

This page contains the minutes for our weekly meetings. 

2022-02-23 - Planning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** C0355, **Time**: Monday 1pm-2pm , **Attendees:** Jesse Wood, Mengjie Zhang

Notes: 
    * Faculty of Graduate Research (FGR) - office on Kelburn Parade. 
    * Forms and information for enrollment is available at the FGR website. 
    * Booked a room for study in MARU101 - Desk 33
    * See Duncan in ECS for an account. 
    * Can work up to 12 hours per week. 
    * Let supervisors and faculty know about trips out of Wellington. 
    * Start as provisional registration, then candidate - write proposal, fully registered - proposal accepted. 
    * Two required meetings, FASLIP (Thursday 2pm-3pm), ECRG (Friday 3pm-5pm).

2022-02-28 - FGR
~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Matthew Vink, Jesse Wood

Notes: 
    * This meeting covers enrollment, we will be confirming details, forms, contacts.
    * PhD Supervisors: Bing Xue, Mengjie Zhang.
    * Documents: 
        1. Confirmation of study - AIML 692 code. 
        2. Fees assessment - two weeks to pay levees. 
    * Information sheet:
        1. Community for needs bank details. 
        2. Tony mcGloughin - School Administrator. 
        3. Confirmation of Proposal Registration Form (CoPR).
        4. Mathew Vink - helped me enroll today. 
        5. Student levees - 2 weeks due.  

2022-02-28 - FASLIP
~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Wednesay 2pm-3pm , **Attendees:** Matthew Vink, Jesse Wood

Neil Dodgeson - Cambridege lecture
    * How to not give a presentation. https://vimeo.com/51597270
    * How to present a paper. https://vimeo.com/7833850

Notes: 
    * Simular to ENGR401 stuff
        * Dont need slides.
        * Trip check technologies. 
        * Face audience. 
        * Relevant stuff only. 
        * No animations. 
    * Research Talks 
        * Don't type the script. 
        * Planning, a lot of time before writing slides. 
        * Audience, can change how you deliver a presentation. 
        * Highlight key points on the last slide. 

2022-03-07 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Bastiaan Kleign, Jesse Wood, et al. 

Papers: 
    1. Conditional Diffuction Probablistic Model for Speech Enhancment. https://arxiv.org/abs/2202.05256
    2. A Study on Speech enhancment on Diffusion Probabilistic Model. https://arxiv.org/abs/2107.11876

Notes:
    * Diffusion models got attention for synthesising images (i.e faces, animals). 
    * Later, it bet the GAN on standard benchmarks. https://proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html
    * Train it to add noise, the reverse the process. 
    * Diffusion: learning to denoise speech signal. 
    * Isotropic gaussian distribution? https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic
    * Learn the signal-to-noise difference, not the mean signal. 
    * Diffusion markov chain is intractable, so we use Elbo to from an approximate objective function. 
    * Ratios and constants to ensure the mean and variance don't explode or vanish. 
    * New Mailing list for DL updates. 
    * Next week: Bayesian Transformers. 

2022-03-08 - Induction
~~~~~~~~~~~~~~~~~~~~~~
**Location:** AM101, **Time**: Monday 2pm-4pm , **Attendees:** Neil Dodgeson, Jesse Wood, et al. 

Notes:
    * Bastiaan slide example for meetings. 
    * Neil Dodgeson - Faculty of Graduate Research Dean. 
    * Faculty of Graduate Research (FGR). 
    * Workshops, writing events, professional development. 
    * Website https://www.wgtn.ac.nz/fgr
    * Workshops are practical and hands-on. 
    * Thesis bootcamp - 20 writing hours. 
        * Aimed at final year students. 
        * June november 
    * Research room 
        * Review, tips, stories, events, resources. 
        * Updates monthly. 
    * Candidate progress form (CPF)
        * Report on 6 monthly progress in a report. 
        * May / November. 
        * Required, not academic, supporting evidence. 
    * 4 weeks annual leave, no formal process. 
    * Suspensions, for illness, bereavement, work. 
    * Forms for aforementioned available online. 
    * Proposal: first major milestone. 
        * 12 month deadline. 
        * no extensions available. 
    * Automatic re-registration for first 2 years. 
    * Constructive relationship with supervisor. 
    * PhD certificate: competent to do invidual research. 
    * Work expert in our PhD Research topic. 
    * Regular meetings times. 
    * Student/supervisor - same page for expectation.
    * Bring agenda to meeting.
    * Project management techniques - scrum, agile. 
    * 2pi rule for time estimation. 
    * Secondary supervisor - (usually) hands off role. 
    * "The only way through it, is to do it." 
    * Books, publications, thesis - different expectations for each course. 

2022-03-10 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-4pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood 

Notes:
    * Let Bing/Meng know about any financial difficulties. 
    * Topic ideas: 
        1. Multi-objective 
        2. Evolutionary computation
        3. Domain expertise. 
    * First two-weeks - extensive background reading. 
    * ECRG - meeting tomorrow from 3pm - 5pm. 
    * CoPR - fill out by the end of March. 
    * Individual induction - copy Bach in email for meeting. 
    * Add Bach to gitlab/github for the paper latex file. 

2022-03-10 - FASLIP
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-4pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Jeff Hawkins - Thousand Brains Theory: https://www.youtube.com/watch?v=O4geanMOsyM

Notes:
    * Voting, similar to droupout, bagged ensemble. 
    * Many models (sub-networks) for the same thing. 
    * Sparse networks, efficient -> noise tolerant. 
    * Only update in one area, without need for back-propagation, doesn't require a full training for each new instance. 
    * Builds a full world model, not a model for each task. 
    * Thousand brain theory - solution to No Free Lunch. 

2022-03-11 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Hui Ma gave presentation on Evolutionary Computation Approaches to Web Service Composition - https://link.springer.com/article/10.1007/s10732-017-9330-4

Notes:
    * Meng will discuss how to write a proposal. 
    * EuroGP conference - ask my supervisor to register. 
    * Introduced myself to the group 
        * paper - finish writing my Summer Research paper. 
        * enrolled - lots of paper work. 
        * Finish writing the paper properly. 
    * Abdullah (lab neighbour) first week in group.
    * Evolutionary Computation Approaches to Web Service Composition. 
    * Over 40 publications in the area. 
    * Holidy booking service used as an example. 
    * Organize services into re-usable modules. 
    * Service composition is a NP-hard problem. 
    * A global search is not possible, a heuristic based local search is required. 
    * Evolutionary principles and techniques - crossover, mutation.
    * Automatcally create hybrid services through composition. 
    * Don't reinvent the wheel, use existing libraries instead. 
    * Scheduling, routing, resource allocation, service composition - all possible for EC. 
    
2022-03-17 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood. 

Notes:
    * Daniel can provide domain expertise for writing the chemistry sections for the paper. 
    * Multi-objective - classify chemical compounds and their percentage.  
    * Multi-label - one instance can belong to multiple classes. 
    * copy Bing and Bach for induction email from Georgia. 
    * pymoo  - multi-objective python library. 
    * Read/write summaries for papers as I go - write content for second chapter iteratively. 
    * Send Daniel conclusions / contributions of paper in email, then organize a follow up meeting.

2022-03-17 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Ying, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Ying suggested a talk on Multi-objective Evolutionary Federated Learning https://vimeo.com/552900291

Notes: 
    * Yaochi Jin - University of Surrey. 
    * Multi-objective machine learning. 
    * Centralized and federated learning. 
    * Evolutionary multi-objecive federated learning. 
    * Evolutionary federated nerual architecture search. 
    * Multi-objective - gives a solution set, as their are tradeoffs between objectives. 
    * Dominance, no X is worse and Y, and X is strictly better than Y for object A. 
    * pareto front (See tegmark2020ai) set of optimal solutions.
        * accuracy, diversity. 
        * Inverse generational distance (IGD).
        * Hypervolume - nadir 
    * Optimize for minimal complexity implies interpretability. 
    * Centralized learning - one database. 
    * Localized learning - everyone trains their own model. 
    * Privacy techniques: 
        * Secure multi-party computation. 
        * Differential privacy. 
        * Homomorphic encyption. 
    * Federated learning 
        * train a high-quality centralized model with training dataq distributed over a large number of clients. 
        * Each with unreliable and relatively slow network connections. 
        * horizontal - all attributes, batches of data. 
        * vertical - trained on subset of attributes (i.e. security reasons). 
    * Federated learning objectives 
        1. Maximise learning performance. 
        2. Minimize communication cost. 
    * Their work efficiently reduce the number of connections while maintaining similar performance. 
    * Neural Architecture Search (TODO - watch the rest and take notes!!!)

2022-03-18 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

A talk on Geometric Semantic Genetic Programming by Qi Chen https://link.springer.com/chapter/10.1007/978-3-642-32937-1_3

Notes: 
    * We have published heaps of papers that are highly cites and hot papers according to https://www.webofscience.com/wos/woscc/basic-search toool that the university has access to.
    * Top 1% of papers cited per discipline for computer science journals. 
    * Evolving neural networks with evolutionary computation. 
    * Me: reading psychology papers on how the brain works with memory - hunting for relative simply neuro-science ideas to apply to machine learning. 
    * Geometric smenatic genetic programming (Morgalio 2012, moraglio2012geometric) https://link.springer.com/chapter/10.1007/978-3-642-32937-1_3 
    * Semantic genetic programming methods. 
    * Traditional GP ignores program semantics. 
    * Consequences - ragged gentype-phenotype mapping. 
    * Is it possible to make GP aware about the effects of the program execution? 
    * Semantics: 
        * Semantics differs from syntax. 
        * Semantics related to the problem domain. 
        * Semantics inform program design (Tegmark 2020, tegmark2020ai).
    * Measure semantic distance between current program and target output (multi-dimensional loss function). 
    * Genetic operators: 
        * Semantic aware cross-over (SAC) 
        * Semantic similarity-based cross-over (SSC)
        * Semantic similarity-based mutation (SSM)
        * Senantic tournament selection. 
            * t-test for statistical signfician with assessing selection. 
    * Search directly in the semantci space of the program. 
    * Semnatics of offsrping must sit in between the ntercept between its two parents in semantic space. 
    * Therefore each offspring minimized distance to target semantics. 
    * Each generation gets closer to the target semantics, or atleast closer than the furthest parent. 
    * Independent of data, good effect on improving generalization, althougt not actual claim made in paper. 
    * Geometric semantic programming leads to a unimodel fitness landscape - a cone where the apex is the target semantics. 
        * manhattan distance - square based pyramid.
        * euclidean distance - cone. 
    * Efficient implementation - only store changes to program tree, similar to git version control - except for GSGP. 
    * GSGR Red (reduce), simplify problems by expanding and recomputing. 
    * Locally geometric semantic crossover (LGSX).
        * Make offsrping similar to eachother than their parents. 
    * Random desired operator (RDO), exploit interoperability of instructions, + can be reversed with -, * can be reversed with division, + and * are communicative. 
    * Semantic backpropogation - decomposibility of the process if important for BP. 
    * Angle aware metrics - larger angle metrics iis more likely to generate offspring closer to target semantics. 
    * Permutations GSX and Random Segment Mutation 
    * Semenatic distance (euclidean) is the same as the loss, just looking at it from a different point of view. 
    * Can geometric smeantic programming work in an unsupervised or combinatorial problem? (Possibly not unimodel semantic space)
    
2022-03-21 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Hayden Dyne, Bastiaan Kleign, Jesse Wood, et al. 

Talk by Hayden on two papers: 
    * End-to-end driving via conditional imitation learning (Cai 2020, cai2020high) https://ieeexplore.ieee.org/abstract/document/8460487/
    * High-speed autonomous drifting with deep reinforcement learning (Codevilla 2018, codevilla2018end) https://ieeexplore.ieee.org/abstract/document/8961997/ 

Notes: 
    * Model-free reinforcement learning - does not rely on human understanding of world and design controllers. 
    * Human driver is the trajectory with is the goal, uses a professional driver playing the game with a steering wheel. 
    * Model performs on different track difficulties. 
    * Reward function is scaled by velocity, so faster lap times are rewarded. 
    * Works for 4 different kinds of vehicles, although the truck struggles to achieve same performance as lighter ones. 
    * Second paper - e2e 
    * Far easier to use real-world data on driving that has already been collected than generate simulation data. 
    * Data augmentation used to help network generalize to new scenarios and edge cases not in the training data. 

2022-03-24 - Faculty Induction
------------------------------
**Location:** Zoom, **Time**: Monday 10am-11am , **Attendees:** Georgia Dix, Jesse Wood, Bach Hoai Nguyen.

Induction to my PhD studies with supervisor and faculty. 

Notes: 
    * Expectations
        * Supervisor
            * Uni life 
            * Framework 
            * Networking 
            * Assessment 
        * Me
            * action plan 
            * identify problems 
            * administration 
            * CDP (6 monthly report)
    * Marking can thesis can take up to 6 months - can work during this time. 

2022-03-17 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood. 

Notes:
    * Daniel can draft the chemistry parts for the paper. 
    * Draft the full paper with Bach, then send to Daniel. 
    * Read "From evolutionary computation to the evolution of things" - Nature
    * Can start coding now - explore ideas for ENGR489 and EC on existing data. 
    * Transformers, LSTM, GAN - yet to be applied to GC-MS data in literature. 
    * CNNs for GC, likely due to libraries, hype, understanding, Diffusion of innovation. 
    * Scuba diver experiment for context-dependent memory is a good analogy for noise in ML models.
    * Came up with evolutionary ideas, like sexual selection, but (Miller 1994) did it quite some time ago.
    * Idea for EC, a dynamic environment where complexity increases, classes or features are added - similar to evolution IRL. 

2022-03-24 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Ying, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Ruwing Jiao suggested a video on Bayesian Optimization from Mark Deisenroth https://www.youtube.com/watch?v=_SC5_2vkgbA 

Notes: 
    * Recommended background reading on this topic: 
        1. A Tutorial on Bayesian Optimization of Expensive Cost Functions (Brochu 2010, brochu2010tutorial) https://arxiv.org/pdf/1012.2599.pdf
        2. Taking the Human Out of theLoop: A Review of Bayesian Optimization (Shahriari 2015, shahriari2015taking) https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306
    * Latent Structural Support Vector Machine (Miller 2012) - **TODO** find this paper/project. 
    * Deep learning often involves a lot of hyper-parameter tuning, this is usally done by the practitioner model. 
    * Alternative approaches: 
        * Manual tuning 
        * Grid search 
        * Random search 
        * Black magic (i.e. lr is 1e-3 is "good")
    * Computationally expensive to search for global maximum in hyper-parameter search space. 
    * Globally optimize a black-box approach to evaluate (e.g. cross validatio error for a massive neural network). 
    * Use a probabilistic model to approximate the black-box model for the hyper-parameter search. 
        * create a proxy model - this learns an approximation of the space - with less computational cost to query that space. 
        * referred to ass proxy / approximate / surrogate. 
    * The standard model for optimizing a bayesian model is a gaussian process. 
    * Evaluate the proxy function once, this saves computation. 
    * A gaussian process minimized the uncertainty of the proxy function.
    * It samples the feature space at the minimum value of the shaded area (== uncertainty).
    * It repeats this often, until the proxy function is close enough to the true objective. 
    * Exploration - sample areas with high uncertainty. 
    * Exploitation - sample places with low mean. 

2022-03-25 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Andrew Lenson, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Andrew gave a talk on Genetic Programming, Explainability and Interdisciplinary AI.

Notes: 
    * Heaps of students successfully submitted papers to the Gecko conference. 
    * Possible to publish in conference at different levels; paper, poster, etc. 
    * If a paper is declined, revise with reviewer comments, and resubmit as poster. 
    * Qurrat Al Ain - Cancer research in AI. 
    * Swiss roll manifold problem
        * Reduce a manifold to a 2D visual representation. 
        * 2D path is representation of non-linear dimensionality reduction (NLDR).
    * Geo-desic, shortest path from A to B, not shortest euclidean distance. 
    * Lower dimensional space is referred to as an embedding. 
    * We can use AI to learn or approximate this embedding (if the problem is intractable).
    * Ways to estimate the intrinsic dimensionality of the dataset - statistical techniques. 
    * Kaka - count distinct nnumber of birds at Zealandia. 
    * GoPro for data collection combined with crack for Kaka. 
    * Law - predicting sentencing lengths with PLSR on judge summaries. 
    * Names with high/low probabilities are often historic cases referred to as 'guidance judgements'. 
    * Combine data analysis and domain expertise to infer knowledge about sentencing lengths. 
    * Home detention or communtiy service are associated with shorter sentences. 
    * Future work, take humans out of the loop, and make sentencing deterministic.
    * ^ This can be done, because their are extenuating circumstances that require a judges opinion.
    * Also, if all sentences are automated, there would no longer be guidance judgements being set historically. 
    * Law is a dynamic and decentralized system, unique and specialized for each country, case, individual, etc... 
    * Research more productive on letting judges analysis their blindspots, and identify bias.

2022-03-28 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Daniel Braithwaite, Bastiaan Kleign, Jesse Wood, et al. 

Daniel Braithwaite talked about two papers related to machine learning for audio wave construction:
    1. Deep Audio Priors Emerge from Harmonic Convolutional Networks https://openreview.net/pdf?id=rygjHxrYDB
    2. Harmonic WaveGAN https://www.isca-speech.org/archive/pdfs/interspeech_2021/mizuta21_interspeech.pdf

Notes: 
    * The idea is to look at harmonic convolutions, think convolution layer but designed for audio. 
    * WaveGAN and Harmonic WaveGAN use deep learning on audio signals. 
    * Harmonic, is better suited towards audio signals, than wave alone. 
    * Harmonic considers local connections / adjaceny better. 
    * **TODO** Read these papers and add to notes. 

2022-03-31 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Jesse Wood. 

Bing and Meng were both unwell this week. Important to send minutes for this meeting to them.

Notes: 
    * Augmentation - boost performance on the fish part dataset. 
        * Time-shift, shift data backwards and forward, to get time-invariant generalized model (may not work well).
        * Impute data, combine existing samples, add noise, etc... 
    * Worked on CNN from ENGR489 for classification task.
        * Issues with keras and sklearn libraries, stratified cross-fold validation and one hot encoding don't play nice together. 
        * CNNs, we use 1D convolution and pooling layers on time-series data. 
        * Existing ML + GC literature also use CNN for classification and regression tasks. 
        * These models are powerful for extracting features in spaces with local connectivity. 
        * Aim to use EC to perform neural architecture search for CNN hyperparamters - these differ for each dataset. 
    * Both EC and Bayesian Optimization approaches work for neural architecture search. 
        * However, EC has more interpretable results, e.g. a genetic algorithm produces an explainable tree. 
        * Neural networks are black-box and esoteric, we understand how (i.e. back-prop, SGD), but not why? 
        * EC produces simpler representations, that can be prodded with domain expertise.
    * Important to read heaps for first few months of PHD. 
        * Take original notes that can contribute toward a backgroup chapter of my proposal. 
        * Get an idea of what has been done, and what I want to do. 
        * Still reading psychology textbook on memory and the brain to establish conceptual framework for learning.
     
2022-03-31 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Fangfang Zhang, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Fangfang suggested a video called the Big Reset 2.0 https://www.youtube.com/watch?v=-ePZ7OdY-Dw

Notes: 
    * Reinforcment learning for Robotic Arms. 
    * Deep blue beat Kasparov, but no AI can set up the chess board, a 6 year old can do that. 
    * Hugh Herr designed his own AI legs https://www.youtube.com/watch?v=CDsNZJTWw0w
        * AI prosthesis is cost prohibitive for the masses, but may work with diffusion of innovation in future. 
        * Prosthesis can up upgraded over time, biological body parts cannot, hardware/software updates for legs. 
    * Fake news - Jon Stewart said MSM has more trackers than ANY other media (adult entertainment, torrent sites, social media included).
    * Chomskey, MSMs job is to sell the educated privelaged wealthy elites as an audience to the corporations advertising. 
    * AI algorithms - social media, fake news, incentives. 
    * AI autonomous warfare proliferation - we need to ban slaughter bots https://www.youtube.com/watch?v=pOv_9DNoDRY
    * AI used for traffic management, screen-time punishment - pick up phone at cafe and pay the bill. 
    * RoboMaster - robot warfare, mechatronics, AI - physical robot warfare as a game/competition. 
    * Cosmo - Boris Sofman https://www.youtube.com/watch?v=U_AREIyd0Fc 
    * Narrow-AI and no free lunch problem - AI is good at solving very specific tasks, but not general intelligence. 
    * I have an industry project, that has real-world applications in a factory settings - i.e. reduce bycatch and maximize efficiency of food processing for fish. 

2022-04-01 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Andrew Lenson, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Meng and Bing were unwell, so Yi chaired the research group meeting. 

Notes: 
    * Bach (my supervisors) first day lecturing for COMP102. 
    * Me: I got 98% accuracy on the fish species dataset using a 1D CNN.
    * Shorter meeting, workshop cancelled, due to Meng being unwell.  

2022-04-04 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Ciaran King, Bastiaan Kleign, Jesse Wood, et al. 

Ciaran King was gave a talk on "Experiences using Github Copilot".
    * Understands the context of code, can make abstractions for helper methods. 
    * Can write documentation for codebases.
    * Not software "correct" code, but (likely) the code we were going to write. 
    * Can write tests for codebases with very little leading. 
Daniel Braithwaite on "Fixed Neural Network for Stenography"
    * Hide messages in adversarial neural network. 
    * Pre-trained stenograph, results in non-zero error, we need perfect reconstruction for encryption.
    * Face anonymization, post a persons face online, then regenerate the face, but encrypt the private face. 
    * This lets friends anonmyously share images with their face online, without revealing their identity.
Bastian - contractivity of neural networks.  
    * Signal processing worries about getting non-stable linear filters for signals. 
Jesse Wood 
    * Evaluating Large Language Models Trained on Code https://arxiv.org/abs/2107.03374
        * 70% accuracy for basic DSA problems. 
        * Can't solve more difficult problems - doesn't optimize solutions for performance. 
        * CoPilot outperforms other state-of-the-art NLP code generation models. 
        * Requires "fine-tuning", supervised human intervention to hint towards correct answer. 
    * Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions https://arxiv.org/abs/2108.09293
        * 40% of code written with CoPilot has cybersecurity vulnerabilities.
        * CodeQL and other static analysis tools used to define the security of the code.
        * Security is a shifting landscape, WannaCry, Log4J - zero days kept secret by intelligence agencies. 
        * This is true of all code, the training data was written by humans. 
        * Potential vulnerability for future attacks if hackers know open-source repos are training data.  
        * Don't treat copilot as a "glass cannon", it doesn't deserve a false sense of security.

2022-04-07 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bing Xue, Jesse Wood.

Notes:
    * Use an existing neural network architecture search algorithm - application analysis. 
    * Callaghan may have extra data work with - arrange a meeting with Daniel. 
    * Pre-traning, tranfer learning, NIST dataset for GC refraction index. 
    * Look at existing proposals, get an idea for mine - possible to submit proposal early. 
    * State-of-the-art, is 50-50 whether it works or is a bust - good to have a backup based in existing literature. 
    * Pareto front with tradeoff between complexity and accuracy. 
    * Proposal does not lock me into using a particular method (i.e. SVM, EC, PSO). 
    * Idea: make sure students have a decent grasp of the field before conducting their own research, if not then read more.
    * Later try out ideas in the proposal, and see if they work. If they don't change tact.

2022-04-07 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Qi Chen, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Qi Chen showed a talk "Artificial Intelligence: A Guide for Thinking Humans" by Melanie Mitchell from https://www.youtube.com/watch?v=NMUqvhuDZtQ

Notes:
    * Shannon, Simon, Minsky - all though AGI was 15 years off in their own time. 
    * Andrew Ng - AI is the new electricity. 
    * Elon Musk - nobody would listen - https://www.youtube.com/watch?v=4RMKLyaqh_8 
    * Deep learning brought back the hype for AGI. 
    * "An Anarchy of Methods" - Joel Lehman 2014. 
    * AI, Machine Learning, Deep Learning, onotologies of fields and their popularity change over time. 
    * Deep learning looks at AI as an aritficial brain - enter the Artificial Neural Network (ANN) - the connectionists.
    * CNN based on the limited understanding of the human brain in 1950s neuroscience. 
    * Facebook used CNN AI for facial recognition when a user uploads a photo. 
    * ImageNet is a famous supvervised classification task that was generated through crowdfunding internet "slave" labour. 
    * Famous 94% result for image classification has a sample size of PhD student (Andrew Kaparthy) - the fake news embellished the story. 
    * Self-driving, stopped fire truck on the highway, the long tail of AI, edge cases. 
    * Adversarial attacks on neural networks, crack networks to make wrong predictions based off of their flaws. 
        * "Intruiging properties of Neural Networks" https://arxiv.org/abs/1312.6199
    * Trick self-driving cars into driving through stop signs with stickers, that make it think it is a speed limit sign. 
    * "I wonder whether or not AI will every crash the barrier of meaning." - Glen Carlo Rote 1988. 
    * "common-sense" machine learning, WinnaGap NLP problem. 
    * DARPA - competition to design a machine with the common-sense of an 18 month old baby. 

2022-04-08 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Harith Al-Sahaf, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

General: 
    * University drops their vaccine mandate https://www.wgtn.ac.nz/covid-19/settings-and-mandates/removal-of-vaccine-mandate 
    * EvoStar conference is coming up soon - April 20-22nd, Spain Madrid http://www.evostar.org/2022/eurogp/
    * Me: 
        * CNN performs relatively well on fish part dataset with manually tuned hyper-parameters. 
        * Also, mentioned MegaSYN annecdote about adversarial attacks on machine learning models for protiens to manufacture deadly nerve toxins.

Harith Al-Sahaf gave a talk on Malware Analysis https://www.al-sahaf.com/harith/ 

Notes: 
    * Malware analysis determines the functionality, origin and potential impact of a given malware. 
    * Applying EC techniques to malware anaylsis. 
    * Harith and the university have a lot of publications in this area https://www.al-sahaf.com/harith/publications.html
    * Siemese neural networks used to identify an unknown instance to a known malware for similarity. 
    * Yann LeCunn invented the idea for Siemese neural networks in the 1980s. 

2022-04-11 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-5pm , **Attendees:** Madhurjya Dev Choudhury, Bastiaan Kleign, Jesse Wood et al.

Madhurjya gave a talk on "Time Series analysis for Machine Health and Diagnosis". 

Notes:
    * "Image-<MUFFLED> tranlation with Conditional Adversarial Networks". 
    * pix2pix 
    * Nuisance parameters is any parameter which is not of immediate interest, but must be accounted for in those parameters which are of interest. 

2022-04-18 - EvoStar #1
~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Madrid, **Time**: Monday 7pm - 6 am, **Attendees:** Jesse Wood, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al.

More information can be found on the full conference program - https://easychair.org/smart-program/Evo2022/

Gabriella gave a keynote address on "Illuminating Computational Search Spaces" https://www.youtube.com/watch?v=EyynDbXnwic

Notes: 
    * Visualisation, explanation, informed configuration. 
    * Complex networks, Local Optima Networks (LON), Search Trajectory Networks (STNs). 
    * Graphs exist since 1800s with Eulers famous bridge problem. 
    * Networks coming from different systems share topological structure. 
    * Force-directed graph layout algorithm, borrows ideas from physics (i.e. simulated annealing). 
    * Difference between Euclidian distance and traversal path distance between nodes (see Andrew's manifold talk from 2022-03-25 - ECRG)
    * Fitness landscpaes f(S,N,F) , search space (S), n<unreadable> (N), fitness (F).
    * Funel - local optima in a course grain structure that can minimize energy. 
    * Local optima Network, nodes - local optima from hill climbing heuristic, edges - transition between local optima. 
    * Number partitioner, given a random set of numbers find a partition at value k, such that the two disjoint sets have equal sums. 
    * Map fitness landscape as a graph, compress the representation, to get an explainable visualisation for a funnel. 
    * Travelling Salesman Problem (TSP) - apply pertubations to existing solutions to find new solutions in the fitness landscape. 
    * CMLON, compress a Local Optima Network into a smaller representation that is easier to understand. 
    * Genetic improvement to CMLON. 
    * STN, allow for representative solujtions. 

Christian Raymond gave a talk on "Multi-objective Genetic Programming for Symbolic Regression with the Adaptive Splines Representation" 

Notes: 
    * Overfitting if a difficult problem for GP, because of flexibility of representation. 
    * Difficult to regularize overfitting in GP. 
    * Parisomony pressure, population distribution control, order of non-linearity. 
    * Limitations: structural complexity has minimal correlation with generalization. 
    * Estimate complexity of a model by estimating a model's behaviour over a subset of training space. 
    * Limitations: difficult to reliably estimate, complexity vs error is a trade-off. 
    * Semi structured representation - a solution to both limitations above. 
    * Spline: defined by multipled low degree k (cubic) polynomials smoothly. 
    * GG-AWS-PP - Adaptive wieght Splines with parsimony Pressure. 
    * Apply multi-objective optimisation, for loss and complexity objectives, both considered. 
    * Training paretor fronts, 2D rperesntation of fintess landscape between two objectives. 
    * Creates interpretable representations which are easier to understand than genetic algorithms. 
    * Works well on low to medium dimension feature sets. 

Another talk on "Morphologicl development of Robots [...]" 

Notes: 
    * Development of agents, aging, is beneficial for generating complex agents. 
    * Voxel based soft robots (VBS). 
    * Aggregates of soft cubes (voxels). 
    * Neural networks for voxel controllers, a net for each voxel. 
    * New voxels are added to the model over time. 
    * This is a scheduling function, that must alloclate the correct time to increase the complexity of the model. 
    * <unlegible> based morphology representation. 
    * Different development schedules. Early development, uniform development, no development. 
    * Early development seems more beneficial than uniform development, the artificial mimics the biological. 
    * No development, shows large deviation in the results, fuzzy accuracy. 
  
"A new evoltionary algorithm based home monitoring device for Parkinsons Dyskinesei"

Notes: 
    * AUC based fitness. 
    * Adaptive size fitness <unlegible> - allocate fitness absed on a subset of training data. 
    * Aaply different representation width lengths using GP to see [...] (coffee break for me)

Zhixing gave a best paper nominee talk "An investigation of Multi-task Linear Genetic Programming for Dynamic Job Shop Scheduling"

Notes: 
    * Job shop scheduling problem. 
    * Complicated dynamic NP-hard combinatorial problem. 
    * Make decisions based on imperfect information. Instead, we use heuristics to decide schedule. 
    * We can't rely on a single heuristic alone, dynamic environment means we must change heuristics in real-time. 
    * Hyper heuristic - a search mechanism to find a heuristic selecting model. 
    * GP has good interpretability, a tree can easily be understood by humans. 
    * Seasonal variance - more demand for ice-cream in summer for an ice cream factory. 
    * Multi-task, conflicting or tradeoff between goals of different stakeholders. 
    * Multi-task model - one ring to rule them all - one model that can balance multiple tasks at once. 
    * Linear gentic programiing, register based instructions, creates directed acyclic graph (DAG). 
    * DAG > Tree; can use diffrent topological structures to perform cross over for DAG, tree reperesentation is limiting. 
    * Operators: linear crossover, macro mutation, micro mutation. 
    * Multi-population based genetic programming (GP) - sub populations that develop in isolation with crossover (migration) allowed. 
    * homogenous/heterogenous - diff-same / same-diff - utilization/objective functions. 
    * M^2GP does not perform the same as LGp methods, M^2GP (tree based) state-of-the-art does better. 
    * Likely because: (1) too large variation step sive, (2) ineffective initializaiton strategy. 
    * One and multi-objective population methods have similar performance. 
    * Graph based crossover is a <unlegible> genetic oeprator 
    * But not used in this work, but is a research direction in ECRG. 

David Wittenberg gave a talk on "Using a denoising autoencover genetic programming to Control Exploration and Explotitation in Search"

Notes: 
    * Capture relevant properties of parent population in a latent representation. 
    * Model (auto-encoder) is trained to reconstruct the input. 
    * Problem: don't want to learn the identity function. 
    * Solution: denoise (slightly mutate) parent to avoid overfitting anf force the auto-encoder to generalize. 
    * Level of corruption can be used to control the exploration and exploitation of genetic algorithm. 
    * Paper explores this idea, we want a latent representation that is a lower resolution to let the model generalize better. 
    * Subtree mutation: can't control corruption. 
    * They propose Levenshtein edit, a genetic operator for mutation on the representation string. 
    * Convert tree into infix string reperesentation - then perform mutation operations on that string. 
    * Levenshtein edit, insert, delete, mutate - with an edit percentage (this determines level of corruption). 
    * The  stronger the corruption, the shtonger the exploration. 

Nicholas Fontbonne gave talk "Coperatative Co-Evolution and Adaptve Tree Composition for a Multi-Rover Resources Allocation Problem"

Notes: 
    * Multi-agent - agents act indepdentdenlt. 
    * Competetive, mixed, co-operative sum games from game theory. 
    * Zero-sum, shared-sum, shared-fitness agents. 
    * In cooperative case, all information about the individual is lost (collectivist idealogies "the greater good"). 
    * A shared fitness that promotes social welfare is not a good learning signal. 
    * Marginal contribution - contribution of an individual agent to the team. 
    * Evolutionary algorithm for multi-agent problems. 
    * issues: stuck in local optima, high computation cost. 
    * solution: grouping mechanism. 
    * reousrce seelction problem: each resource has a satisfcation score. 
    * k is the group size for the grouping mechanism. 
    * Co-operative co-veolutionary algorithm for adhoc autonmous agents. 
    * Efficient anytime larning without apriori knowledge of the problem. 

2022-04-21 - Weekly
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Wednesay 12pm-1pm , **Attendees:** Bing Xue, Bach Hoai Nguyen, Mengjie Zhang, Jesse Wood.

Notes:
    * EvoStar conference yesterday, not much work for covid isolation week. 
    * Demelza is giving a talk tongight for EvoStar. 
    * Likely to attend EuroGP in-person next year. 
    * EvoCNN - encodes basic CNN components. 
    * Apply GP to classifier problem directly. 
    * Try several techniques (initially). 
    * Also later what new tasks Daniel may want.  

2022-04-21 - EvoStar #2 
~~~~~~~~~~~~~~~~~~~~~~
**Location:** Madrid, **Time**: Tuesday 7pm - 6 am, **Attendees:** Jesse Wood, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al.

More information can be found on the full conference program - https://easychair.org/smart-program/Evo2022/

Demelza gave a talk on "Genetic Algorithm for Automated Spectral Pre-processing in Nutrient Assessment"

Notes:
    * Rapid collection, non-destructive, construct a model. 
    * Partial Least Squares Regression (PLSR). 
    * Spectral data is easy to collect, but the pre-procsing is a bottleneck. 
    * PLSR tranlsates the feature space into latent varaibles. 
    * Spectral pre-processing, remove noise and redundant intensity values. 
    * Manual process for pre[rpcessing spectra is laborious and lacks standardization. 
    * Automate pre-processing for choosing appropriate techniques and their order of application. 
    * Representation, a two chromosome structure that encodes objectives for: 
    * Fitness function: combination of MSE and number of features. 
    * GA-PLSR-PPS perform btter for most cases and has a smaller standard devition for IR and Raman. 

Zakaria Dehi gave a talk on "A machine learning based approach for Economics-Tailored Applications: The Spanish Case Study". 

Notes: 
    * Use AI for dynamic budget allocations in governmental bodies. 
    * (1) ETL, (2) Profiling, (3) Predicting. 
    * ETL - gathered 30,000 economic features, and identified 50 types of related economic metrics. 
    * Pre-processing required to make the building blocks for the ETL model. 
    * Different metrics can have the same profiles for all cities (i.e. unemployment/retirement).
    * Eliminate the redudnat metrics (duplicate class profiles) - for concise metric space. 
    * DCGA-kMeans - unsupervised clustering algorithm to determine profiles (k). 
    * Profiling deals with indentifying groups og cities based on an economic profile dervied from metrics. 
    * Prediction: Long Short-Term Memory (LSTM) - recurrent neural network. 
    * This unlocks the time series component of ecnomic data, persistance of time observed by the model. 
    * The ideal number of profiles for most metrics was 3. 
    * Massive feature and metric reduction - creates meaninful data through feature construction. 
    * Can decrease complexity of number of cases when officials must make complex politicy decisions based on near-infinite combinatorial search spaces. 
  
Daniel Lopez gave a talk on "EvoDesigner: Towards Aiding Graphic Design"

Notes: 
    * Readability, balance, innovation, style - all measured by Mean Squared Error (MSE). 
    * Adobe plugin to produce variations of a design based on rough input using evolutionary computation. 
    * Intitial input is a rought sketch, that paints an idea of a possible layout. 
    * Evolutionary engine creates a good design graphic based on that initial input. 
    * Then the engine generates variations of that good design using evolutionary computation. 
    * Still needs work, but offered a good tool for aiding ideation in graphic design. 

Julia Reuter gave a talk on "Genetic Programming-Based Inverse-Kinematics for Robotic Manipulators... <unlegible>".

Notes: 
    * Develop prototypical solution for Inverse Kinematics (IK) problems. 
    * We want closed-form solutions, apply to non-standard SCARA, with explainable representations (equations). 
    * Kuke koubot - was the brand of SCARA they ran their simulations on. 
    * Different objective functions + co-creation / co-evolutionary approaches.
    * IK-CCGP was compared to an ANN. It did better than the ANN. 
    * Co-evoltuion for the two-joints, performed better than for 3 (a more complex task). 
    * The equations were tested on simlated - not real world - robotic arms. 
    * But these equations can be verified later, by applying them to real-wrold systems, and testing for collisions/singularities. 
  
Partick Indri gave a talk on "One-shot learning of Ensembles of Temporal Cage Fomrulaes for Anomaly Detection in Cyber-Physical Systems".

Notes: 
    * Monitor behaviour of CPS, e.g. AV, power plant, medical monitoring, security systems, smart house. 
    * CPS are dynamic - need to quantify the system in terms of time. 
    * Signal Time Logic (STL), is an expression grammer for time based operations. 
    * GP oeprators to construct STL for optimized performiang at controlling a CPS. 
    * One-shot algorithm, GP is population based, we can use the population to build ensembles.
    * Water treatment CPS was used as dataset for training/test. 
    * Their method performs well when comapred to other state-of-the-art methods. 
    * One-shot G3P achieves more complext formulates than the standard G3P. 
    * One-shot approach cpatures wave temporal operators better than standard G3P.
    * It can learn more complext models, that include "time-based" operators (i.e. STL). 
  
Gloria Pietropoli gave a talk on "Combining Geometric Semantic GP with Gradient-descent Optimisation"

Notes: 
    * Geometric smentaic metric programiing (GSCP) is a well known variant of genetic programming (GP), 
    * GSCP used recombination and mutation operators that have a clear semantic effect. 
    * Combine GSCP with Adam Optimizer. 
    * GP, we can presresent an individual as a point a real n-dimensioanl semantic space. 
    * Geometric smenatic operators: 
        * Geometric semantic crossover (GSC). 
        * Geometric semantic mutation (GSM). 
    * Adapate Moment estimation (Adam, kingma2014adam), is a first order gradient-based optimization of stochastic functions. 
    * GSCP makes a good jump in the solution space, then adam can refine a candidate solution. 
    * Two approaches: (1) one-step GSCP then one-step ADAM (HYB-GSCP), or, (2) full GSCP then full ADAM (HCH-GSCP).
    * HYB-HSCP does better than HCH-GSCP. 

Dominik Sobaria gave a talk on "Program Synthesis with Genetic Programming: The Influence of Batch Size"

Notes: 
    * Program synthesis with genetic programming. 
    * Anayluse perfromance and generalization ability of programs generated by GP. 
    * Batch size effects programming synthesis. 
  
2022-04-22 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Notes:
    * Small editing (layout) is allowed when responding to referee feedback on a paper. 
    * Me: "I worked on catching COVID his week"
    * New datasets from Daniel, he proposed some new research objectives https://mail.google.com/mail/u/0/?hl=en/#inbox/FMfcgzGpFWTMDnsjKxswvdtWdrhKzKdH 
  
2022-04-22 - Proposal Workshop 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Mengjie Zhang, Jesse Wood et al.

We had a proposal workshop hosted by my former supervisor Mengjie Zhang. 

Notes: 
    * First (of many) proposal workshops about preparing this document. 
    * Target adueince for workshops are first year PhD students (me). 
    * PhD is a great way to go for a very good job later in life. 
    * Don't have to do theoretical work, but focus on application, key point is to make a major contribution to the field. 
    * First year of PhD, we are a student not a candidate. A candidate has passed their proposal. 
    * Bad PhDs can be converted into a masters degree - this happens if the proposal is very bad (try to avoid). 
    * Read 50-100 papers in order to get a good grounding for a PhD proposal. 
    * Some people do not have the capability to do a PhD, they can propose, but must demostrate their capability with preliminary work. 
    * Structure: 
       1. Introduction 
       2. Literature survey (read) + 
       3. preliminary work (code) + 
       4. contributiosn (major) / miletones / thesis outline / resources. 
    * Overall goal: a single over-arching scientific/engineering argument to unify my PhD stydies as one body of work. 
    * 100,000 words (is a lot of words) is the expectation for a PhD thesis. 
    * Minimize dependencies - encourage modularity for research objectives. 
    * Coherence is very important for choosing research objectives. 
    * Literature review should cover most recent work and domain specific (biology/chemistry) papers. 
    * Honours work can't be counted as preliminary work (but can be references as a citation). 
    * Not required to publish - but encouraged to do so (prevents concurrent thinking issues later down the line). 

2022-04-21 - EvoStar #3
~~~~~~~~~~~~~~~~~~~~~~
**Location:** Madrid, **Time**: Tuesday 9.30pm - 11.30pm, **Attendees:** Jesse Wood, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al.

The final day of the conference, involved a plenary talk and prize giving ceremonies. 
More information can be found on the full conference program - https://easychair.org/smart-program/Evo2022/

Pablo Geruas gave a EvoMUSART talk on "Evolutionary construction of Stories that Combine Several Story Lines"

Notes: 
    * Star Wars used as an example during the talk for storylines (kudos :D). 
    * Use evolutionary algorithms to evolve <unlegible> <unlegible> plot lines for a story. 
    * Made up of 20 popular plot lines from a textbook, assign a plot line to each character. 
    * Tricky to come up with a genetic representation for a story plotline.
    * Genetic representation must be manipulatable with genetic operators. 
    * Difficult to design an objective/fitness function. 
    * Fitness: validity - continuity of life and death, each character falls in love once (simplification). 
    * Fitness function averages over all validity metrics to evluate fitness. 
    * Eolvutionary approach good for creating multi-plotline stories with semantically valid discourse. 
    * Speaker gave a similar application e.g. a robot that generates plot line scripts for a leage of legends game. 
    * https://nil.fdi.ucm.ec/ 

Pedro Larrange gave the plenary talk on "Estimator of Distribution Algorithms In Machine Learning" 

Notes: 
    * Machine learning is a large focus of artificial intelligence nowadays. 
    * Construct a model from data, perform (non)-parametric optimization. 
    * Estimation of Distribution Algorithm (EDA). 
    * Bayesian Networks  (DAG + CPT), Directed Acyclic Graph (DAG), Conditional Probability Table (CPT).
    * Feature subset selection, Filter - only consider features, Wrapper - evaluate performance at machine learning task. 
    * Classification - a greedy algorith. 
    * Artifical Neural Network (ANN) - (Baluja 1995). 
    * Logistic regresion (Roles et al 2008) - led to intrepretable results for complex models. 
    * AdaBoost (Cagninin et al. 2018) - aggregated voting system between classifiers. 
    * Hierarchcial clustering (Fan 2019). 
    * k-Means (Forgy 1965), most popular culstering algorithnm, centroids shiftwed with hill climbing strategy. 
    * Reinfrocment learning (Honda and Nishive 2008) - relies on conditional markov fields. 
    * EDAs have not yet been applied to Support Vector Machines (SVM) - possible future work here (for me). 

2022-04-26 - Thesis Workshop 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Kirsten Reid, Jesse Wood et al.

Kirsten Reid, from learning support, hosted a Thesis Workshop for the Faculty of Graduate Research (FGR). 

Notes: 
    * Shape/formate appropriate for the field - i.e. Engineering / Artifical Intelligence. 
    * Linking ideas between chapters - see About Face 3 for great example. 
    * (Sub)headings are used effectively - maximum three levels of indentation for understandability by reader. 
    * Topic sentence for each new paragraph. 
    * If needed - glossary, acronyms, abbreviations - up front. 
    * Reminder of key concepts when needed, jog the readers memory, guide the reader along (a thesis is long). 
    * "Reader-friendly", avoid jargon, use plain text. 
    * Sentence length should vary to avoid monontonous tone (see https://bit.ly/38mTmAE)
    * Avoid too many nominisations - e.g. "we decided" rather than "we came to the decision" - focus on action verbs over their nouns. 
    * Don't have front-ended setnences, cut the wheat from the chaff, 1/3 of writing can be removed for brevity usually. 
    * Make appointment with Student Learning - limited to 50 minutes. 
    * Sentence cohesion worksop coming up soon. 
    * Kirsten happy to reply to Emails for further advice on this workshop. 
    * Three voices: research, data, researcher. 
    * From FGR: 4 workshops coming up for PhD students in May.

2022-04-28 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO 352, Cotton Building, **Time**: Monday 12pm-1pm , **Attendees:** Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Notes: 
    * Can claim GC dataset research as preliminary work for the REIMS data in my proposal. 
    * Transfer models from GC to REIMS data (likely) - this supports the preliminary claim. 
    * Pre-training on NIST GC refraction index data - needs a parametric (neural network) model for this technique. 

TODO: 
    * [x] Apply Genetic Programming (GP) data to GC data. 
    * [ ] Apply EvoCNN to GC data. 

2022-04-28 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Jesse Wood (me) showed a talk "Telsa AI Day (supercut)". 

Notes: 
    * Disclaimer: I submitted this talk request on the 21st of April (7 days ago) - this is not political, just a coincidence. 
    * "Iron Man in Real life" - a comment from the Zoom chat. 
    * If you are interested in another high level summary:
    * Lex Fridman AI Day (summary by MIT researcher) - https://www.youtube.com/watch?v=ABbDB6xri8o
    * Here is the full video, it includes references to academic papers:
    * Full AI Day (3 hours long) - https://www.youtube.com/watch?v=j0z4FweCy4M
    * Supervisor liked my running commentary and links to further watching. 

2022-04-22 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

General Notes:
    * Zhixing won best paper for EuroGP. 
    * Shaolin had a TEVC paper revision. 
    * Vincent and Tao had a paper accepted by CEC. 

Talk from Xioying Sharon Gee on "Text Representation" 

Notes:
    * Inverse document frequency - use to elminate words that are too common - e.g. "the". 
    * Word emeddings, analyse similarity (relateness) with PCA - reduce to lower dimensional space (2d / 3d). 
    * CNN, Transformers, Attention, BERT, Pre-training. 
    * Pre-training of deep bidirectional transformers for language understading (BERT). 
    * Word masking, pre-training, next sentence prediction (NSP). 

2022-04-02 - Research Workshop 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 11pm-1pm , **Attendees:** Jesse Wood 

FGR hosted a workshop "How do I publish and disseminate my research". 

Notes: 
    * https://scopus.com - see citation statistics for papers/journals - useful metrics to judge quality. 
    * Can see stats for a journal, and comapre this to other jounrals, to assess the credibility. 
    * These tools are useful for making/measuring goals as a researcher. 
    * http://login.webofknowledge.com/ -  useful tool for mainly STEM disciplines. 
    * Eslever Journal Finder - provices acceptance nodes. turn around times - useful for finding which journal is appropriate. 
    * www2.cabells.com/jouranlytics - see a journal's publishing frequency, may be anually, this is a long wait. 
    * Consequences: 
        * Who reads it? 
        * Does it publish articles like yours? 
        * Does your style match? 
        * Would you need to change to submit? 
        * Peer-reviwed? 
        * Time to publish? 
        * Substantial paper. 
        * Tolerable rejection rate. 
        * Preferred type of journal. 
        * Solid reputation/metrics. 
        * Many articles a year. 
    * Keywords are good for SEO, they ensure the discoverability of work, and extend your audience. 
    * Rejection happens, may provide feedback (may not). 
    * Query letter - send an abstract to a journal to test the waters - see if a paper is appropriate. 


2022-04-11 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-5pm , **Attendees:** Mathew O'Connor, Bastiaan Kleign, Jesse Wood et al.

Matt O'Connoer gave a talk "Unsampling Artifacts in Neural Audio Synthesis" https://ieeexplore.ieee.org/abstract/document/9414913

Notes: 
    * Imrpove the quality of audio using a neural network approach. 
    * CNN is for images, but we need an architecture for audio specifically - this must handle audio specific problems (e.g. time, harmonics). 
    * Convolution (collapse), transposed (expand). 
    * Transposed convolutions are widely used. 
    * Upsampling methods: 
        * Stretch (insert zeros) 
        * NN (nearest neighbours)
        * Linear 
    * The unsampling algorithm leaves artifacts in the output sample. 
    * Spectral replicas emerge when sampling/discretizing the signal. 
    * All up sampling methods sound very similar - in the human audible range. 
    * NN, no artifacts, but frequency filter at zero frequency. 
    * Out of distribution test, shows major artifacts, since it wasn't in training. 
    
2022-05-05 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO 352, Cotton Building, **Time**: Monday 12pm-1pm , **Attendees:** Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Notes: 
    * Reading this week - Evolutionary ML Survey (60% done). 
    * Apply GP to the GC fish dataset. 
    * Later consider the time domain: terminal set, symbolic regression. 
    * GP Hello World! Try find a tutorial for this. 
    * DEAP does not have "Elitism" - I will need to implement my own.
    * Elitism:
        * Keep the top solutions between generations. 
        * Ensures performance can't decrease. 
        * Does not guarantee performance will increase. 
    * Future work - transfer learning can be: 
        * Paramters 
        * Model 
        * Feature (selected/constructed)
    * Terminology; domain adaptation, domain generalisation. 
    * N.B. I shoudl record my tutor meetings on zoom (even if they are in person) to make use of Panopto's free transcription software while I am a staff member. 

2022-05-05 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Peng Wang, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Peng Wang proposed a talk from Prof. Zhihua Zhou on "From Pure Learning to Learning+Reasoning" https://www.youtube.com/watch?v=LAvRDCcXCMc 

Notes: 
    * Most machine learning techniques rely on large database of labelled training data. 
    * We can leverage unlablled data, to improve performance on labelled data. 
    * Self-learning, use a classifier to apply psuedo classes to unlabelled data. 
    * SETRED 2005 - data editing, cleans up self learning. 
    * Active learning (AL); uses an oracle to query (label) unlabelled data, rely on minimizing queries to oracle (this requires human supervision). 
    * Representative AL approach - informative/representative. 
    * AL requires human-in-the-loop. 
    * Semi-supervised learning (SSL) - see Lex podcast for more details https://www.youtube.com/watch?v=FUS6ceIvUnI 
    * Indepedndent and Identically Distributed (IID). 
    * Semi-supervised SVMs (S3VMs) (Zemma 201u6, zemmal2016adaptative).
    * Using inlabeleld data to ensure the decision boundaries are drawn through low density areas. 
    * Tri-training approach, three learners, tha can teach eacother, and perform ensemble learning. 
    * Ensemble learning - uses multiple models and combines them to make a prediction. 
    * Holy grail: machine learning + logical reasoning. 
    * Probabilistic Logic Programming (PLP) - heavy-reasoning light-learning.  
    * Statistical Relation Learning (SRL) - light-reasoning heavy-learning.
    * Proposal: abductive learning: 
        * Deductive 
        * Inductive 
        * Abductive - Inversly embed deductive reasoning into inductive reasoning. 
    * Knowledge Base (KB), a series of first-order logic predicates. 
    * Instance --> Psuedo lables --> Psuedo groundings --> KB. 
    * Optimize minimzed inconsistency in the system. 
    * ABL does not rely on ground-truth labels. 
    * SSL for court sentences in China, similar to Andrews work (see "2022-03-25 - ECRG" above). 

2022-05-06 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Notes:
    * Celebtration for AI group since Bastiaan recieved his RSNZ Fellowship - Wednesday 18th May from 12 pm - 1 pm in CO3250. 
    * Hayden, camera-ready (?) paper for Gecko (?) workshop. 
    * Candidate Development Plan (CDP) is due by the end of May - this is due every 6 months (see email https://mail.google.com/mail/u/0/#search/CDP/FMfcgzGpFgvPXpqBwmBtBvqBZPxxQFtB).
    * University policy for thesis students does not require use to publish any papers. 
    * STEM has a publishing-forward culture when compared to other fields (e.g. law or humanities). 
    * Evolutionary Algorithm:  
        * Initialisation
        * Cycle 
            1. Evaluation. 
            2. Selection. 
            3. Reproduction. 
            4. Repeat 1-3 until termination condition. 
    * Meng has not seen anyone miss genetic operators, but a fair number of research papers ommit the "evaluation" and "selection" sections. 
    * DON'T OMMIT THESE SECTIONS! (Unless you have a very good reason not to). 
    * Ideally, have a nice flow diagram in a paper, to explain the training process for the model (this figure is a good use of space!!!). 

2022-05-09 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Ciaran King, Jesse Wood, Bastiaan Kleign, et al.

Ciaran King gave a talk on "Learning from Protein Structure with Geometric Vector Perceptrons" (Jing 2020, jing2020learning) https://openreview.net/forum?id=1YLJDvSx6J4 

Notes: 
    * Perverse incenstives for acadmiecs to over sell their work and a major reproducability crisis in deep learning. 
    * Graph nerual Networks can be used for protien folding. 
    * Equivariance to rotations - if the networks thinks the same instance rotates is a completely different structure, this is very inefficient. 
    * Instead we want rotation invariant representations for things like protiens. (Like we wan't time invariant representations for gas chromatography). 
    * Voxels are 3D pixels, these can be used to make a 3D representation of an instance, which then applies a 3D Convolutional Neural Network. 
    * We think that (1) message passing and (2) spatial convolution, are both well suited for different types of reasoning. 
    * In protein folding, their are chemical propoerties of protiens that simplify the combinatorial search space for the graphical neural network. 
    * This is similar to how the AI Feynman (Tegmark 2020, tegmark2020ai) used properties of physics equations to simplify symbolic regression. 
    * I would like to apply simplification using domain expertise in chemistry to my gas-chromatography and mass spectrometry data. 

2022-05-12 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Thursday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes: 
    * Audio-only private recordings of weekly meetings for study purposes is ok. 
    * Plant & Food collecting information on students (like myself) that have been working for them. 
    * Conferences: 
        * GECCO - Genetic Evolution Computation Conference. 
        * EvoStar (attended in 2022). 
        * IEEE CC - IEEE Congress of Evolutionary Computation. 

    * Camera ready? This is ready for print, a final version of the paper that has responded to feedback and been formatted for the journal. 
    * CDP - Candidate Development Plan, is due this month. Scope is for 6 months only. This is my first so I have no goals from previous CDP. 
    
TODO: 
    * [x] Send a draft CDP to my supervisors for feedback. 
    * [ ] Readings on Genetic Programming (GP)
        1. TranEtAl2015GPfsfc https://link.springer.com/article/10.1007/s12293-015-0173-y
        2. tran2019genetic https://www.sciencedirect.com/science/article/pii/S0031320319301815?dgcid=rss_sd_all
        3. tran2017new https://ieeexplore.ieee.org/document/7956226

2022-05-12 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Cuie Yang, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Cuie suggested the talk "You are what you read" https://www.youtube.com/watch?v=Du7qLsToW-o

Notes: 
    * Light reading, figure fishing - 80% comprehension. 
    * Skip to pictures like a kid, we are all kids at heart. 
    * Deep read, creative/critical reading.
    * Shallow reading is important, a paper has four parts.
        1. Title 
        2. Abstract 
        3. Introduction 
        4. Rest 
    * Quick reading and shallow learning is most important for the majority of readers. 
    * Reviwers are busy people, bathroom reviews, a lot of reviwed is based on superficial details. 
    * Notes - a format for notes from skim reads of a paper: 
         1. About 
         2. Problem 
         3. Interesting 
         4. New
         5. Neat
    * Two word titles: "Snappy sampling". 
    * Find collegaues to collaborate with and share knowledge. 
        * Give and take, time is a resource. 
        * what NOT to read? 
    * Future work: 
        1. Good ideas
        2. Improvements
        3. Applications/Extensions
        4. Your opinion
    * Come up with your own: 
        * Other domains 
        * Moon-shit
        * Related ideas

TODO: 
    * [ ] How to get your SIGGRAPH paper rejected https://www.kormushev.com/public/How_to_Get_Your_SIGGRAPH_Paper_Rejected-by_Jim_Kajiya.pdf

2022-05-13 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

General notes: 
    * Possible to ahve future meetings in person sson - waiting on university to update their covid policy. 
    * Bastiaan's celbration is next week. 
    * Meng asks who would like to go first at the start of each meeting. BE READY! FIFO 
    * me: GA Tutorial, Evolutionary ML Survey, 3x Papers to read. 
    * Journals are encouraging people to publish their work in interactive ways. See obersvable for visualization techniques. 
    
Junhong Zhao from another department gave a talk on "AI Effects (AIX) in Computer Graphics (CG)". 

Notes: 
    * Reconstructing reflection maps using a stacked CNN for Mixed Reality Rendering. 
    * Automates/improved re4ndering practises with good estimation for reflection maps. 
    * Gist: "Get accurate reflection maps on artificial objects in Augmented Reality Environments". 
    * Challenges: wide range of sensors/lens - people have different phones with different quality camersas. 
    * A robust neural network is needed to handle out of distribution and real world data. 
    * DLNet - "Adaptive Light Eximtation using Dynamic Filtering Terms". 

TODO: 
    * [x] CDP is due this Monday, for part I. (finished!)

2022-05-06 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Daniel Braithwaithe, Jesse Wood, Bastiaan Kleign, et al.

Daniel Braithwaithe gave a talk on "Estimating KL Divergence with Kernal Estimators". 

Notes: 
    * MINE - Mutual Information Neural Estimation. 
    * GANs suffer from mode collapse, due to a lack of diversity in GAN generators. 
    * MINE labels as model and loss are not typically convex, so convergence is not guaraneteed. 
    * Non-trivial to implement a MINE model.
    * Maxmimizing entropy is an intractable problem. Instead estimate thi for a GAN. 
    * Instead use KKLE (KL Divergence using Kernal Estimators), KKLE is convex. 
    
Maxwell clarke gave a talk on "Why Deep Learning Works". 

Notes: 
    * Zero Eigenvalue, parameters can be shifted without affecting the loss. 
    * Compress the area of the search space for efficiency. 
    * Networks which used generalisable representations are "simpler" than networks that don't. 
    * Occam's razor, or, 'simple as possible but no simpler' - Enstein. 

2022-05-26 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Thursday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes: 
    * Add proposal to the CDP goals. 
    * End of October - proposal due. 
    * Set 18th June for Australassian AI paper submission as soft deadline for my first paper. 
    * IP is 1/3 creators, 1/3 university, 1/3 commercial (note: still far better than previous industry experience). 
    * Code is not IP, but is copy-writable.

2022-05-26 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Cuie Yang, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Bach (my supverisor) suggested the talk "The Master Algorithm" from Pedro Domnigos. https://www.youtube.com/watch?v=B8J4uefCQMc 

Notes: 
    * Knowledge: evolution, experience, culture, computers. 
    * Each knowledge acquistion is an order of magnitude faster than the previous. 
    * 5 tribes of AI: Symbolists, Connectionists, Evolutionaries, Bayesians, Analogizers. 
    * Each tribute is related to a different field of study outside of machine learning. 
    * Each 5 algorithms have a master algorithm, a agneral method that with enough data can learn anything. 
    * Robotic machine that can perform biological experiments in drug discovery (see Lee Cronin Chemputer https://www.youtube.com/watch?v=ZecQ64l-gKM)
    * Google's famout "cat" network, was an ANN trained on Youtube videos, not surpisingly it became very good at recognizing images of cats. 
    * Koza took Evolutionaries one step fouther, by inventing genetic programming, representing a candidate solution in program semantics. 
    * Bayesians are the most fanatical of the tributes. Strict adherence to statistical inference through Bayes Theorum. 
    * As we see more evidence, probablites of certrain hypothesis will become more likely. 
    * Weakness of AI, is it cannot predict events that are not explicitly given in training, things that have never happened have P(A) = 0. 
    * Analogizers, Douglas Hoftstader, author of Godel Escher Bach. 
    * Vladimir Vapnik is the creator of Support Vector Machines (SVM). 
    * Kernal machines are the master algorithm for analogizers. 
    * A master algorithm, for general intelligence, would include all 5 tribes of AI. 
    * Pedro prposed the Markov Logic Network, objective function indepdendent. 
    * All AI fits the evolutionary paradigm; Evaluation, Selection, Reproduction, Fitness. 

2022-05-30 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Daniel Braithwaithe, Jesse Wood, Bastiaan Kleign, et al.

Bastiaan gave a talk on "Distributed Processing"

Notes: 
    * Knowledge required: graph theory, probability, convex optimisation, domain optimization. 
    * Parallel vs Distributed Processing. 
    * Dual Gradient Ascent. 
    * ADMM, PPMM. 
    * Parallel processing - fast as slowest node, central coordination. 
    * Distributed processing - no central coordination, any connected topology, gossip consensus algorithm. 
    * Gossip - take the mean of each group of nodes and communicate that between groups of nodes. 
    * Duual Gradient Ascent - requires strong duality (e.g. Slater's condition). 
    * Dual Gradient Ascent is useful for distributed processing. 
      * We assume f(x) is seperable, get dis<unlegible> 
      * Instead of global optimization, we optimize locally. 
    * Alternating Direction Method of Multipliers (ADMM). 
    * Lasso (L1) regularization is the most common predecessor to ADMM. 
    * ADMM converges much faster because it is quadratic. 
    * Fast enough, their are faster algorithms, but this is fast enough in signal processing. 
    * ADMM is a distributed MSE. 
    * Primal-Dual Method of Multipliers (PDMM). 
    * Lifting - add a new variable, then constrin them, this allows us to associate variables with models only. 
    * PDMMM was developed for solving decomposible optimisation problems in a distributed fashion. 
    * Note: traditional somewhat ad-hoc deviations, nicer alternative is is <unlegible> greater based. 

2022-06-02 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Christian Raymond, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

(Misc) Heiter Gomes is a new lecturer in the department. 
Christian Raymond proposed a talk "Aritificial Intelligence 1980s-2021 and Beyond" by Professor Jurgen Schimidhunter https://www.youtube.com/watch?v=pGftUCTqaGg&t=1s 

Notes: 
    * Long-short-term memory (LSTM) 
    * Recurrent Neural Network (RNN)
    * What is predicatbale is compossable [sic]
    * RNN requires unsupervised pre-training. 
    * LSTM does not require unsupervised pre-training - an improvement on the RNN. 
    * DanNET fast deep CNN based image processing revolution in 2011. 
    * Highway nets (May 2015) - first nerual network with over 100 layers, e.g. ResNET. 
    * DL networks suffer from the problem of vanishing and exploding gradients. 
    * LSTM used for google speech recognition, Amazon Alexa, (Samsung Bixby xD )
    * Compressed network search was the first RL to learn policies from video for a controller. 
    * Reinforcement learning LSTM, 2007-2010. 
    * 2019, deep mind bet Starcraft player. 
    * 2018, OpenAI Five, bet competitive players in Dota 2. 
    * World models + RL + controller - e.g. RoboCop AI soccer. 
    * Motivate controller to design experiments to improve the world model. 
    * 1990 Generation Adversarial Networks (GANs). 
    * nnaisense - the dawn of AI - his company. 
    * AI in 3D printing - additive manufacturing. 

2022-06-03 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

General Notes: 
    * Mass exodus of academics from university (Nature article). 
    * New PhD students have to be in New Zealand to study at VUW. 
    * Christian Raymond had a very intuitive video about AI history yesterday at FASLIP. 
    * Make sure simiarlity score on Turntin is less thean 25%, bearing in mind we can plagarize ourselves. 
    * Me: reading (lehman 2020, lehman2020surprising), writing my first draft of my paper for Australassian AI. 

Fangfan Zhang gave a presentation on "Responding to Reviewers"

Note:   
    * Peer review processing - some papers are accepted with major/minor reviews. 
    * AS long as a paer is accpeted, this is a success, in the peer-review process. 
    * R1, R2, R3, ... -> Revision 1, Revision 2, Revision 3, ...
    * It is common to have up to 4-5 revisions when submitting a paper to a journal. 
    * 7 is the maximum number of revisions within the ECRG group. 
    * Important to take the revision process very seriously, when publishing as an academic. 
    * When first receiving feedback, keep calm and carry on, read all feedback before responding. 
    * Respond to each point when replying to a reviwers comment. 
    * Can disagree with reviwers comments, but be professional about it, be resepctful. 
    * Make is easy for your reviwers! 
        * Write a cover letter. 
        * Copy text directly for short changes. 
        * Reference for larger sections. 
        * Use color to highlight changes. 
    * A good revisions would mean the reviewer does not have to re-read your original paper to accept the changes. 
    * Summary: 
        * Tale a break. 
        * Point-by-point response. 
        * Well reasoned arguments. 
        * Pay attention to details. 
        * Appreciate reviewers work. 
    * Papers need to make a contribution to science/journal.  

2022-06-09 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Thursday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes:
    * Print a bigger vesion of the paper for next time, fit to page setting. 
    * Meng uses emacs, Pondy used to use emacs for everything (i.e. email). 
    * GP paper on GC-fish data. 
    * GP - set max tree depth to 8. 
    * Protected division - don't divide by 0 -> NAN. 
    * Later, use more compelx GP variations. 
    * Australassian AI, papers need novelty. 

2022-06-02 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Quinglan Fan, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Quinglan Fan shared a talk on "The Extremes of Interpretability" https://tads.research.iastate.edu/dr-cynthia-rudin-extremes-interpretability-machine-learning-sparse-decision-trees-scoring-systems

Notes: 
    * Interpretable machine learnings models objey a domain-specific set of constraints. 
    * Needed for high-stakes or troubleshooting, important to have interpretable models for real-world problems. 
    * Wildfires CA, Google Air Quality, breezeomoter ML model failed. 
    * Glenn Rodriguiz, COMPAS score, AI that bas borken left a man in prison. 
    * Florida COMPAS data on recidivim (similar to Andrew Lenson's talk from 2022-03-25 - ECRG).
    * Blackbox model that is used in the criminal justice system. 
    * Correctional Offender Management Profiling for Alternative Sanctions (COMPAS). 
    * Very basic decision tree, corels, was able to outperform the COMPAS model. 
    * c4.5 and Cart are greedy top down algorithms that often overfit the training data. 
    * 1990s: non-greedy algorithm, and, statistiscians improve splitting criteria. 
    * Genetic Programming has been tried to create fully optimal decision trees. 
    * GODST 2020, fastest algorithm by 3 orders of magnitude. 
    * Dynamic programming/ brnach and band 
        * Eliminate duplication. 
        * no need to solve pure leaves. 
        * reduce search space by theorum: 
             * minimum support bound. 
             * onestep lookahead. 
    * Generalized and Scalable Optical Sparse Decition Tree (GODST)
    * Improved representation: 
        * store only the leaves. 
        * use bitvectors -> crazy fast. 
        * Extened computation (avoid repetitions) - caching 
        * Consolidation of repeated sub-problems. 
    * Results: 
        * Similar classification range. 
        * For custom lost, much better than greedy algos. 
        * Sparser than all heuristic models. 
        * orders of magnitude faster than next best model. 
    * Explaining deep NNs with saliency maps does not work. 
    * i.e. given an image of a dog, the salience map for a Husky and a Music Instrument both examine the same areas. 
    * "This Looks Like That", that forces a blackbox NN to be intereptable with a prototype layer. 
    * CUB-200, a staple computer vision dataset on birds. 
    * Even for black-box NN, we can have an interpretable model. 
    * Interpetable AAI algorithm for Breat Lesions (IAIA-BL). 
    * An AI, that used domain specific apporach of radoilogy, and presents the radiologist an interpretable model, 
    * They can understand the reasons behind decisions, be skeptical, and disagree or agree based on reason-based interpretable models. 

2022-06-10 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Qurret Ul Ain, Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Qurret Ul Ain gave a talk on "Artificial Intelligence to Diagnose Skin Cancer"

Notes: 
    * Early detection of cancer is fundamental in saving lbies. 
    * Moles are an example of Skin Cancer. 
    * Combine knowledge of dermetology + computer vision. 
    * A multi-disciplinary research project (like mine). 
    * Skin cancer can spread to the body, in later stages, it can reach the blood vessels, and lead to tumours. 
    * Skin cancer is a disprportionately high cuase of mortality in New Zealand - 4,000 diagnoses, 300 deaths per year. 
    * Causes: radiation, modles, high altitude, immune system, hereditary. 
    * Diagosis: 
        * A - Assymetry 
        * B - Border 
        * C - Colour
        * D - Diameter 
        * E - Evolving
    * Tool: punch biopsy (skin biopsy) - manually check a mole (very painful procedure). 
    * Computer diagnostics let a dermetologist determine if a punch biopsy is needed. 
    * Punch biopsy (painfuly) barrier of entry to diagnosis. 
    * This motivates computer vision techniques for detection. 
    * Multi-stage classification systems requrie significant expertise; segementation, remove hair, ambient lighting, rotation. 
    * Existing methods rely on gray-pixel computer vision, but colour is important too. 
    * Lens (camera) callbiration varries due to different resolutions of camera. 
    * Limitations of NNs, reduce all to 256x256 pixel images, this distorts the aspect ratio.
    * Hair removal - is this because of the training data? Because a dermatologist shaves a patients hair before scanning. 
    * Multi-tree GP has achieved better performance than single tree. 
    * Multi-tree is similar to multi-cellular organism from biology. 
    * Local Binary Patterns (LBP) is a dense image descriptor. 
    * Uses a sliding window of foxed raidus, it computes the value of the central pixel based on the intensities of its neighbouring pixels. 
    * Can concatenate LBP frm multiple channels (i.e. RGB) to construct a new feature vector. 
    * Classifications: 
        * Benign (not harmful)
        * Malignant (harmful)
    * Can extract tabular data with domain specific knowledge, interpretablity "we know these work". 
    * Same-index-crossover-mutation - group together features for one method from constructed feature vector - i.e. Blue channel LBP. 
    * j48, NB, SVM, KNN. 
    * Analyze the feature apperance as feature frequency, then relate to domain expertise/OG features, to get interpretable models. 
    * Wavelet: different-scale information (mean/std summary statistics). 
    * "Wavelet" features have perormed well in the existing literature; Exsemble classifier (i.e. AdaBoost) performs very well. 
    * Emsemble balances the apporach of several classification algrotihms, voting/bagging, best performance. 
    * Wrapper: based on the performance accuracy on a subset of the training data labels (y). 
    * Filter: based only on the features (X). 
    * Important to use balanced accuracy for imbalanced datasets, otherwise results are biased towards the majority class. 
    * GP has the opportunity to generate new knowledge that can be verifiedand encorporated by domain experts into their field. 

TODO: 
    * [ ] READ: GP for Multi-FC in skin cancer image classification. 
    * [ ] READ: Two-stage GP for auotmated dianoging Skin Cancer from Multi-Modelity Images 

2022-06-17 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Cui Yang, Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

General notes: 
    * No meeting next week due to the (first) celebration of Matariki as a public holiday. 
    * Writing is an important part of research, we refine our ideas, and correct our incorrect assumptions, we organize our thoughts. 
    * For example, when re-writing my draft paper, I realized my understanding of treated fish biomass waste was incorrect.
    * Something I would have never realized if I did not explicitly have to re-write my introduction section. 

Cui Yang gave a talk on "Domain Adaptation". 

Notes: 
    * Machine learning learns to predict data from identical and indepedendly (iid) probability distributions. 
    * Transfer learning, use knowldge from one problem and apply this to another problem. 
    * Motivation: training ML models can be expensive, requires hundreds of GPU/TPU hours. 
    * Goal: improve performance of a model by training it on data from a source domain, to improve performance in the target domain. 
    * Source: domain to extract valuable information from. 
    * Target: area to improve performance in. 
    * Types: 
        * Instance 
        * Feature 
        * Paramater 
        * Rational-knowledge 
    * Instance based adaptation; KMM, AdaBoost. 
    * Feature based adaptation; CORAL, TCA, Class-conditional. 
    * Deep domain adaptation; MK-MMD, CORAL, GRL, HDA. 
    * Domain invariant and specific parts in prepresentations. 
    * Partial domain adaptation: classes in target domain are a subset of that in the source domain. 
    * Transfer learning in dynamic environments; concept-drift tolerance, hybrid ensemble approach. 

2022-06-20 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Jesse Wood, Bastiaan Kleign, et al.

Jesse Wood (me) gave a talk on the paper "Hierarchical Text-Conditional Image Generation with CLIP Latents" (ramesh 2022, ramesh2022hierarchical) https://arxiv.org/abs/2204.06125 

Notes: 
    * CLIP trains an auto-enocder to have minimize the distance between image and text embeddings in the latent space. 
    * Those image embeddings are fed to an autoregressive or diffusion prior to generate image embeddings. 
    * Then this embedding is used to condition a diffusion decoder which produces an image. 
    * The model is trained on 250 Million images, and has 3.5 billion parameters. 
    * We can use CLIP to interpolate between two images in the latent space. 
    * As we increase the dimensionality of the latent space we can represent more complex hierarchical structures. 
    * CLIP fails at producing text, and reconstruction can mix up objects and their attributes. 

2022-06-23 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Thursday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes: 
    * After finished Australassian AI paper, start writing the proposal. 
    * Write the motviation and research objectives. 
    * Good to finish proposal early - before Christmas! 
    * All important conference deadliens are in Jan - Feb. 
    * Eye contact on zoom, is not possible with cameras and remote audience. 

2022-06-23 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Jiablin Lin, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Jiablin Lin showed a talk on "The Three Magic Ingredients of Amazing Presentations" https://www.youtube.com/watch?v=yoD8RMq2OkU

Notes: 
    * Three things to consider: 
        1. The Audience 
        2. The Speaker 
        3. Transformation 
    * Call to actions (CTA), what can the audience do? 
    * Then we re-arrange the roadmap and can get the presentation order. 
    * Can use this technique for speeches, emails, proposals. 
    * Common mistake to put too much information into a presentation/paper, sometimes less is more, more important to change mind of audience.

Jiablin Lin suggested "The margical science of story-telling". https://www.youtube.com/watch?v=Nj-hdQMa3uA 

Notes: 
    * A 99 cent horse head was sold for $62.00. 
    * He bought random items of little value, then attactched a sory to each object written by authors, this increased their value a thousand-fold. 
    * The more emotionally invested in something, the less critical you are of that thing. 
    * 13 months after "honey-moon" period, our neo-cortex and objective reasoning comes back. 
    * The same thing ahppens during an advertisment that aims for brand experience - i.e. Coca Cola. 
    * All story-telling generates dopamine, because we are waiting for soemthing, i.e. cliff hanger. 
    * Oxytocin makes people bond towards a person, it makes them feel human, we can do this by telling a person a story that makes the audience empathetic. 
    * Endorphins, laughing makes the audience relax, comfortable and more open. 
    * Devils cocktail; high levels of cortisel and adrenaline. 
    * Functional story telling: 
        1. Everybody is a good story-teller from birth. 
        2. Write down your stories! 3-4x times more than you think. 
        3. Pick story to enduce emotions you want. 
 
 2022-06-27 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Daniel Braithwaite, Jesse Wood, Bastiaan Kleign, et al.

Daniel Braithwaite gave a presentation on "Audio Source Seperation - Using GAN/flow priors"

Notes: 
    * We have two unknown source signals, mixed together into a single signal. 
    * Minimize distance between the original source and generated source, create a latent space that accurately encodes the mixed audio signals and their individual source signals.
    * Proposed approach is low audio quality, but it can seperate two audio signals. 
    * Problem with GAN approach is Mode Collapse (not iid data) and low quality audio. 
    * Flow based generateds, train an AI model to generate instruments from an audo strack. 
    * These flow-based models use the inverse STFT. 
    * Easy to verify the accuracy of an audio seperation model by listening to its output. 
    * Problems with likelihood maxmimisation, highly discontinious, "deep generative models don't know what they don't know". 

Bastiaan Kelign breifly discussed a paper "Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models" (bao 2022, bao2022estimating) https://arxiv.org/abs/2206.07309

Notes: 
    * Diffusion Probabilistic Models (DPM) are special Markov Models with Gaussian Transitions. 
    * Paper shows how to go from noisy-to-clean with a deterministic process. 
    * A new approach to diffusion based models.

2022-06-30 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Junhong Huang, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al.

General notes:
    * FASLIP was started 5-6 years ago in September 2016. 
    * Ying Bi's last day in New Zealand - she was in FASLIP from the beginning. 

Junhong Huang shared a lectured from Andrew Ng "Lecture 8 - Career Advice / Reading Research Papers" https://www.youtube.com/watch?v=733m6qBH-jI

Notes: 
    * Two topics: (1) Reading research papers, and (2) career advice. 
    * Many PhD students learn to read papers by osmosis, from those around them, picking up tips and tricks. 
    * Andrew Ng outlines his method for reading papers. 
    * Start with a complete list of papers (including blogposts / Github).
    * Skip around the list. 
    * How many? 5-10 papers is good for implementation. 50-100 is needed for thorough understanding and research. 
    * Everywhere he goes, he has a stack of papers that he takes around with him.
    * He reads 6 papers a week, and presents 2 to a research group. 
    * Take multiple passes over a paper: 
        1. Title, abstract, figues. 
        2. Intro, conclusion, figures + skip rest. 
        3. Read paper + skip math.
        4. Whole things + skip parts that don't make sense. 
    * Questions (after readings): 
        - What did authors try to accomplish? 
        - What were the key elements? 
        - What can you use yourself? 
        - What other references do you want to follow? 
    * Math: 
        - Rederive from scratch. 
        - Art Gallery, we see art student's sitting on the floor copying the work of the greats. 
        - Do the same for Machine Learning, copy the greats, learn to dervice (and even invent) algorithms. 
    * Code: 
        - Run the open-source code. 
        - Re-implement from scratch. 
    * Steady reading, not short bursts! Sparse repetitions work better than cramming. 
    * Goal for most PhD students is a job (either big company or startup). 
    * Either way, we intend to do important work! Leave the world a better place than we found it! 
    * Recruiters are looking for skills (ML + coding), and meaningful work. 
    * The idea candidated is a "T" shaped individual, someone who has a breadth of knowledge in AIML, and depth of knowldege in one or two areas of expertise. 
    * Course work is an efficient way to gaina bredth of knowledge in AI field. Also +reading papers, +relevant projects. 
    * Saturday morning problem: (1) read paper, (2) open-source contribution, (3) project, (4) TV? 

2022-07-01 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Friday 12.30pm-1pm , **Attendees:** Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes: 
    * SIGEVO member for free virtual attendence to GECCO conference.
    * Conference is in July 9th - 13th 2022, Boston Masacheseutts. 
    * Focus on tutors, conference has different tracks - i.e. GP, Neural Evolution, GA, Swarm Intelligence. 
    * Big names in Evolutionary computations are coming to Wellington. Good opportunity to meet. 
    * Math is an important part of machine learning, different areas of math required for different tribes of AI. 
    * Bach is on holiday... 
    * Writing the paper is taking longer than I thought it would. But making slow progress... 
    * Deadline for Australassia AI is July 17th (just over two weeks away). 

TODO: 
    * [x] Email Yi to become a member.
    * [x] Register for GECCO 2022.

2022-07-01 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Shaolin Wang, Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al.

General notes: 
    * Any time after 1 pm is actaully 10 monutes past that time, so 1 pm is 1.10 pm, to give students a 20 minute break for lunch. 
    * Impact factor for 2021 was released. Many top journals in AI increased their impact factor - i.e. IEEE TPAMI (>24 now). 
    * Register as a SIGEVO member ($10 USD), Yi will buy membership, free attendence virtually to GECCO. 
    * Wolfgang Baanzhaf https://www.cse.msu.edu/~banzhafw/, a foundational figure in Genetic Programming, is visiting the university soon. 
    * AI undergraduate major has been approved by the university, I am involved in advertising for this major. 

Shaolin Wang gave a talk on "LRE for GP-evolved Policies for UCARP". 

Notes: 
    * Uncertain Capacitated Arc Routing Problem (UCARP). 
    * Use genetic programming to provide hyper-heuristics. 
    * Benefits: flexible representation, less domain knowledge, potential interpretability. 
    * Existing work to increase the interpretability of routing policy. 
    * Hard to describe the full mapping learned by a model.
    * Local explanations, easier to give explanations to less complex solution subspaces. 
    * Local Ranking Explanations (LRE) method, is used to explain the behaviour of routing policy in a decision situation. 
    * Dataset - 6 representative UCARP problems.
    * Results - by error, correlation and consistency. 
    * Table shows a linear model can represent the LRE for a routing problem.
    * Proposed a new LRE method for UCARP problems. 
    * This presentations recieved a best paper nomination for GECCO 2022. 

 2022-07-06 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Wednesday 3pm-4pm , **Attendees:** Jesse Wood, Bastiaan Kleign, et al.

Bastiaan Kleign discussed a paper "Denoising Diffusion Implicit Models" https://arxiv.org/abs/2010.02502

Notes: 
    * Other approaches: Flowbased, VAE, auto-regressive, WaveNext+ 
    * Diffusion; reverse forward process where we gradually add noise. 
    * Problems: over-denoise, extremely computationally complex, many steps - same number backwards as forwards. 
    * Can be derived from Langevin Diffusion equation from physics. 
    * Consider a forawrd Markov Process that gradually replaces the signal with Guassian noise.
        :math:`q(x_t|x{t-1})= N(x_t;\sqrt{1-\beta_t} x_t - 1, \beta_t I)`
        where :math:`x_t = \sqrt{\alpha_t} x_{t-1} + \beta_t \epsilon_t`
    * :math:`\beta_1,...,\beta_t` is the noise schedule (fixed by designer), but some new papers automate schedule selection. 
    * We assume we can approximate the inverse process with a reverse Markov Process. 
    * Naturally objective function is the cross entropy - negative log liklihood --> KL, which can be made tractable with Jensen's inequality. 
    * The symbol :math:`\epsilon` is the noise, the objective function can be reformulated to predict the noise :math:`\epsilon`. 
    * This diffusion method can generate images of people that do not exist, but this takes 100,000 steps, so diffusion models are very slow. 
    * Langevin's Equation: :math:`m\frac{d^2x}{dt^2} = - \gamma \frac{dx}{dt} = \nabla V_t + \eta(t)`, where :math:`x` is particle location, :math:`V` is a stiationary potential, :math:`\eta(t)` is a random force
    * From physics, requires stochastic calculus. 
    * Choise :math:`v(x) = - \log \pi(x)`, with :math:`\pi(x)` an equilibrium density, and no accelation term, results in random walk sampling from :math:`\pi(x)`. :math:`\nabla V` is then the score. 
    * A more generalized diffusion approach satisifed the equations, but is no longer a Markov Process. 
    * Forward process is no longer Markovian, but Backward process is. 
    * If its not Markovian, we donot need to add noise each step, we only add noise at the end. 
    * The actual forward steps can be derived from BAtes Rule, but we don't need them, we only need Eq. (9) for training. 
    * When :math:`\sigma_{t=0}` we end up with a deterministic relationship between the noise :math:`\epsilon` and the output. 
        * Backward process is deterministic. 
        * Map straight from noise to output. 
    * We can interpolate between two noise inputs, and get meaniful output, due to deterministic nature of noise-output mapping. 
    * In DDIM the initial state is the only place where stochasicity occurs --> meaningful interpolation. 
    * No requirement for number of steps foward and back to be the same, can choose fewer steps (the usually choose 100) for backward process. 
    * Neural Ordinary Differential Equations (NODE).

2022-07-06 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Shaolin Wang, Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al.

General Notes: 
    * Due to time difference for GECCO, rest during the day between sessions for wellbeing. 
    * Be there for best paper nominiation for GECCO conferences - will be early in the morning. 
    * Yi has reigstered those who asked as SIGEVO members, we get free virtual attendence to GECCO. 
    * A lot of students are getting sick - important to take care, so we can recover and back at it. 
    * Attend Shaolin's GECCO session on Tuesday, so we can vote for his best paper nomination. 

Ruang Jaio gave a presention on "Solving Multi-objective feature selection problems in Classification via Problem Reformation and Duplication Handling"

Notes: 
    * "Data and features determine the upper limit f machine learning, and models and algorithms just approach that lmit."
    * High-dimnesional issues: 
        1. Curse of dimensionality. 
        2. Not all features are useful. 
    * To address (1) there are techniques: 
        * Feature selection - pick a subset of the relevant. 
        * Feature extraction - create new, smaller set of features. 
        * Feature construction - produce a new high-level feature using some original features.  
    * FS motivation
        * Improve classification performance. 
        * Reduce the dimensionality. 
        * Simplify learnt model. 
    * Multi-objective motivation 
        * A set of non-dominated features can be found taht meet real-world applications. 
    * FS categories: 
        * filter - only consider data. 
        * wrapper - compare to classification performance. 
        * embedding - ? 
    * Proposed model is evluated against many state-of-the-art moethods and on many real-world datasets. 
    * Combination of duplication/constraint handling gives better results. 
    * Sequential search: 
        * Forward 
        * Backward 
        * Bi-directional 
    * Issues with sequential search - gready, easy to fall into local optimum. 
    * Stochastic search - using evolutionary algorithm. 
    * Pros: 
        * global search, 
        * does not require domain knowledge. 
    * Issues with mutli-objective feature selection (MOFS): their are frequently appeared duplication in the objective space, leads to poor diversty and permature convergence in the population. 
    * Goals: develop a novel approach to balancing the preferences between classification accuracy, number of features, reduction of duplication. 
    * Genetic operators: 
        * Crossover - single point crossover 
        * Mutation - bit flip generation 
    * Novelty: duplication handling - only duplicate solutions inthe objected space that are not dominated by other solutions are retained int he population. 
    * non-dominated solution - in a multi-objective problem, a non-dominiated slution if a candidate solution, if values generated by one (or more) objective functions reduces the quality of values by the other objective function. These candidates are paretor optimal - we can explore the pareto front to get th set of pareto optimal solutions. All pareto optimal solutions are considered equally good and considers equal to eachother (Khanmohammadi 2021). 
    * Constraint handling: select solutions that satisfy constraints preferences btween the difference objectives. 

2022-07-09 - GECCO #1
~~~~~~~~~~~~~~~~~
**Location:** Boston, Masacheseutts **Time**: Saturday 12:30am - 2:00am, **Attendees:** Frank Neuman, Jesse Wood et al.

Frank Neuman gave a presentation on "Evolutionary Diversity Optimization". 

Notes: 
    * Diveristy plays a crucial role in EC.
        * prevent premature convergence. 
        * allows for pareto-optimal set of solutions.
    * Goals: 
        * A good set of solutions that differ in terms of interesting features/attributes. 
    * TSP - Travelling Sales Problem, give a set of nodes, determine the path for a round trip.
    * USe evolutionary algorithsm to evolve isntances. Because constructing instances if a very difficult problem in general. 
    * Genetic operators: explosion and implixion operaters. The mutations operators such that they have a much higher impact on their points. 
    * Produce diverse image sets using evoluitionary computation methods. 
    * USe the :math:`(\mu + \lambda)_{EA}` for evolving images instances. 
    * Select individuals based on contribution to diversity of the image. 
    * Discrepency-based evolutionary diversity optimization. 
    * Adapative random walk mutaltion operator. 
    * Use of star discrepeny measure for diversity optimization of images. 
    * Indactor-based multi-objective optimization. 
    * Diversity optimization makes a multi-obkjective indicate based search for diversity for a single-objective problem.

2022-07-10 - GECCO #2
~~~~~~~~~~~~~~~~~
**Location:** Boston, Masacheseutts **Time**: Saturday 12:30am - 2:00am, **Attendees:** Deppo Izzo, Jesse Wood et al.

Depp Izzo gave a keynote address on "Optimization Challenges at th European Space Agency (ESA)". 

Notes:
    * Cassini (NASA), controlling the velocity of a spaceprobe to nagivate our solar system. 
    * Space-probes slingshot around planets, a solution to the three body problem, to get free rotation from the mass of a stellar object. 
    * Deep Space Maneuver (DSM). 
    * Messenger (NASA), had high thrust ion engines to adjust its orbit around the sun to allow its trajectoty to reach another planet, Venus. 
    * Hyabusa (JAXA), ion propulsion is an electric engine, DSM are more difficult to design tractories for, but we can pull it off with less mass, and non depending on planetary orbits (more frequent less time constrained flights). 
    * Dawn (NSAN), Repi Columbo (ESA). 
    Interplanetary trajectories, GTOC problems - an America's cup fo Rocket Science, design near impossible trajectory problems. 
    * Near-to-impossible interplanetary trajectory problems: compelxity ensures a clear competition winner. 
    * Gloabl Trajectory Optimization Competition (GTOC). 
    * GTOC 1; save the earth problem, won by NASA Jet Propulsion Labratory (JPL).
        * An asterord/meteor is heading to Earth, and we must optmize trajectory to reach it. 
    * GOTC 2: Multiple asteroid rendevous, visit 4 asteroids. 
    * Trajectory optimiztion is a multi-objective problem; optmize mass and time. 
    * GOTC X; conline the galaxy as uniformly as possible (Sumbission by JPL). 
    * ESA Libraries: 
        * pyrep - trajectory problem building blocks. 
        * pygmo2 - massovely parelled evolutions.
        * dcpy - differential genetic programming. 
        * GTOC portal - very difficult trajectory problems. 
        * optimize.esa.int - a gym from ESA with problems. 
    * Building blocks: 
    * Lagrange prpagation - predicts the time evoltoin of an orbit from starting conditions. 
    * Lambarts problem - going form on planet to another in a fixed time. 
    * Mivovitch slingshot, velocity is convered in slingshot, design the amount of rotation. 
    * Optimization problems: 
        * Mutiple Gravist Assist Interplanetary (MGA).
        * MGA-1DSM, only one DSM is allowed between each <unlegible>. 
        * MGA-LT, only low thrust maneuvers. 
        * Tours and multiple visits. 
    * Problems vary largely as we increase the number of spatial dimensions. 
    * Designing an adequate representation to capture the problem was the most important part - needed to get results that make any sense. 
    * took +8 hours to perform the evolutionary search. 

2022-07-03 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Wednesday 3pm-4pm , **Attendees:** Jesse Wood, Bastiaan Kleign, et al.

Bastiaan Kleign gave a discussion of the paper: "Bayesian Deep Ensemblers via the Neural Tangent Kernal" https://proceedings.neurips.cc/paper/2020/hash/0b1ec366924b26fc98fa7b71a9c249cf-Abstract.html 

Notes: 
    * Neural Tangent Kernel (NTK) is a kernel of deep learning ANNS, it describes the evolution of the ANN during training by gradient descent. https://en.wikipedia.org/wiki/Neural_tangent_kernel 
    * Scalar output case: 
        * An ANN with scalar output consists of a family of cuntions :math:`f(.,\theta): \R^{n_{in}} \rightarrow \R` 
        * parameterized by vector of parameters :math:`\theta \in \R^P`. 
        * The NTK is a kernel :math:`\theta(x,y;\theta) = \sum_{p=1}^{P} \partial_{\theta_p} f(x;\theta) \partial_{\theta_p} f(y;\theta)`
    * (Jacot 2018, jacot2018neural) proposed that an infinite-width limit, ANNs have a Gaussain distributions described by the Kernel of Gaussian processes. 
    * (Lee 2019, lee2019wide) Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent. 
    * (He 2020, he2020bayesian) We exmploe the link between deep ensembles and Gaussian Processes (GPs) through the lens of Neural Tangent Kernels. 
    * See (Rajat 2019) blog post at https://rajatd.github.io/NTK/ for a great explanation of NTKs. 

2022-07-13 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Friday 12.30pm-1pm , **Attendees:** Bach Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes: 
    * Abstract deadline - only need paper title and an abstract - deadline: Monday 18th of July. 
    * Submission deadline - submit the finished paper for acceptance/review - deadline: Monday 25th of July. 
    * Shaolin got the best paper award for the GECCO 2022 with his LRE for GP based UCARP. 
    * Next draft - for Bach's feedback - deadline: Friday 15th of July. 
    * Standard PSO with single-objective function. Combines error and selection ratio. Can cite (Kennedy 1995) in paper.

TODO: 
    - [x] Next Draft - Friday 15th of July. 
    - [x] Abstract Registration - Monday 18th of July. 
    - [x] Revision - Tuesday 18th of July. 
    - [x] Submission deadline - Monday 25th of July. 

2022-07-14 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Ramya, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Ramya suggested a talk from Yoshua Bengio called "Yoshua Bengio: Deep Learning Cognition | Full Keynote - AI in 2020 & Beyond
" https://www.youtube.com/watch?v=GibjI5FoZsE

Notes: 
    * He talks about deep learning and whate are the future perspectives. He makes three mains points: 
        1. We must build a model which learns meta-learning causual effects in abstract space of casual variables. This requires a necessity to quickly adapt to change and generalize out-of-distribution by sparsely recombining modules. 
        2. The necessity to acquire knowledge and encourage exploratory behaviour. 
        3. The need to rbidge the gap between the aforementioned (1) and (2) ways of thinking, with old neural networks and conscious reasoning taken into account. 
    * Issues: sample complexity, human-in-the-loop, autonomy. 
        * Sample complexity - we need a large number of samples to get performant deep neural networks. 
        * human-in-the-loop - domain expertise is requried to verify ifficaacy of a model or create labelled training sets. 
        * Babies can learn on their own, without supervision they learn a physics model of the world. 
        * Generalize to out-of-distribution data. 
    * Humans are able to learn out-of-distribution data. E..g tell a human a science fiction story, despite never visiting space or seeing aliens, we have a complex conceptual framework where we can recombine aspects from previous narratives to synthesise an understanding of a novel scenario. 
    * When we humans learn a new concept we update a small part of our existing owrld model and adapt our knowledge efficiently. Most machine learning modles require full re-training to learn to handle new instances. 
    * Models need to learn high0level abstractions of the knowledge, good to distentation representations. (Higgen 2018, Ramesh 2022). 
    * Afforadance - we conceptualize the world, not as a set of objects, but as doing things, with a purpose or use, that can be manipulated by humans. 
    * Richard Dawkins an evolutionary bioligist proposed "The Evolutionary Imaginiation: Animals as models fo their world" (Dawkins 1995). 
    * Attention (Vaswani 2017, vaswani2017attention) is focusiing on one thing at a time, analogous to consciouness, what we consider important? 
    * Humans store episodic memories in their hippocampus. Auto-encoded are an approximation of concepts from associate memories.
    * The problem of credit assignment through time is inefficient due to backpropagration. Can be solved with "Sparsely Attentive Backtracking" (Ke 2018, ke2018sparse). Credit Assignment is like when you drive and you hear a loud pop sound. Later you find a flat tire. Then you remember the loud sound - and deduce the tire must have poppsed then. 
    * System 1 & 2 thinking (Chen 2019, chen2019deep)
        * System 1: intuitive, fast approach (intuition).
        * System 2: slower, analytical, reason (reasoning). 
        * When a human plays chess, tehy encoproate both system 1 and 2 level thinking - this is true intelligence, a combination of intuition and reason. 
    * (Bengio 2017, bengio2017consciousness) A new paper is proposed for learning representations of high0level concepts of the kind we manipulate through language. 
    * (Chevalier 2018, chevalier2018babyai) Grounded reasoning about language in the real-world. A combination of language models and vision. BabyAI allows humans to interatively train artificial agents to understand language instructions. 

2022-07-15 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Shaolin Wang, Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al.

Mengjie Zhang hosted a Q&A on Genetic Programming and CNNs literature.

General Notes: 
    * Shaalin Wang won the best paper award for the GECCO 2022 conference in Bostom massachusetts. 
    * Abstract registration for AJCAI 2022 is due 18th July, sumission deadline is 25th of July. 
    * GECCO 2023-2024 will continue to run in the dual conference mode - allowing for virual and in-person attendence. 
    * I plan to get my paper draft finished for Friday evening (today), hopefully I can get two more revsisions before the deadline on the 25th of July - Monday week. 
    
Notes: 
    * GP paradigms:
        * TGP - tree genetic programming. 
        * LGP - linear genetic programming. 
        * CGP - cartesian genetic programming.
        * :math:`G^3P` - grammar guided GP. 
    * CNNs (LeCunn 1998, lecun1998gradient)
        * OG: Feature Extraction + Classification network. 
        * Convolution operation exploits local connectivity between pixels. 
        * Shared Weights Neural Networks (SWNN). 
        * Important to use consistent terminilogy in AI research. 

2022-07-20 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Wednesday 3pm-4pm , **Attendees:** Hayden Dyne, Jesse Wood, Bastiaan Kleign, et al.

Hayden Dyne gave a talk about the paper "Guassia Process-based Stochastic Model Predictive Control for Overtaking in Autonomous Racing" (Brudigam 2021, brudigam2021gaussian) https://arxiv.org/pdf/2105.12236.pdf

Notes: 
    * A fundamental aspect of racinn is overtaking other race cars. Previous work focusses on the laptime optimization. 
    * Prpose a method to plan overtaking procedures in autonomous racing. 
    * A Gaussian Process (Ramussen 2004) is used to learn the behaviour of the leading vehicle. 
    * Based on the output of the GP, a stochastic Model Predictive Control algorithm plans optmized trajectories such that controlled autonmous race car is able to overtake the leading vehicle. 
    * They generate safety constraints, similar to (Brudigam 2018). The safety rectanle ensures the vehicle safely passes the leading vehicle. 
    * The GP identifies weaknesses in the driving behaviour of the leading behicle while the controlled vehicle is trying to overtake. 
    * Future work - extend the current model to simulate a whole lap and include multiple cars to overtake. 

2022-07-20 - IEEE Writing Seminar 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Wednesday 3pm-4pm , **Attendees:** Bach Nguyen, Mengjie Zhang, Peter Andreae, Jesse Wood, et al.

Notes:
    * Pondy was one of Bach's supervisors when he was studying for his PhD. Pondy was my COMP100 level lecturer for many papers. 
    * Audience for the workshop is particularly thesis students, Masters/PhD. 
    * Audience - If you don't know who your adueince is, you won't know hwat to say or how to say it. 
    * Assumed knowledge - general knowledge in the field that we assume the audience will know. 
    * How much to explain? Write to level other PhD students, not at an undergraduate level. 
    * Thesis: real audience is examiners. Intended audience is general experts. 
    * What - What should the paer or thesis say? Consider waht can and should not be included. 
    * Writing should be scoped to the correct level of detail. 
    * Introduction - critical for setting the authors expectations. Many people won't read a paper at ALL if the introduction is not clear. (Sign post, mental model, prepare). 
    * Planning - Top down and bottom up approaches to plaaning writing. 
        * Top down - has a skeleton for an essay that is later expanded. 
        * Bottom up - is a brain dump that is iteratively refined. 
    * Hook - setup a context/framework for the reader to understand the next bit. "Tell 'em what your gonna say, say it, and tell 'em what you said" (Goldberg 1999). 
    * Grammarly - use grammarly on all academic work, but don't assume that is is always right. Sometimes grammar rlies on context sensitive information the application does not understand. 
    * SPAG - always fix basic spelling errors, otherwise the examiner will think you are just being lazy. 
    * Pronouns - use plurals and try to avoid having to use gendered langauge or the singular they, avoid the confusion entirely. 
    * I - avoid using I, academics use the passive far too much, it is ahrder to read, hides important information, ambigious. "I was told you were driving 60 km/hr in Kelburn yesterday". 
    * Paper - in a paper we can use "we", referring to the author(s), but thesis is individual work, so avoid "we", to make it clear who takes credit for work. 
    * Active entity - make the paper an active entity, "this paper shows that", "section 2.5 showed that", "the proposed method uses". 
    * Name - you have a system, give it a name. Then we can refer to it by name henceforth. These names can be invented, e.g. "TADPOLE" and "HOPPER", or as acronyms, e.g. "XGG", "RTF". 
    * Parallel ideas in parellel form. Use the same sentence structure; don't restructure to make it interesting. 
    * Use belleted lsits frequently if you have a sequence of similar/related statements. 
    * Aphorisms:
        * Omit needless words. 
        * Write simply - flowery language is hard to read. 
        * Activate your sentences; avoid the passive. 
        * State important ideas first, reader may skip. 
        * Give it a name, easy to describe things with names. 
        * Avoid negatives where possible. 

2022-07-21 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO355, **Time**: Friday 12.00pm-12:30pm , **Attendees:** Bach Nguyen, Mengjie Zhang, Jesse Wood. 

Notes: 
    * Bing is at WCCI 2022 in Italy so she could not attend the meeting. 
    * Bach is finishing up lecturing this week, so he will have a draft revision ready a bit later. Daniel will need to sign off on the paper from PFR side. 
    * Focus on my thesis work, need to write my proposal, and the proposal needs prelminary work. I can do some GP on the GC data. 
    * The AJCAI deadline have been pushed back. The abstract registraion is 29th of July, the submission deadline is 5th of August. 

TODO: 
    - [x] Submission deadline - 5th of August. 
    - [x] GP on the GC dataset. 
    - [x] New dataset from Daniel
    - [x] Reading on GP. 
    - [ ] Start writing proposal. 

2022-07-21 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Qi Chen, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Bing suggested a series of talks from foundational figures in Genetic Progamming. 

Bing suggested "Wolfgang Banzhaf - Full Interview" https://www.youtube.com/watch?v=tj5-H6ECxyM

Notes: 
    * AI effect - as soon as we solve an issue it is no longer considered intelligent. 
    * Active devices force solutions to adapt representations with changing complexities. 
    * Real-world application- given the correct constrainted/discretized representation we ca creativly apply randomness to many applications.
    * Evolution takes shortcuts the programmers were not expecting, we have to gradually evolve our fitness functions to get the desired behaviour. Similar to (Lehman 2022). 
    * Hardware - Technologies for parallell and distributed computing should allow for harware growth in Evonltionary Computation. 
    * Proofs - proofs are very difficult to formulate for real-world problems. Engineers can write tests for safety critical systems. But it is difficult to find proofs for complex evolutionary computations. 
    * Exploitations vs. exploration - we want to allow diversity, but we have constraints for a desired. 


Bing suggested "Risto Miikkulainen - Full Interview" https://www.youtube.com/watch?v=6H9jzq0Oj0s

Notes:
    * def. Aritifical Intelligence - something more complex than what was programmed emerged. 
    * Evolutionary computation - a family of methods imspired from biology. Almost all EC is population based. 
    * EML - we can apply evolutionary computation to optimizing the architecture and weights for neural networks. 
    * Neural evolution - is a very computationally expensive, but with improving technology we have the comput, and more so they are being employed for this. 
    * Domain insights - evolution does not have the same preconcieved notions of what the solution should be. It has the potential to discover new knowledge, exploring areas outside of the domain expertise. 
    * EC is similar to DL, as the ideas and principles have been around for a while, but only now are we approaching comput necessary for efficiency. 
    * Diffusion - diffusion of innovtation; EC needs to be taught to AIML practitioners and be part of their toolbox for solving problems.  Democratizing evolutionary computation. 

Bing suggested "Ken Stanley - Full Interview" https://www.youtube.com/watch?v=XWUsl24zYOU 

Notes: 
    * Accesibility - AI is not really accessible to the non-technical general public. It is a few years off, but one day an everday person should make an AI to automate a mundane task of their choosing. 
    * Games - create a game where users can train robots to fight in a robot war. But a game is a wat for non-technical users to AIML practitioners without barriers. 
    * Incentives - A picture breeding website that allowed for organic human-in-the-loop exploration of image gneration. We can incentivise humans to join the loop and offer supvervision in areas where AI struggles. 
    * Bottlenecks - similar to DL, their was a large fondation of theoretical work, but lacking hardware. THE DL revolution was mostly a hardware advance. The same is true for EC, where populaiton-based algorithms can be optimized with parellelisation. 
    * Compute - with compute, it becomes tractable to search high-dimensional spaces. 

2022-07-22 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al.

Notes: 
    * Bing (my supervisor) got an IEEE early career award. This is a very difficult award to get. She was also given an "outsanding editor" from IEEE transactions on evolutionary computations. 
    * Bing is also giving a plenary talk at WCCI 2022 in Italy. 
    * This Wednesday we had a writing  seminar organized by the IEEE young professionals, with a talk by Pondy, organized by Bach, Fangfang, Menjie (see minutes 2022-07-20 - IEEE Writing Seminar). 
    * Progress: writing my paper for AJCAI, deadline was extended to August 5th, so room for more revisions and Plant and Food Research (PFR) sign off. 
    * Daniel and Kevin are flying up from Nelson to talk about the domain expertise required for REIMS data. 
    * Starting preliminary work on GP for GC data, and writing my proposal for my PhD. Important to clearly state and refine my research intentions. 
    * Meng has sent out a good list of reccommended readings on CNNs (Lecun 1998, lecun1998gradient), foundational work from Geoffrey Hinton, a good read for Deep Learning background.

2022-07-27 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Wednesday 3pm-4pm, **Attendees**: Mawell Clarke, Bastiaan Kleign, Jesse Wood et al. 

Maxwell Clark gave a talk on Neural Tangent Kernels (NTKs):

Notes: 
    * A Gaussain Process is a stochastic process (a collection of random variables indexed over time or space), such that every finite collection of those variables has a multivariate normal distribution. https://en.wikipedia.org/wiki/Gaussian_process
    * The direection of a learning network becomes fixed, as we increase the width towards infinity. (Or) As the width of the nueral network increases, we see that the distribution of outputs over different random instantiations of the network becomes Gaussian (He 2020, he2020bayesian). 
    * In linear algbera, an eigenvector, or characteristic vector of a linear transformation, is a non-zero vector that changes at most by a scalar factor when that linear transformation is applied to it. The corresponding eigen value :math:`\lambda` is the factor by which the eigenvector is scaled. https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors 
    * A Taylor series, named after Brook Taylor who introduced it in 1715, of a function is an infinite sum of terms that are expressed in terms of the functions derivitve at a single point. For most functions, the function and the sum of its Talyor series are equal near this point. 
    * Transformers can do Bayesian inference, The propose prior-data fitted networks (PFNs). PFNs leverage large-scale machine learning techniques to approximate a larget set of posteriors (Muller 2021, muller2021transformers).
    * Requires the ability to sample from a prior distribution over supverised learning tasks (or functions). 
    * Their method restates the objective prosterior apprimixation as a supervised classification problem with set valued input: it repeatedly draws a task (or function) from the prior, draws a set of data points and their labels from it, marks on of the labels and learns to make probabilistic predictions for it based on the set-valued input of the rest of the data points.
    * PFNs can nearly perfectly mimic Gaussian Processes and also enable efficient Bayesian Inference for intractable problems, with 200-fold speedups in networks evaluated. 
    * PFNs perofrm well in GP regression, Bayesian NNs, classification on tabular data, few-shot iamge classification - there applications demonstrate generality of PFNs. 

2022-07-28 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO355, **Time:** Friday 12pm-1pm, **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood. 

Notes: 
    * PFG have performed preliminary EDA on the REIMS data, classification/regreesion task for Hoki, Mackeral and Hoki-Mackeral mixture. 
    * Do not need to go into maths at the moment, instead look at the high level ideas, see existing work in the field, and see where future work is needed. 
    * Use GP on the GC data to get some preliminary work done for the PhD proposal. Don't set the tree depth to deep. 
    * PSO is wrapper, but what is it wrapping? It wasn't clear in my paper draft what classifier the PSO wrapped, it should be made explicit the PSO used an SVM classifier. 
    * A table for feature selection with a set k, to show the accuracy with reduced featureset, compared this to full dataset from classification section.
    * Include best (not just average) PSO run in the table. 
    * Comare our results to Daniel's preliminary work, in terms of accuracy and time taken to perform anaylsis. 
    * Add "AI Application" as a keyword to the abstract.  

TODO 
    * [x] GP Tree on GC data. 
    * [x] PSO wraps SVM classifier for AJCAI paper. 
    * [x] Add table for Feature Selection to AJCAI paper. 
    * [x] Include best (not just average) PSO run in the table. 
    * [x] Add "AI Application" as a keyword to the abstract.  

2022-07-28 - FASLIP
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm, **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al.

Bing suggested an interveiw from Darrell Whitley https://www.youtube.com/watch?v=ZJXxaoOBCOA 

Notes: 
    * Satellite scheduling in space using Evolutionary computation. Detecing space junk to.  EC has applications in aerospac (very similar to GECCO 20220-07-10 GTOC Challenge).
    * EC expands capabilites of AI to performtasks beyond jst things humans are good at, i.e. vision and text processing. EC is on the bleeding edge of artificial intelligence and search algorithms. 
    * We track up to 13,000 pieces of space junk. We try to ping every object once a week. Trying to pink these objects is an optimization probelm, Important for aerospace as we don't want ot hit space junk during a rocket launch. 
    * General Electric have a jet engine for Boeing that used evolutionary algorithm, EC designs engines that humans never thought of by diversity. 

Bing suggested an interview from Malcom Haywood https://www.youtube.com/watch?v=3M3N2o1sGbM 

Notes: 
    * He did a PhD in neural networks, fuzzy logic, then settled on Evolutionary Computation. You have to try all these fields out, and find the one that suits you. 
    * Gradient descent methods offer effective results, but are very constrained, and don't offer simple/elegent solutions. 
    * Evolutionary algorithms offer effective solutions that can be run on a local machine, and don't require +10,000 GPU hours. 
    * Desiging an agent to esccape a swarm of oponent spaceships. the agent fell into a black hole that was a rounding error. This shows the suprising creativity of digital evolution (Lehman 2020, lehman2020evolution).
    * Follow the money, is possibly a way for EC to track into the mainstream. Full-self driving would benefit from simple and interpretable solutions. 
    * Hardware is so complex, we as humans don't have the capacity to write code that utilizes it (Similar to Bjarne Stroustrup in C++ compiler optimization https://youtu.be/uTxRF5ag27A).

Bing suggested an interview from Erik Goodman https://www.youtube.com/watch?v=BkORxgpOc7w 

Notes: 
    * A center for the study of evolution is funded by the natural science fondation. They have 8/10 years of funding left. 
    * How do we define intelligence? Prevsiouly, we thoug off this as the turing test. But now, especially after LaMDA Google AI engineer Blake Lemoine (https://twitter.com/cajundiscordian) claimed their language model was sentient. We can easily mimic intelligent behaviour, but this doesn't make the machine intelligent. 
    * We define intelligent as the ability of an agent to synthesize new knowledge and adapt to novelty. 
    * When computer scientists listen to evolutionary biologists, and vice versa, we get interesting results. 
    * Nature has massive parellisim embedded, allowing for organisms to compete together to survive to be the firsst for the environment it adapts to.
    * In EC parellisation can correspond to population-based search. 
    * Evolutionary deep learning is likely the future. 
    * Democratization, small scale DL with EC. 

2022-08-03 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time:** Wednesday 3pm-4pm, **Attendees:** Daniel Braithwaithe, Bastiaan Kleign, Jesse Wood, et al. 

Notes:
    * Grad Student Descent (Gencoglu 2019, gencoglu2019hark).
    * Learning strides in convolutional neural networks (Riad 2022, riad2022learning).
    * Convolution theorum corresponds to multiplication in the fourier domain. Because multiplication is more efficient than convolution. 
    * A 2013 paper proposes replacing the signal-domain convolution with multiplication in the fourier transform (Mathieu 2013, mathieu2013fast).
    * FFT is a linear unitary transform - gradients in the fourier domain can be compared with the ivnerse fourier transform. 
    * It is non-trivial to learn non-linearity (activations) in the fourier domain. We have to take inverse fourier transform (IFT), then apply the non-linearity, and transform back. 
    * Learn kernals in the fourier domain, spectral pooling - projects into the fourier tdomain and truncates the singal's frequency representation. 
    * Pooling is an operation that reduces the spatial dimension of the input. 
    * Adaptive attention span to learn the cropping size for spectral pooling (Riad 2022, riad2022learning). :math:`m_z(x)` is a softclamp function to learn the attention span. 

2022-08-04 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time:** Friday 12pm - 1pm, **Attendees:** Bach Hoai Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes: 
    * I need to put the table and new graphs on overleaf. Also respond to Bing's comments. 
    * The submission is due on August 5th Friday, Bach wants me to send the new results before midnight tonight. 
    * Should still have access to the ECS cloud computing grid. This is a distributed system ideal for EC algorithms that use parellisation. 

2022-08-04 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:**: CO350, **Time:** Thursday 2pm - 3pm, **Attendees:** Quinglan Fan, Bing Xue, Mengjie Zhang, Jesse Wood, et al.

Notes: 
    * Classify an image into a category, i.e. cat or dog. ImageNet is classic benchmark for this task. 
    * Challenges: 
        1. High dimensionality
        2. Variations - rotation, scale, hue, intra-class. 
        3. Interpretability - NNs are blackbox. 
    * CNNs have high classification accuracy, but! 
        - requires sample complexity (big data). 
        - needs compute (TPUs, GPUs) 
        - black-box means low interpretability. 
    * Genetic Programming (GP) - is an evolutionary computation technique. 
    * They use strongly-typed GP where we define the input-output data tpyes. 
    * Motivations for GP: 
        - Flexible-lenghth representation
        - Evolving solutions automatically
        - Global search ability
        - interpretability 
        - multi-task, multi-objective 
    * The most important bottleneck is the representation (see 2022-07-10 - GECCO #2). Sufficient representation was needed to capture complexity to the problem. Garbage in, garbage out. 
    * Function set: image related operators, operators that are specialized to work on image data specifically. 
    * Global and local features, can be combined with flexible-feature reuse. Proposed method can automatically learn to choose classification algorithms based on the task. 
    * Terminal set: achieve flexible feature re-use, and, allow automatic classifier selection. 
    * This is benchmarked on 12 standard image classification datasets, and against state-of-the-art deep learning and traditional methods. E.g. CIFAR-10 MNIST. 
    * Results are comparable to state-of-the-art deep learning methods. 
    * We can examine GP trees, especailly for image classification, by seeing what features are used to make classification decisions, visually. E.g. A facial expression classifier looks at the mouth. 
    * We can't use the results of the experiment to justify its parameter settings. This is tuning to the validation set, and not based on theory/previous work. Instead refer to initial experiments, background literature; expert knolwdge (not) black magic, Cthulhu, Grad Student Descent (Gencoglu 2019, gencoglu2019hark).

2022-08-05 - ECRG 
~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time:** Friday 3pm - 5pm, **Attendees:** Bach Hoai Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood et al.

Notes:
    * Submission deadline is August 5th (today!!!) for AJCAI 2022, this this is likely flexible (as no explicit time was given). Can submit tomorrow with minor fixes. 
    * Computer Communication and Artificial Intelligence (CCAI) deadline for this conference is Jan 10th 2023. 
    * Bing's ingagural lectures is Tuesday evening on 9th August.

2022-08-10 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time:** Wednesday 3pm - 4pm, **Attendees:** Felix, Bastiaan Kleign, Jesse Wood et al.

Notes:
    * Usually too expensive to compute the Jacobian matrix and its inverse. Most models restrict the function class with easy evaluation of the Jacobian determinant, or an efficient estimator thereof. 
    * Self normalizing flows replaces expensive tasks in gradient by learned approximate inverses at each layer. This reduces the complexity from :math:`O(N^3)` to :math:`O(N^2)`.
    * What is a Jacobian Matrix? What is a normalizing flow? Inverse Jacobian? 
    * This model can perform efficiently and well on high-dimensional image data, for example the MNIST handwriting dataset. A visual analysis of their reconstruction appears to be good. 

2022-08-11 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time:** Friday 12pm - 1pm, **Attendees:** Bing Xue, Mengjie Zhang, Jesse Wood.

Notes: 
    * Classification Map cab be extended to automatically determine the boundary on the floating point line using clustering. 
    * Can use multiple trees, one for each class (Multi-tree), or one-vs-rest (Committee). 
    * Flexibility, there is a lot of options to explore with the GP representation. 
    * Parsimony pressure - fitness function includes accuracy and GP tree size. Can choose to only use Parisonomy pressure for selection, and evaluate fitness as only accuracy. 
    * VUW has a thesis on program simplification, to eliminate redundancy in GP tree. 
    * Start with 4 artihmetic operators, can extend later if needed. 
    * Diminishing returns - Simplification may be a very costly research objective, but may have bery little impact on the classification accuracy for the industry application. Industry research should be results driven. 
    * AutoML - e.g. TPOT, automated ML is the process of automating the process of applying machine learning to real-world problems. 
    * Interpretability is a good objective for industry research. 
    * Ricardo Poli has a GP Guide that is available for free online (Riccardo 2009, riccardo2009field). https://ia801902.us.archive.org/3/items/AFieldGuideToGeneticProgramming/AFieldGuideToGeneticProgramming.pdf

2022-08-11 - FASLIP
~~~~~~~~~~~~~~~~~~~
**Location:** CO350, **Time:** Thursday 2pm - 3pm, **Attendees:** Hayden Anderson, Bing Xue, Jesse Wood et al.

Notes: 
    * Explainable AI (XAI) considerations and perspectives from the social sciences. 
    * Interpetability - levels of understanding of how an outcome is produced. The focus is on the system not the outcme. 
    * Explainability - Explanation of how the outcome is rpodces. The focus is on the outcome not the system. 
    * Pyschology - from psychology these are around teh wrong way, in psychology we reverse these definitions. 
    * Interpretable: ability of a human to meaning from a given stimulus. OFten high level of imprecision. 
    * Context - who is it interpretable to? Is it the target audience of the interpretation? Is their assumed knowledge needed in order to correctly implement the model. 
    * Linear model is interpretable - we consider linear models white-box because they are linear... 
    * BUt if we have a linear model to our Grandmother, she would consider it nonsense. 
    * Interpretability can be applied to GP trees. I.e. protected division, specialized operators, uncommon tree structures. 
    * Fuzzy-trace theory is a mental model of how we process incoming stimuli, verbatim to gist scale. We constantly forget. 

2022-08-07 - Deep Learning
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time:** Wednesday 3pm - 4pm, **Attendees:** Demelza Robinson, Bastiaan Kleign, Jesse Wood et al. 

Demelza gave a talk on Gaussain Processes for Regression. GPs can be used for fitting a function to data, we can train GP to predict given training data. 

Notes:  
    * Demelza gave a talk on Gaussian Processes for Regression. GPs can be used for fitting a function to data, we can train GP to predict given training data. 
    * GPs find a condition distribution of the targets given the inputs. Compare our prior beleifgs with the likelihood using Bayes Rule. 
    * To make predictions, average over all possible parameter values. Linear model lacks expressiveness, so we can map the probelm to a higher dimensionanal sapce using basis functions. 
    * Probabilistc classification where test predictions are in the form of class probabilities. 
    * Tricker because we could assume the likelohood function was Guassian, but for classification the likelohood is non-gaussian, but posterior can be approximated. 
    * Confusing words: posterior, prior, positive semi-definite, basis functions, kernel trick, reproducing hilbert space, heteroskedastic. 
    * heteroskedastic - can observe the same features of an instance, at two seperate times, and belong to different classes at each time. 

2022-08-18 - FALSIP 
~~~~~~~~~~~~~~~~~~~
**Location:** CO350, **Time:** Thursday 2pm - 3pm, **Attendees:** Kaan Demir, Bach Hoai Nguyen, Jesse Wood et al.

Kaan gave a presentation on feature selection for mult0lable classification. 

Notes: 
    * Predicting a set of labels that correspond to an instance, Different to binary/multi-class classification, as an instance can correspond to multiple classes simulateniously. 
    * Problem tranformation - can be used to transform the multi-label problem into a simpler representation. 
    * Binary - break multi-label into a set of binary classification problems, run classifier, then concatenate labels together to get the final prediction.
    * For example, the SVM model uses a one-vs-rest approach for multi-class, it breaks the problem into a series of binary classification problems, same solution proposed for (Kerber 1992, kerber1992chimerge).
    * The issue with binary classification approach is that the classifiers are independent, and share no information about the problem. 
    * Classifier chains, label a class, and append that label as a feature to the next classifier. 
    * Issue with classifier chains, there is no clear order to chain the classifiers in. 
    * Label powerset, convert multi-label into multi-class problem. 
    * Issues - Very sparse and comibinatorial explosion for many label problems. 
    * Multi-label KNN (ML-KNN) - an extremely expensive version of KNN that is suited to multi-label datasets natively. KNN scales by number of instances, with online learning, that requires the whole training set in memory. The complexity scales by number of lables and instances. 
    * Sparsity-based classification, diffentiable and works for optmizing with neural networks. 
    * Feature selection - reduce the number of features; they can be irrelevant, redundant, complimentary.
    * Search - search methods can be sequential, guided stochasity (EC), balanced with a quality-metric (classification accuracy) and feature interactions. 
    * FS methods, MRMR, reliefF, classifier, ML-KNN, Sparsity-based models. 
    * Sparsity based models: :math:`\min{z} ||XZ - Y||_{2,1} + \Lambda ||z||_{2,1}`. Where :math:`||z||_{1,2}` is the sum each column norm, and, :math:`||z||_{2,1}` is the sum each row norm.
    * The :math:`\Lambda` provides a constraint to sparesely cluster the feature into a lower dimensional representation. E.g. Globe :math:`\to` Plane, the geometric locality (distance) is lost in this transformation. 
    * Use a kernal to capture the non-linear interactions between features, conserve the distance in the manifold, A gaussian affinity matrix.
    * Laplacian eigenmap, the overall "connectesness" of each feature toward all other features. Graph regularization - embed laplacian eigenmap into a linear model. 
    * Use evolutionary computation, and co-operative co-volution. We require EC to search this comibinaratorially explosive search space for multi-label feature selection.
    * Caveat: the choice of kernel, i.e. Radial Bias Function (rbf), Gaussian, Linear, determines the non-linear patterns the Laplacian eigenmap can capture.  
    * Similar to SVM, a kernel that captures the complexity of the data accurately must be chosen, but in this case it is difficult to evaluate the kernel quality, difficult to find an appropriate metrix. 
    * Gist: A linear model for mult0label classification taht considers the non-linear interactions, preserving the manifold without a graph regularization term. 

2022-08-19 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** C0364, **Time:** Friday 11am - 12pm, **Attendees:** Bach Hoai Nguyen, Jesse Wood.

Notes: 
    * A one tree approach is very primitive Genetic Program for multi-class classification. 
    * Multi-tree GP is better. Each tree seperates one class from the rest. There is one tree for each of the classes. 
    * E.g. one tree would seperate blue cod from all the other class. The next tree would seperate snapper from the other classes. 
    * Class dependent crossover, ensure crossover is only happening within the individual trees, not across the multi-tree, i.e. crossover between snapper and bluecod trees is not allowed. 
    * Mutation can happen to any tree, not class dependent. 
    * Class depedent Feature Construction; Genetic Programming for Multi-feature Construction on High-Dimensional Classification (Tran 2019, tran2019genetic). 
    * Multi-feature Construction (MFC), Feature Selection (FS), Genetic Programming (GP). 
    * Mid-trimester break: for next two weeks, meeting time will be Thursday 12pm (the old time).


TODO: 
    * Implement Multi-tree GP in Python. 
    * [ ] Read (Tran 2019, tran2019genetic). 

2022-08-19 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time:** Friday 3pm - 5pm, **Attendees:** Peng Wang, Yi Mei, Bach Hoai Nguyen, Jesse Wood et al. 

Notes: 
    * Marine Conference - Bing, Meng and Fangfang are at the Marine Conference in Nelson. So Yi had to chair the ECRG meeting this week. 
    * IEEE Symposium - Register for the IEEE Symposium, presenting at this event provides free membership to the IEEE organisation. New students SHOULD register and attend. Most IEEE conferences require IEEE membership for attendence. 
    * EvoStar - a major conference that we are targeting. The deadline is September it is important to start preparations for work we with to submit for this conference. 
    * Progress: 
        - A single GP tree for multi-class classification, 
        - will extend this to be multi-tree GP, 
        - using one-vs-rest approach, 
        - also prepare for FASLIP presentation next week, 
        - content from this can be revised for IEEE Symposium. 

Peng Wang gave a presentation "Differential Evolution-Based Feature Selection: A Niching-based GP Approach". Peng is a third-year PhD candidate. 

Notes:
    * Motivations - curse of dimensionality, redundancy in features, improve compute, and perhaps performance. 
    * Goals - multi-objective (MO) proivdes a pareto optimal set of candidate solutions that are non-dominated and balance desired objectives; i.e. accuracy and size. 
    * Filter, Wrapper, Embedded are the three approaches to feature selection (FS) and feature construction (FC). 
    * Embedded - feature selection is embedded in the learning process. 
    * Wrapper - feature selection method servers as a wrapper for an existing prediction algorithm (i.e. classifier). 
    * Filter - a ftiler method is featuress are evaluated absed on general characteristics, a classifier model is not used. 
    * Aim - get better feature subsets with less redundnacy. Feature subsets that maintain classification algorithms. 
    * Tradeoff - there is a tradeoff between accuracy and feature subset size. 
    * Differential Evolution, is a method that optimizes a problem iteratively trying to improve a candidate solution with respect to a given measure of quality. https://en.wikipedia.org/wiki/Differential_evolution 
    * Real - for DE the genotype is some form of real-valued vector. 
    * Operators - the mutation/crossover operators make use of the difference between two or more vectors in the population to create a new vector. 
    * Differentiable - DE does not require the optimization problem to be differentiable, it does not rely on gradient descent as many deep learning methods do. This allows for more difficult tasks without domain expertise / differentiable mathematics. 
    * Genotype - a genotype is the genetic makeup of an individual (in biological terms), genotypes are encoded as strings or trees of values, it is the representation, the encoding. 
    * Phenotype - a phenotype is the expression of a trait, this is the decoded genotype, we evaluate a candidate individual to get its predictive output is the solution space, evaluated in the environment. 
    * Example. The gene for ginger hair is recessive and stored in an individuals DNA - this is the genotype. The hair colour of the indiviudal, are they ginger. is the expression of that trait - phenotype. 
    * For evolutionary computation, an example is, Genotype - a GP tree, Phenotype - the prediction/output. 

TODO: 
    * [ ] 2022-08-24 Register for the IEEE Symposium 
    * [ ] 2022-08-25 FASLIP Presentation 
    * [ ] 2022-08-31 Prepare IEEE Symposium presentation 
    * [ ] 2022-09-30 Plan a EvoStar 2023 paper. 