Minutes
=======

This page contains the minutes for our weekly meetings. 

2022-02-23 - Planning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** C0355, **Time**: Monday 1pm-2pm , **Attendees:** Jesse Wood, Mengjie Zhang

Notes: 
    * Faculty of Graduate Research (FGR) - office on Kelburn Parade. 
    * Forms and information for enrollment is available at the FGR website. 
    * Booked a room for study in MARU101 - Desk 33
    * See Duncan in ECS for an account. 
    * Can work up to 12 hours per week. 
    * Let supervisors and faculty know about trips out of Wellington. 
    * Start as provisional registration, then candidate - write proposal, fully registered - proposal accepted. 
    * Two required meetings, FASLIP (Thursday 2pm-3pm), ECRG (Friday 3pm-5pm).

2022-02-28 - FGR
~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Matthew Vink, Jesse Wood

Notes: 
    * This meeting covers enrollment, we will be confirming details, forms, contacts.
    * PhD Supervisors: Bing Xue, Mengjie Zhang.
    * Documents: 
        1. Confirmation of study - AIML 692 code. 
        2. Fees assessment - two weeks to pay levees. 
    * Information sheet:
        1. Community for needs bank details. 
        2. Tony mcGloughin - School Administrator. 
        3. Confirmation of Proposal Registration Form (CoPR).
        4. Mathew Vink - helped me enroll today. 
        5. Student levees - 2 weeks due.  

2022-02-28 - FASLIP
~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Wednesay 2pm-3pm , **Attendees:** Matthew Vink, Jesse Wood

Neil Dodgeson - Cambridege lecture
    * How to not give a presentation. https://vimeo.com/51597270
    * How to present a paper. https://vimeo.com/7833850

Notes: 
    * Simular to ENGR401 stuff
        * Dont need slides.
        * Trip check technologies. 
        * Face audience. 
        * Relevant stuff only. 
        * No animations. 
    * Research Talks 
        * Don't type the script. 
        * Planning, a lot of time before writing slides. 
        * Audience, can change how you deliver a presentation. 
        * Highlight key points on the last slide. 

2022-03-07 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Bastiaan Kleign, Jesse Wood, et al. 

Papers: 
    1. Conditional Diffuction Probablistic Model for Speech Enhancment. https://arxiv.org/abs/2202.05256
    2. A Study on Speech enhancment on Diffusion Probabilistic Model. https://arxiv.org/abs/2107.11876

Notes:
    * Diffusion models got attention for synthesising images (i.e faces, animals). 
    * Later, it bet the GAN on standard benchmarks. https://proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html
    * Train it to add noise, the reverse the process. 
    * Diffusion: learning to denoise speech signal. 
    * Isotropic gaussian distribution? https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic
    * Learn the signal-to-noise difference, not the mean signal. 
    * Diffusion markov chain is intractable, so we use Elbo to from an approximate objective function. 
    * Ratios and constants to ensure the mean and variance don't explode or vanish. 
    * New Mailing list for DL updates. 
    * Next week: Bayesian Transformers. 

2022-03-08 - Induction
~~~~~~~~~~~~~~~~~~~~~~
**Location:** AM101, **Time**: Monday 2pm-4pm , **Attendees:** Neil Dodgeson, Jesse Wood, et al. 

Notes:
    * Bastiaan slide example for meetings. 
    * Neil Dodgeson - Faculty of Graduate Research Dean. 
    * Faculty of Graduate Research (FGR). 
    * Workshops, writing events, professional development. 
    * Website https://www.wgtn.ac.nz/fgr
    * Workshops are practical and hands-on. 
    * Thesis bootcamp - 20 writing hours. 
        * Aimed at final year students. 
        * June november 
    * Research room 
        * Review, tips, stories, events, resources. 
        * Updates monthly. 
    * Candidate progress form (CPF)
        * Report on 6 monthly progress in a report. 
        * May / November. 
        * Required, not academic, supporting evidence. 
    * 4 weeks annual leave, no formal process. 
    * Suspensions, for illness, bereavement, work. 
    * Forms for aforementioned available online. 
    * Proposal: first major milestone. 
        * 12 month deadline. 
        * no extensions available. 
    * Automatic re-registration for first 2 years. 
    * Constructive relationship with supervisor. 
    * PhD certificate: competent to do invidual research. 
    * Work expert in our PhD Research topic. 
    * Regular meetings times. 
    * Student/supervisor - same page for expectation.
    * Bring agenda to meeting.
    * Project management techniques - scrum, agile. 
    * 2pi rule for time estimation. 
    * Secondary supervisor - (usually) hands off role. 
    * "The only way through it, is to do it." 
    * Books, publications, thesis - different expectations for each course. 

2022-03-10 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-4pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood 

Notes:
    * Let Bing/Meng know about any financial difficulties. 
    * Topic ideas: 
        1. Multi-objective 
        2. Evolutionary computation
        3. Domain expertise. 
    * First two-weeks - extensive background reading. 
    * ECRG - meeting tomorrow from 3pm - 5pm. 
    * CoPR - fill out by the end of March. 
    * Individual induction - copy Bach in email for meeting. 
    * Add Bach to gitlab/github for the paper latex file. 

2022-03-10 - FASLIP
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-4pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Jeff Hawkins - Thousand Brains Theory: https://www.youtube.com/watch?v=O4geanMOsyM

Notes:
    * Voting, similar to droupout, bagged ensemble. 
    * Many models (sub-networks) for the same thing. 
    * Sparse networks, efficient -> noise tolerant. 
    * Only update in one area, without need for back-propagation, doesn't require a full training for each new instance. 
    * Builds a full world model, not a model for each task. 
    * Thousand brain theory - solution to No Free Lunch. 

2022-03-11 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Hui Ma gave presentation on Evolutionary Computation Approaches to Web Service Composition - https://link.springer.com/article/10.1007/s10732-017-9330-4

Notes:
    * Meng will discuss how to write a proposal. 
    * EuroGP conference - ask my supervisor to register. 
    * Introduced myself to the group 
        * paper - finish writing my Summer Research paper. 
        * enrolled - lots of paper work. 
        * Finish writing the paper properly. 
    * Abdullah (lab neighbour) first week in group.
    * Evolutionary Computation Approaches to Web Service Composition. 
    * Over 40 publications in the area. 
    * Holidy booking service used as an example. 
    * Organize services into re-usable modules. 
    * Service composition is a NP-hard problem. 
    * A global search is not possible, a heuristic based local search is required. 
    * Evolutionary principles and techniques - crossover, mutation.
    * Automatcally create hybrid services through composition. 
    * Don't reinvent the wheel, use existing libraries instead. 
    * Scheduling, routing, resource allocation, service composition - all possible for EC. 
    
2022-03-17 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood. 

Notes:
    * Daniel can provide domain expertise for writing the chemistry sections for the paper. 
    * Multi-objective - classify chemical compounds and their percentage.  
    * Multi-label - one instance can belong to multiple classes. 
    * copy Bing and Bach for induction email from Georgia. 
    * pymoo  - multi-objective python library. 
    * Read/write summaries for papers as I go - write content for second chapter iteratively. 
    * Send Daniel conclusions / contributions of paper in email, then organize a follow up meeting.

2022-03-17 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Ying, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Ying suggested a talk on Multi-objective Evolutionary Federated Learning https://vimeo.com/552900291

Notes: 
    * Yaochi Jin - University of Surrey. 
    * Multi-objective machine learning. 
    * Centralized and federated learning. 
    * Evolutionary multi-objecive federated learning. 
    * Evolutionary federated nerual architecture search. 
    * Multi-objective - gives a solution set, as their are tradeoffs between objectives. 
    * Dominance, no X is worse and Y, and X is strictly better than Y for object A. 
    * pareto front (See tegmark2020ai) set of optimal solutions.
        * accuracy, diversity. 
        * Inverse generational distance (IGD).
        * Hypervolume - nadir 
    * Optimize for minimal complexity implies interpretability. 
    * Centralized learning - one database. 
    * Localized learning - everyone trains their own model. 
    * Privacy techniques: 
        * Secure multi-party computation. 
        * Differential privacy. 
        * Homomorphic encyption. 
    * Federated learning 
        * train a high-quality centralized model with training dataq distributed over a large number of clients. 
        * Each with unreliable and relatively slow network connections. 
        * horizontal - all attributes, batches of data. 
        * vertical - trained on subset of attributes (i.e. security reasons). 
    * Federated learning objectives 
        1. Maximise learning performance. 
        2. Minimize communication cost. 
    * Their work efficiently reduce the number of connections while maintaining similar performance. 
    * Neural architecture search (TODO - watch the rest and take notes!!!)

2022-03-18 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

A talk on Geometric Semantic Genetic Programming by Qi Chen https://link.springer.com/chapter/10.1007/978-3-642-32937-1_3

Notes: 
    * We have published heaps of papers that are highly cites and hot papers according to https://www.webofscience.com/wos/woscc/basic-search toool that the university has access to.
    * Top 1% of papers cited per discipline for computer science journals. 
    * Evolving neural networks with evolutionary computation. 
    * Me: reading psychology papers on how the brain works with memory - hunting for relative simply neuro-science ideas to apply to machine learning. 
    * Geometric smenatic genetic programming (Morgalio 2012, moraglio2012geometric) https://link.springer.com/chapter/10.1007/978-3-642-32937-1_3 
    * Semantic genetic programming methods. 
    * Traditional GP ignores program semantics. 
    * Consequences - ragged gentype-phenotype mapping. 
    * Is it possible to make GP aware about the effects of the program execution? 
    * Semantics: 
        * Semantics differs from syntax. 
        * Semantics related to the problem domain. 
        * Semantics inform program design (Tegmark 2020, tegmark2020ai).
    * Measure semantic distance between current program and target output (multi-dimensional loss function). 
    * Genetic operators: 
        * Semantic aware cross-over (SAC) 
        * Semantic similarity-based cross-over (SSC)
        * Semantic similarity-based mutation (SSM)
        * Senantic tournament selection. 
            * t-test for statistical signfician with assessing selection. 
    * Search directly in the semantci space of the program. 
    * Semnatics of offsrping must sit in between the ntercept between its two parents in semantic space. 
    * Therefore each offspring minimized distance to target semantics. 
    * Each generation gets closer to the target semantics, or atleast closer than the furthest parent. 
    * Independent of data, good effect on improving generalization, althougt not actual claim made in paper. 
    * Geometric semantic programming leads to a unimodel fitness landscape - a cone where the apex is the target semantics. 
        * manhattan distance - square based pyramid.
        * euclidean distance - cone. 
    * Efficient implementation - only store changes to program tree, similar to git version control - except for GSGP. 
    * GSGR Red (reduce), simplify problems by expanding and recomputing. 
    * Locally geometric semantic crossover (LGSX).
        * Make offsrping similar to eachother than their parents. 
    * Random desired operator (RDO), exploit interoperability of instructions, + can be reversed with -, * can be reversed with division, + and * are communicative. 
    * Semantic backpropogation - decomposibility of the process if important for BP. 
    * Angle aware metrics - larger angle metrics iis more likely to generate offspring closer to target semantics. 
    * Permutations GSX and Random Segment Mutation 
    * Semenatic distance (euclidean) is the same as the loss, just looking at it from a different point of view. 
    * Can geometric smeantic programming work in an unsupervised or combinatorial problem? (Possibly not unimodel semantic space)
    
2022-03-21 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Hayden Dyne, Bastiaan Kleign, Jesse Wood, et al. 

Talk by Hayden on two papers: 
    * End-to-end driving via conditional imitation learning (Cai 2020, cai2020high) https://ieeexplore.ieee.org/abstract/document/8460487/
    * High-speed autonomous drifting with deep reinforcement learning (Codevilla 2018, codevilla2018end) https://ieeexplore.ieee.org/abstract/document/8961997/ 

Notes: 
    * Model-free reinforcement learning - does not rely on human understanding of world and design controllers. 
    * Human driver is the trajectory with is the goal, uses a professional driver playing the game with a steering wheel. 
    * Model performs on different track difficulties. 
    * Reward function is scaled by velocity, so faster lap times are rewarded. 
    * Works for 4 different kinds of vehicles, although the truck struggles to achieve same performance as lighter ones. 
    * Second paper - e2e 
    * Far easier to use real-world data on driving that has already been collected than generate simulation data. 
    * Data augmentation used to help network generalize to new scenarios and edge cases not in the training data. 

2022-03-24 - Faculty Induction
------------------------------
**Location:** Zoom, **Time**: Monday 10am-11am , **Attendees:** Georgia Dix, Jesse Wood, Bach Hoai Nguyen.

Induction to my PhD studies with supervisor and faculty. 

Notes: 
    * Expectations
        * Supervisor
            * Uni life 
            * Framework 
            * Networking 
            * Assessment 
        * Me
            * action plan 
            * identify problems 
            * administration 
            * CDP (6 monthly report)
    * Marking can thesis can take up to 6 months - can work during this time. 

2022-03-17 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood. 

Notes:
    * Daniel can draft the chemistry parts for the paper. 
    * Draft the full paper with Bach, then send to Daniel. 
    * Read "From evolutionary computation to the evolution of things" - Nature
    * Can start coding now - explore ideas for ENGR489 and EC on existing data. 
    * Transformers, LSTM, GAN - yet to be applied to GC-MS data in literature. 
    * CNNs for GC, likely due to libraries, hype, understanding, Diffusion of innovation. 
    * Scuba diver experiment for context-dependent memory is a good analogy for noise in ML models.
    * Came up with evolutionary ideas, like sexual selection, but (Miller 1994) did it quite some time ago.
    * Idea for EC, a dynamic environment where complexity increases, classes or features are added - similar to evolution IRL. 

2022-03-24 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Ying, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Ruwing Jiao suggested a video on Bayesian Optimization from Mark Deisenroth https://www.youtube.com/watch?v=_SC5_2vkgbA 

Notes: 
    * Recommended background reading on this topic: 
        1. A Tutorial on Bayesian Optimization of Expensive Cost Functions (Brochu 2010, brochu2010tutorial) https://arxiv.org/pdf/1012.2599.pdf
        2. Taking the Human Out of theLoop: A Review of Bayesian Optimization (Shahriari 2015, shahriari2015taking) https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306
    * Latent Structural Support Vector Machine (Miller 2012) - **TODO** find this paper/project. 
    * Deep learning often involves a lot of hyper-parameter tuning, this is usally done by the practitioner model. 
    * Alternative approaches: 
        * Manual tuning 
        * Grid search 
        * Random search 
        * Black magic (i.e. lr is 1e-3 is "good")
    * Computationally expensive to search for global maximum in hyper-parameter search space. 
    * Globally optimize a black-box approach to evaluate (e.g. cross validatio error for a massive neural network). 
    * Use a probabilistic model to approximate the black-box model for the hyper-parameter search. 
        * create a proxy model - this learns an approximation of the space - with less computational cost to query that space. 
        * referred to ass proxy / approximate / surrogate. 
    * The standard model for optimizing a bayesian model is a gaussian process. 
    * Evaluate the proxy function once, this saves computation. 
    * A gaussian process minimized the uncertainty of the proxy function.
    * It samples the feature space at the minimum value of the shaded area (== uncertainty).
    * It repeats this often, until the proxy function is close enough to the true objective. 
    * Exploration - sample areas with high uncertainty. 
    * Exploitation - sample places with low mean. 

2022-03-25 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Andrew Lenson, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Andrew gave a talk on Genetic Programming, Explainability and Interdisciplinary AI.

Notes: 
    * Heaps of students successfully submitted papers to the Gecko conference. 
    * Possible to publish in conference at different levels; paper, poster, etc. 
    * If a paper is declined, revise with reviewer comments, and resubmit as poster. 
    * Qurrat Al Ain - Cancer research in AI. 
    * Swiss roll manifold problem
        * Reduce a manifold to a 2D visual representation. 
        * 2D path is representation of non-linear dimensionality reduction (NLDR).
    * Geo-desic, shortest path from A to B, not shortest euclidean distance. 
    * Lower dimensional space is referred to as an embedding. 
    * We can use AI to learn or approximate this embedding (if the problem is intractable).
    * Ways to estimate the intrinsic dimensionality of the dataset - statistical techniques. 
    * Kaka - count distinct nnumber of birds at Zealandia. 
    * GoPro for data collection combined with crack for Kaka. 
    * Law - predicting sentencing lengths with PLSR on judge summaries. 
    * Names with high/low probabilities are often historic cases referred to as 'guidance judgements'. 
    * Combine data analysis and domain expertise to infer knowledge about sentencing lengths. 
    * Home detention or communtiy service are associated with shorter sentences. 
    * Future work, take humans out of the loop, and make sentencing deterministic.
    * ^ This can be done, because their are extenuating circumstances that require a judges opinion.
    * Also, if all sentences are automated, there would no longer be guidance judgements being set historically. 
    * Law is a dynamic and decentralized system, unique and specialized for each country, case, individual, etc... 
    * Research more productive on letting judges analysis their blindspots, and identify bias.

2022-03-28 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Daniel Braithwaite, Bastiaan Kleign, Jesse Wood, et al. 

Daniel Braithwaite talked about two papers related to machine learning for audio wave construction:
    1. Deep Audio Priors Emerge from Harmonic Convolutional Networks https://openreview.net/pdf?id=rygjHxrYDB
    2. Harmonic WaveGAN https://www.isca-speech.org/archive/pdfs/interspeech_2021/mizuta21_interspeech.pdf

Notes: 
    * The idea is to look at harmonic convolutions, think convolution layer but designed for audio. 
    * WaveGAN and Harmonic WaveGAN use deep learning on audio signals. 
    * Harmonic, is better suited towards audio signals, than wave alone. 
    * Harmonic considers local connections / adjaceny better. 
    * **TODO** Read these papers and add to notes. 

2022-03-31 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Jesse Wood. 

Bing and Meng were both unwell this week. Important to send minutes for this meeting to them.

Notes: 
    * Augmentation - boost performance on the fish part dataset. 
        * Time-shift, shift data backwards and forward, to get time-invariant generalized model (may not work well).
        * Impute data, combine existing samples, add noise, etc... 
    * Worked on CNN from ENGR489 for classification task.
        * Issues with keras and sklearn libraries, stratified cross-fold validation and one hot encoding don't play nice together. 
        * CNNs, we use 1D convolution and pooling layers on time-series data. 
        * Existing ML + GC literature also use CNN for classification and regression tasks. 
        * These models are powerful for extracting features in spaces with local connectivity. 
        * Aim to use EC to perform neural architecture search for CNN hyperparamters - these differ for each dataset. 
    * Both EC and Bayesian Optimization approaches work for neural architecture search. 
        * However, EC has more interpretable results, e.g. a genetic algorithm produces an explainable tree. 
        * Neural networks are black-box and esoteric, we understand how (i.e. back-prop, SGD), but not why? 
        * EC produces simpler representations, that can be prodded with domain expertise.
    * Important to read heaps for first few months of PHD. 
        * Take original notes that can contribute toward a backgroup chapter of my proposal. 
        * Get an idea of what has been done, and what I want to do. 
        * Still reading psychology textbook on memory and the brain to establish conceptual framework for learning.
     
2022-03-31 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Fangfang Zhang, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Fangfang suggested a video called the Big Reset 2.0 https://www.youtube.com/watch?v=-ePZ7OdY-Dw

Notes: 
    * Reinforcment learning for Robotic Arms. 
    * Deep blue beat Kasparov, but no AI can set up the chess board, a 6 year old can do that. 
    * Hugh Herr designed his own AI legs https://www.youtube.com/watch?v=CDsNZJTWw0w
        * AI prosthesis is cost prohibitive for the masses, but may work with diffusion of innovation in future. 
        * Prosthesis can up upgraded over time, biological body parts cannot, hardware/software updates for legs. 
    * Fake news - Jon Stewart said MSM has more trackers than ANY other media (adult entertainment, torrent sites, social media included).
    * Chomskey, MSMs job is to sell the educated privelaged wealthy elites as an audience to the corporations advertising. 
    * AI algorithms - social media, fake news, incentives. 
    * AI autonomous warfare proliferation - we need to ban slaughter bots https://www.youtube.com/watch?v=pOv_9DNoDRY
    * AI used for traffic management, screen-time punishment - pick up phone at cafe and pay the bill. 
    * RoboMaster - robot warfare, mechatronics, AI - physical robot warfare as a game/competition. 
    * Cosmo - Boris Sofman https://www.youtube.com/watch?v=U_AREIyd0Fc 
    * Narrow-AI and no free lunch problem - AI is good at solving very specific tasks, but not general intelligence. 
    * I have an industry project, that has real-world applications in a factory settings - i.e. reduce bycatch and maximize efficiency of food processing for fish. 

2022-04-01 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Andrew Lenson, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Meng and Bing were unwell, so Yi chaired the research group meeting. 

Notes: 
    * Bach (my supervisors) first day lecturing for COMP102. 
    * Me: I got 98% accuracy on the fish species dataset using a 1D CNN.
    * Shorter meeting, workshop cancelled, due to Meng being unwell.  

2022-04-04 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Ciaran King, Bastiaan Kleign, Jesse Wood, et al. 

Ciaran King was gave a talk on "Experiences using Github Copilot".
    * Understands the context of code, can make abstractions for helper methods. 
    * Can write documentation for codebases.
    * Not software "correct" code, but (likely) the code we were going to write. 
    * Can write tests for codebases with very little leading. 
Daniel Braithwaite on "Fixed Neural Network for Stenography"
    * Hide messages in adversarial neural network. 
    * Pre-trained stenograph, results in non-zero error, we need perfect reconstruction for encryption.
    * Face anonymization, post a persons face online, then regenerate the face, but encrypt the private face. 
    * This lets friends anonmyously share images with their face online, without revealing their identity.
Bastian - contractivity of neural networks.  
    * Signal processing worries about getting non-stable linear filters for signals. 
Jesse Wood 
    * Evaluating Large Language Models Trained on Code https://arxiv.org/abs/2107.03374
        * 70% accuracy for basic DSA problems. 
        * Can't solve more difficult problems - doesn't optimize solutions for performance. 
        * CoPilot outperforms other state-of-the-art NLP code generation models. 
        * Requires "fine-tuning", supervised human intervention to hint towards correct answer. 
    * Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions https://arxiv.org/abs/2108.09293
        * 40% of code written with CoPilot has cybersecurity vulnerabilities.
        * CodeQL and other static analysis tools used to define the security of the code.
        * Security is a shifting landscape, WannaCry, Log4J - zero days kept secret by intelligence agencies. 
        * This is true of all code, the training data was written by humans. 
        * Potential vulnerability for future attacks if hackers know open-source repos are training data.  
        * Don't treat copilot as a "glass cannon", it doesn't deserve a false sense of security.

2022-04-07 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bing Xue, Jesse Wood.

Notes:
    * Use an existing neural network architecture search algorithm - application analysis. 
    * Callaghan may have extra data work with - arrange a meeting with Daniel. 
    * Pre-traning, tranfer learning, NIST dataset for GC refraction index. 
    * Look at existing proposals, get an idea for mine - possible to submit proposal early. 
    * State-of-the-art, is 50-50 whether it works or is a bust - good to have a backup based in existing literature. 
    * Pareto front with tradeoff between complexity and accuracy. 
    * Proposal does not lock me into using a particular method (i.e. SVM, EC, PSO). 
    * Idea: make sure students have a decent grasp of the field before conducting their own research, if not then read more.
    * Later try out ideas in the proposal, and see if they work. If they don't change tact.

2022-04-07 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Qi Chen, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Qi Chen showed a talk "Artificial Intelligence: A Guide for Thinking Humans" by Melanie Mitchell from https://www.youtube.com/watch?v=NMUqvhuDZtQ

Notes:
    * Shannon, Simon, Minsky - all though AGI was 15 years off in their own time. 
    * Andrew Ng - AI is the new electricity. 
    * Elon Musk - nobody would listen - https://www.youtube.com/watch?v=4RMKLyaqh_8 
    * Deep learning brought back the hype for AGI. 
    * "An Anarchy of Methods" - Joel Lehman 2014. 
    * AI, Machine Learning, Deep Learning, onotologies of fields and their popularity change over time. 
    * Deep learning looks at AI as an aritficial brain - enter the Artificial Neural Network (ANN) - the connectionists.
    * CNN based on the limited understanding of the human brain in 1950s neuroscience. 
    * Facebook used CNN AI for facial recognition when a user uploads a photo. 
    * ImageNet is a famous supvervised classification task that was generated through crowdfunding internet "slave" labour. 
    * Famous 94% result for image classification has a sample size of PhD student (Andrew Kaparthy) - the fake news embellished the story. 
    * Self-driving, stopped fire truck on the highway, the long tail of AI, edge cases. 
    * Adversarial attacks on neural networks, crack networks to make wrong predictions based off of their flaws. 
        * "Intruiging properties of Neural Networks" https://arxiv.org/abs/1312.6199
    * Trick self-driving cars into driving through stop signs with stickers, that make it think it is a speed limit sign. 
    * "I wonder whether or not AI will every crash the barrier of meaning." - Glen Carlo Rote 1988. 
    * "common-sense" machine learning, WinnaGap NLP problem. 
    * DARPA - competition to design a machine with the common-sense of an 18 month old baby. 

2022-04-08 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Harith Al-Sahaf, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

General: 
    * University drops their vaccine mandate https://www.wgtn.ac.nz/covid-19/settings-and-mandates/removal-of-vaccine-mandate 
    * EvoStar conference is coming up soon - April 20-22nd, Spain Madrid http://www.evostar.org/2022/eurogp/
    * Me: 
        * CNN performs relatively well on fish part dataset with manually tuned hyper-parameters. 
        * Also, mentioned MegaSYN annecdote about adversarial attacks on machine learning models for protiens to manufacture deadly nerve toxins.

Harith Al-Sahaf gave a talk on Malware Analysis https://www.al-sahaf.com/harith/ 

Notes: 
    * Malware analysis determines the functionality, origin and potential impact of a given malware. 
    * Applying EC techniques to malware anaylsis. 
    * Harith and the university have a lot of publications in this area https://www.al-sahaf.com/harith/publications.html
    * Siemese neural networks used to identify an unknown instance to a known malware for similarity. 
    * Yann LeCunn invented the idea for Siemese neural networks in the 1980s. 

2022-04-11 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-5pm , **Attendees:** Madhurjya Dev Choudhury, Bastiaan Kleign, Jesse Wood et al.

Madhurjya gave a talk on "Time Series analysis for Machine Health and Diagnosis". 

Notes:
    * "Image-<MUFFLED> tranlation with Conditional Adversarial Networks". 
    * pix2pix 
    * Nuisance parameters is any parameter which is not of immediate interest, but must be accounted for in those parameters which are of interest. 

2022-04-18 - EvoStar #1
~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Madrid, **Time**: Monday 7pm - 6 am, **Attendees:** Jesse Wood, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al.

More information can be found on the full conference program - https://easychair.org/smart-program/Evo2022/

Gabriella gave a keynote address on "Illuminating Computational Search Spaces" https://www.youtube.com/watch?v=EyynDbXnwic

Notes: 
    * Visualisation, explanation, informed configuration. 
    * Complex networks, Local Optima Networks (LON), Search Trajectory Networks (STNs). 
    * Graphs exist since 1800s with Eulers famous bridge problem. 
    * Networks coming from different systems share topological structure. 
    * Force-directed graph layout algorithm, borrows ideas from physics (i.e. simulated annealing). 
    * Difference between Euclidian distance and traversal path distance between nodes (see Andrew's manifold talk from 2022-03-25 - ECRG)
    * Fitness landscpaes f(S,N,F) , search space (S), n<unreadable> (N), fitness (F).
    * Funel - local optima in a course grain structure that can minimize energy. 
    * Local optima Network, nodes - local optima from hill climbing heuristic, edges - transition between local optima. 
    * Number partitioner, given a random set of numbers find a partition at value k, such that the two disjoint sets have equal sums. 
    * Map fitness landscape as a graph, compress the representation, to get an explainable visualisation for a funnel. 
    * Travelling Salesman Problem (TSP) - apply pertubations to existing solutions to find new solutions in the fitness landscape. 
    * CMLON, compress a Local Optima Network into a smaller representation that is easier to understand. 
    * Genetic improvement to CMLON. 
    * STN, allow for representative solujtions. 

Christian Raymond gave a talk on "Multi-objective Genetic Programming for Symbolic Regression with the Adaptive Splines Representation" 

Notes: 
    * Overfitting if a difficult problem for GP, because of flexibility of representation. 
    * Difficult to regularize overfitting in GP. 
    * Parisomony pressure, population distribution control, order of non-linearity. 
    * Limitations: structural complexity has minimal correlation with generalization. 
    * Estimate complexity of a model by estimating a model's behaviour over a subset of training space. 
    * Limitations: difficult to reliably estimate, complexity vs error is a trade-off. 
    * Semi structured representation - a solution to both limitations above. 
    * Spline: defined by multipled low degree k (cubic) polynomials smoothly. 
    * GG-AWS-PP - Adaptive wieght Splines with parsimony Pressure. 
    * Apply multi-objective optimisation, for loss and complexity objectives, both considered. 
    * Training paretor fronts, 2D rperesntation of fintess landscape between two objectives. 
    * Creates interpretable representations which are easier to understand than genetic algorithms. 
    * Works well on low to medium dimension feature sets. 

Another talk on "Morphologicl development of Robots [...]" 

Notes: 
    * Development of agents, aging, is beneficial for generating complex agents. 
    * Voxel based soft robots (VBS). 
    * Aggregates of soft cubes (voxels). 
    * Neural networks for voxel controllers, a net for each voxel. 
    * New voxels are added to the model over time. 
    * This is a scheduling function, that must alloclate the correct time to increase the complexity of the model. 
    * <unlegible> based morphology representation. 
    * Different development schedules. Early development, uniform development, no development. 
    * Early development seems more beneficial than uniform development, the artificial mimics the biological. 
    * No development, shows large deviation in the results, fuzzy accuracy. 
  
"A new evoltionary algorithm based home monitoring device for Parkinsons Dyskinesei"

Notes: 
    * AUC based fitness. 
    * Adaptive size fitness <unlegible> - allocate fitness absed on a subset of training data. 
    * Aaply different representation width lengths using GP to see [...] (coffee break for me)

Zhixing gave a best paper nominee talk "An investigation of Multi-task Linear Genetic Programming for Dynamic Job Shop Scheduling"

Notes: 
    * Job shop scheduling problem. 
    * Complicated dynamic NP-hard combinatorial problem. 
    * Make decisions based on imperfect information. Instead, we use heuristics to decide schedule. 
    * We can't rely on a single heuristic alone, dynamic environment means we must change heuristics in real-time. 
    * Hyper heuristic - a search mechanism to find a heuristic selecting model. 
    * GP has good interpretability, a tree can easily be understood by humans. 
    * Seasonal variance - more demand for ice-cream in summer for an ice cream factory. 
    * Multi-task, conflicting or tradeoff between goals of different stakeholders. 
    * Multi-task model - one ring to rule them all - one model that can balance multiple tasks at once. 
    * Linear gentic programiing, register based instructions, creates directed acyclic graph (DAG). 
    * DAG > Tree; can use diffrent topological structures to perform cross over for DAG, tree reperesentation is limiting. 
    * Operators: linear crossover, macro mutation, micro mutation. 
    * Multi-population based genetic programming (GP) - sub populations that develop in isolation with crossover (migration) allowed. 
    * homogenous/heterogenous - diff-same / same-diff - utilization/objective functions. 
    * M^2GP does not perform the same as LGp methods, M^2GP (tree based) state-of-the-art does better. 
    * Likely because: (1) too large variation step sive, (2) ineffective initializaiton strategy. 
    * One and multi-objective population methods have similar performance. 
    * Graph based crossover is a <unlegible> genetic oeprator 
    * But not used in this work, but is a research direction in ECRG. 

David Wittenberg gave a talk on "Using a denoising autoencover genetic programming to Control Exploration and Explotitation in Search"

Notes: 
    * Capture relevant properties of parent population in a latent representation. 
    * Model (auto-encoder) is trained to reconstruct the input. 
    * Problem: don't want to learn the identity function. 
    * Solution: denoise (slightly mutate) parent to avoid overfitting anf force the auto-encoder to generalize. 
    * Level of corruption can be used to control the exploration and exploitation of genetic algorithm. 
    * Paper explores this idea, we want a latent representation that is a lower resolution to let the model generalize better. 
    * Subtree mutation: can't control corruption. 
    * They propose Levenshtein edit, a genetic operator for mutation on the representation string. 
    * Convert tree into infix string reperesentation - then perform mutation operations on that string. 
    * Levenshtein edit, insert, delete, mutate - with an edit percentage (this determines level of corruption). 
    * The  stronger the corruption, the shtonger the exploration. 

Nicholas Fontbonne gave talk "Coperatative Co-Evolution and Adaptve Tree Composition for a Multi-Rover Resources Allocation Problem"

Notes: 
    * Multi-agent - agents act indepdentdenlt. 
    * Competetive, mixed, co-operative sum games from game theory. 
    * Zero-sum, shared-sum, shared-fitness agents. 
    * In cooperative case, all information about the individual is lost (collectivist idealogies "the greater good"). 
    * A shared fitness that promotes social welfare is not a good learning signal. 
    * Marginal contribution - contribution of an individual agent to the team. 
    * Evolutionary algorithm for multi-agent problems. 
    * issues: stuck in local optima, high computation cost. 
    * solution: grouping mechanism. 
    * reousrce seelction problem: each resource has a satisfcation score. 
    * k is the group size for the grouping mechanism. 
    * Co-operative co-veolutionary algorithm for adhoc autonmous agents. 
    * Efficient anytime larning without apriori knowledge of the problem. 

2022-04-21 - Weekly
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Wednesay 12pm-1pm , **Attendees:** Bing Xue, Bach Hoai Nguyen, Mengjie Zhang, Jesse Wood.

Notes:
    * EvoStar conference yesterday, not much work for covid isolation week. 
    * Demelza is giving a talk tongight for EvoStar. 
    * Likely to attend EuroGP in-person next year. 
    * EvoCNN - encodes basic CNN components. 
    * Apply GP to classifier problem directly. 
    * Try several techniques (initially). 
    * Also later what new tasks Daniel may want.  

2022-04-21 - EvoStar #2 
~~~~~~~~~~~~~~~~~~~~~~
**Location:** Madrid, **Time**: Tuesday 7pm - 6 am, **Attendees:** Jesse Wood, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al.

More information can be found on the full conference program - https://easychair.org/smart-program/Evo2022/

Demelza gave a talk on "Genetic Algorithm for Automated Spectral Pre-processing in Nutrient Assessment"

Notes:
    * Rapid collection, non-destructive, construct a model. 
    * Partial Least Squares Regression (PLSR). 
    * Spectral data is easy to collect, but the pre-procsing is a bottleneck. 
    * PLSR tranlsates the feature space into latent varaibles. 
    * Spectral pre-processing, remove noise and redundant intensity values. 
    * Manual process for pre[rpcessing spectra is laborious and lacks standardization. 
    * Automate pre-processing for choosing appropriate techniques and their order of application. 
    * Representation, a two chromosome structure that encodes objectives for: 
    * Fitness function: combination of MSE and number of features. 
    * GA-PLSR-PPS perform btter for most cases and has a smaller standard devition for IR and Raman. 

Zakaria Dehi gave a talk on "A machine learning based approach for Economics-Tailored Applications: The Spanish Case Study". 

Notes: 
    * Use AI for dynamic budget allocations in governmental bodies. 
    * (1) ETL, (2) Profiling, (3) Predicting. 
    * ETL - gathered 30,000 economic features, and identified 50 types of related economic metrics. 
    * Pre-processing required to make the building blocks for the ETL model. 
    * Different metrics can have the same profiles for all cities (i.e. unemployment/retirement).
    * Eliminate the redudnat metrics (duplicate class profiles) - for concise metric space. 
    * DCGA-kMeans - unsupervised clustering algorithm to determine profiles (k). 
    * Profiling deals with indentifying groups og cities based on an economic profile dervied from metrics. 
    * Prediction: Long Short-Term Memory (LSTM) - recurrent neural network. 
    * This unlocks the time series component of ecnomic data, persistance of time observed by the model. 
    * The ideal number of profiles for most metrics was 3. 
    * Massive feature and metric reduction - creates meaninful data through feature construction. 
    * Can decrease complexity of number of cases when officials must make complex politicy decisions based on near-infinite combinatorial search spaces. 
  
Daniel Lopez gave a talk on "EvoDesigner: Towards Aiding Graphic Design"

Notes: 
    * Readability, balance, innovation, style - all measured by Mean Squared Error (MSE). 
    * Adobe plugin to produce variations of a design based on rough input using evolutionary computation. 
    * Intitial input is a rought sketch, that paints an idea of a possible layout. 
    * Evolutionary engine creates a good design graphic based on that initial input. 
    * Then the engine generates variations of that good design using evolutionary computation. 
    * Still needs work, but offered a good tool for aiding ideation in graphic design. 

Julia Reuter gave a talk on "Genetic Programming-Based Inverse-Kinematics for Robotic Manipulators... <unlegible>".

Notes: 
    * Develop prototypical solution for Inverse Kinematics (IK) problems. 
    * We want closed-form solutions, apply to non-standard SCARA, with explainable representations (equations). 
    * Kuke koubot - was the brand of SCARA they ran their simulations on. 
    * Different objective functions + co-creation / co-evolutionary approaches.
    * IK-CCGP was compared to an ANN. It did better than the ANN. 
    * Co-evoltuion for the two-joints, performed better than for 3 (a more complex task). 
    * The equations were tested on simlated - not real world - robotic arms. 
    * But these equations can be verified later, by applying them to real-wrold systems, and testing for collisions/singularities. 
  
Partick Indri gave a talk on "One-shot learning of Ensembles of Temporal Cage Fomrulaes for Anomaly Detection in Cyber-Physical Systems".

Notes: 
    * Monitor behaviour of CPS, e.g. AV, power plant, medical monitoring, security systems, smart house. 
    * CPS are dynamic - need to quantify the system in terms of time. 
    * Signal Time Logic (STL), is an expression grammer for time based operations. 
    * GP oeprators to construct STL for optimized performiang at controlling a CPS. 
    * One-shot algorithm, GP is population based, we can use the population to build ensembles.
    * Water treatment CPS was used as dataset for training/test. 
    * Their method performs well when comapred to other state-of-the-art methods. 
    * One-shot G3P achieves more complext formulates than the standard G3P. 
    * One-shot approach cpatures wave temporal operators better than standard G3P.
    * It can learn more complext models, that include "time-based" operators (i.e. STL). 
  
Gloria Pietropoli gave a talk on "Combining Geometric Semantic GP with Gradient-descent Optimisation"

Notes: 
    * Geometric smentaic metric programiing (GSCP) is a well known variant of genetic programming (GP), 
    * GSCP used recombination and mutation operators that have a clear semantic effect. 
    * Combine GSCP with Adam Optimizer. 
    * GP, we can presresent an individual as a point a real n-dimensioanl semantic space. 
    * Geometric smenatic operators: 
        * Geometric semantic crossover (GSC). 
        * Geometric semantic mutation (GSM). 
    * Adapate Moment estimation (Adam, kingma2014adam), is a first order gradient-based optimization of stochastic functions. 
    * GSCP makes a good jump in the solution space, then adam can refine a candidate solution. 
    * Two approaches: (1) one-step GSCP then one-step ADAM (HYB-GSCP), or, (2) full GSCP then full ADAM (HCH-GSCP).
    * HYB-HSCP does better than HCH-GSCP. 

Dominik Sobaria gave a talk on "Program Synthesis with Genetic Programming: The Influence of Batch Size"

Notes: 
    * Program synthesis with genetic programming. 
    * Anayluse perfromance and generalization ability of programs generated by GP. 
    * Batch size effects programming synthesis. 
  
2022-04-22 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Notes:
    * Small editing (layout) is allowed when responding to referee feedback on a paper. 
    * Me: "I worked on catching COVID his week"
    * New datasets from Daniel, he proposed some new research objectives https://mail.google.com/mail/u/0/?hl=en/#inbox/FMfcgzGpFWTMDnsjKxswvdtWdrhKzKdH 
  
2022-04-22 - Proposal Workshop 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Mengjie Zhang, Jesse Wood et al.

We had a proposal workshop hosted by my former supervisor Mengjie Zhang. 

Notes: 
    * First (of many) proposal workshops about preparing this document. 
    * Target adueince for workshops are first year PhD students (me). 
    * PhD is a great way to go for a very good job later in life. 
    * Don't have to do theoretical work, but focus on application, key point is to make a major contribution to the field. 
    * First year of PhD, we are a student not a candidate. A candidate has passed their proposal. 
    * Bad PhDs can be converted into a masters degree - this happens if the proposal is very bad (try to avoid). 
    * Read 50-100 papers in order to get a good grounding for a PhD proposal. 
    * Some people do not have the capability to do a PhD, they can propose, but must demostrate their capability with preliminary work. 
    * Structure: 
       1. Introduction 
       2. Literature survey (read) + 
       3. preliminary work (code) + 
       4. contributiosn (major) / miletones / thesis outline / resources. 
    * Overall goal: a single over-arching scientific/engineering argument to unify my PhD stydies as one body of work. 
    * 100,000 words (is a lot of words) is the expectation for a PhD thesis. 
    * Minimize dependencies - encourage modularity for research objectives. 
    * Coherence is very important for choosing research objectives. 
    * Literature review should cover most recent work and domain specific (biology/chemistry) papers. 
    * Honours work can't be counted as preliminary work (but can be references as a citation). 
    * Not required to publish - but encouraged to do so (prevents concurrent thinking issues later down the line). 

2022-04-21 - EvoStar #3
~~~~~~~~~~~~~~~~~~~~~~
**Location:** Madrid, **Time**: Tuesday 9.30pm - 11.30pm, **Attendees:** Jesse Wood, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al.

The final day of the conference, involved a plenary talk and prize giving ceremonies. 
More information can be found on the full conference program - https://easychair.org/smart-program/Evo2022/

Pablo Geruas gave a EvoMUSART talk on "Evolutionary construction of Stories that Combine Several Story Lines"

Notes: 
    * Star Wars used as an example during the talk for storylines (kudos :D). 
    * Use evolutionary algorithms to evolve <unlegible> <unlegible> plot lines for a story. 
    * Made up of 20 popular plot lines from a textbook, assign a plot line to each character. 
    * Tricky to come up with a genetic representation for a story plotline.
    * Genetic representation must be manipulatable with genetic operators. 
    * Difficult to design an objective/fitness function. 
    * Fitness: validity - continuity of life and death, each character falls in love once (simplification). 
    * Fitness function averages over all validity metrics to evluate fitness. 
    * Eolvutionary approach good for creating multi-plotline stories with semantically valid discourse. 
    * Speaker gave a similar application e.g. a robot that generates plot line scripts for a leage of legends game. 
    * https://nil.fdi.ucm.ec/ 

Pedro Larrange gave the plenary talk on "Estimator of Distribution Algorithms In Machine Learning" 

Notes: 
    * Machine learning is a large focus of artificial intelligence nowadays. 
    * Construct a model from data, perform (non)-parametric optimization. 
    * Estimation of Distribution Algorithm (EDA). 
    * Bayesian Networks  (DAG + CPT), Directed Acyclic Graph (DAG), Conditional Probability Table (CPT).
    * Feature subset selection, Filter - only consider features, Wrapper - evaluate performance at machine learning task. 
    * Classification - a greedy algorith. 
    * Artifical Neural Network (ANN) - (Baluja 1995). 
    * Logistic regresion (Roles et al 2008) - led to intrepretable results for complex models. 
    * AdaBoost (Cagninin et al. 2018) - aggregated voting system between classifiers. 
    * Hierarchcial clustering (Fan 2019). 
    * k-Means (Forgy 1965), most popular culstering algorithnm, centroids shiftwed with hill climbing strategy. 
    * Reinfrocment learning (Honda and Nishive 2008) - relies on conditional markov fields. 
    * EDAs have not yet been applied to Support Vector Machines (SVM) - possible future work here (for me). 

2022-04-26 - Thesis Workshop 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Kirsten Reid, Jesse Wood et al.

Kirsten Reid, from learning support, hosted a Thesis Workshop for the Faculty of Graduate Research (FGR). 

Notes: 
    * Shape/formate appropriate for the field - i.e. Engineering / Artifical Intelligence. 
    * Linking ideas between chapters - see About Face 3 for great example. 
    * (Sub)headings are used effectively - maximum three levels of indentation for understandability by reader. 
    * Topic sentence for each new paragraph. 
    * If needed - glossary, acronyms, abbreviations - up front. 
    * Reminder of key concepts when needed, jog the readers memory, guide the reader along (a thesis is long). 
    * "Reader-friendly", avoid jargon, use plain text. 
    * Sentence length should vary to avoid monontonous tone (see https://bit.ly/38mTmAE)
    * Avoid too many nominisations - e.g. "we decided" rather than "we came to the decision" - focus on action verbs over their nouns. 
    * Don't have front-ended setnences, cut the wheat from the chaff, 1/3 of writing can be removed for brevity usually. 
    * Make appointment with Student Learning - limited to 50 minutes. 
    * Sentence cohesion worksop coming up soon. 
    * Kirsten happy to reply to Emails for further advice on this workshop. 
    * Three voices: research, data, researcher. 
    * From FGR: 4 workshops coming up for PhD students in May.

2022-04-28 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO 352, Cotton Building, **Time**: Monday 12pm-1pm , **Attendees:** Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Notes: 
    * Can claim GC dataset research as preliminary work for the REIMS data in my proposal. 
    * Transfer models from GC to REIMS data (likely) - this supports the preliminary claim. 
    * Pre-training on NIST GC refraction index data - needs a parametric (neural network) model for this technique. 
    * TODO: Apply EvoCNN to GC data. 
    * TODO: Apply Genetic Programming (GP) data to GC data. 

2022-04-28 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Jesse Wood (me) showed a talk "Telsa AI Day (supercut)". 

Notes: 
    * Disclaimer: I submitted this talk request on the 21st of April (7 days ago) - this is not political, just a coincidence. 
    * "Iron Man in Real life" - a comment from the Zoom chat. 
    * If you are interested in another high level summary:
    * Lex Fridman AI Day (summary by MIT researcher) - https://www.youtube.com/watch?v=ABbDB6xri8o
    * Here is the full video, it includes references to academic papers:
    * Full AI Day (3 hours long) - https://www.youtube.com/watch?v=j0z4FweCy4M
    * Supervisor liked my running commentary and links to further watching. 

2022-04-22 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

General Notes:
    * Zhixing won best paper for EuroGP. 
    * Shaolin had a TEVC paper revision. 
    * Vincent and Tao had a paper accepted by CEC. 

Talk from Xioying Sharon Gee on "Text Representation" 

Notes:
    * Inverse document frequency - use to elminate words that are too common - e.g. "the". 
    * Word emeddings, analyse similarity (relateness) with PCA - reduce to lower dimensional space (2d / 3d). 
    * CNN, Transformers, Attention, BERT, Pre-training. 
    * Pre-training of deep bidirectional transformers for language understading (BERT). 
    * Word masking, pre-training, next sentence prediction (NSP). 

2022-04-02 - Research Workshop 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 11pm-1pm , **Attendees:** Jesse Wood 

FGR hosted a workshop "How do I publish and disseminate my research". 

Notes: 
    * https://scopus.com - see citation statistics for papers/journals - useful metrics to judge quality. 
    * Can see stats for a journal, and comapre this to other jounrals, to assess the credibility. 
    * These tools are useful for making/measuring goals as a researcher. 
    * http://login.webofknowledge.com/ -  useful tool for mainly STEM disciplines. 
    * Eslever Journal Finder - provices acceptance nodes. turn around times - useful for finding which journal is appropriate. 
    * www2.cabells.com/jouranlytics - see a journal's publishing frequency, may be anually, this is a long wait. 
    * Consequences: 
        * Who reads it? 
        * Does it publish articles like yours? 
        * Does your style match? 
        * Would you need to change to submit? 
        * Peer-reviwed? 
        * Time to publish? 
        * Substantial paper. 
        * Tolerable rejection rate. 
        * Preferred type of journal. 
        * Solid reputation/metrics. 
        * Many articles a year. 
    * Keywords are good for SEO, they ensure the discoverability of work, and extend your audience. 
    * Rejection happens, may provide feedback (may not). 
    * Query letter - send an abstract to a journal to test the waters - see if a paper is appropriate. 


2022-04-11 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-5pm , **Attendees:** Mathew O'Connor, Bastiaan Kleign, Jesse Wood et al.

Matt O'Connoer gave a talk "Unsampling Artifacts in Neural Audio Synthesis" https://ieeexplore.ieee.org/abstract/document/9414913

Notes: 
    * Imrpove the quality of audio using a neural network approach. 
    * CNN is for images, but we need an architecture for audio specifically - this must handle audio specific problems (e.g. time, harmonics). 
    * Convolution (collapse), transposed (expand). 
    * Transposed convolutions are widely used. 
    * Upsampling methods: 
        * Stretch (insert zeros) 
        * NN (nearest neighbours)
        * Linear 
    * The unsampling algorithm leaves artifacts in the output sample. 
    * Spectral replicas emerge when sampling/discretizing the signal. 
    * All up sampling methods sound very similar - in the human audible range. 
    * NN, no artifacts, but frequency filter at zero frequency. 
    * Out of distribution test, shows major artifacts, since it wasn't in training. 
    
2022-05-05 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO 352, Cotton Building, **Time**: Monday 12pm-1pm , **Attendees:** Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Notes: 
    * Reading this week - Evolutionary ML Survey (60% done). 
    * Apply GP to the GC fish dataset. 
    * Later consider the time domain: terminal set, symbolic regression. 
    * GP Hello World! Try find a tutorial for this. 
    * DEAP does not have "Elitism" - I will need to implement my own.
    * Elitism:
        * Keep the top solutions between generations. 
        * Ensures performance can't decrease. 
        * Does not guarantee performance will increase. 
    * Future work - transfer learning can be: 
        * Paramters 
        * Model 
        * Feature (selected/constructed)
    * Terminology; domain adaptation, domain generalisation. 
    * N.B. I shoudl record my tutor meetings on zoom (even if they are in person) to make use of Panopto's free transcription software while I am a staff member. 

2022-05-05 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Peng Wang, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Peng Wang proposed a talk from Prof. Zhihua Zhou on "From Pure Learning to Learning+Reasoning" https://www.youtube.com/watch?v=LAvRDCcXCMc 

Notes: 
    * Most machine learning techniques rely on large database of labelled training data. 
    * We can leverage unlablled data, to improve performance on labelled data. 
    * Self-learning, use a classifier to apply psuedo classes to unlabelled data. 
    * SETRED 2005 - data editing, cleans up self learning. 
    * Active learning (AL); uses an oracle to query (label) unlabelled data, rely on minimizing queries to oracle (this requires human supervision). 
    * Representative AL approach - informative/representative. 
    * AL requires human-in-the-loop. 
    * Semi-supervised learning (SSL) - see Lex podcast for more details https://www.youtube.com/watch?v=FUS6ceIvUnI 
    * Indepedndent and Identically Distributed (IID). 
    * Semi-supervised SVMs (S3VMs) (Zemma 201u6, zemmal2016adaptative).
    * Using inlabeleld data to ensure the decision boundaries are drawn through low density areas. 
    * Tri-training approach, three learners, tha can teach eacother, and perform ensemble learning. 
    * Ensemble learning - uses multiple models and combines them to make a prediction. 
    * Holy grail: machine learning + logical reasoning. 
    * Probabilistic Logic Programming (PLP) - heavy-reasoning light-learning.  
    * Statistical Relation Learning (SRL) - light-reasoning heavy-learning.
    * Proposal: abductive learning: 
        * Deductive 
        * Inductive 
        * Abductive - Inversly embed deductive reasoning into inductive reasoning. 
    * Knowledge Base (KB), a series of first-order logic predicates. 
    * Instance --> Psuedo lables --> Psuedo groundings --> KB. 
    * Optimize minimzed inconsistency in the system. 
    * ABL does not rely on ground-truth labels. 
    * SSL for court sentences in China, similar to Andrews work (see "2022-03-25 - ECRG" above). 

2022-05-06 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Notes:
    * Celebtration for AI group since Bastiaan recieved his RSNZ Fellowship - Wednesday 18th May from 12 pm - 1 pm in CO3250. 
    * Hayden, camera-ready (?) paper for Gecko (?) workshop. 
    * Candidate Development Plan (CDP) is due by the end of May - this is due every 6 months (see email https://mail.google.com/mail/u/0/#search/CDP/FMfcgzGpFgvPXpqBwmBtBvqBZPxxQFtB).
    * University policy for thesis students does not require use to publish any papers. 
    * STEM has a publishing-forward culture when compared to other fields (e.g. law or humanities). 
    * Evolutionary Algorithm:  
        * Initialisation
        * Cycle 
            1. Evaluation. 
            2. Selection. 
            3. Reproduction. 
            4. Repeat 1-3 until termination condition. 
    * Meng has not seen anyone miss genetic operators, but a fair number of research papers ommit the "evaluation" and "selection" sections. 
    * DON'T OMMIT THESE SECTIONS! (Unless you have a very good reason not to). 
    * Ideally, have a nice flow diagram in a paper, to explain the training process for the model (this figure is a good use of space!!!). 

2022-05-09 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Ciaran King, Jesse Wood, Bastiaan Kleign, et al.

Ciaran King gave a talk on "Learning from Protein Structure with Geometric Vector Perceptrons" (Jing 2020, jing2020learning) https://openreview.net/forum?id=1YLJDvSx6J4 

Notes: 
    * Perverse incenstives for acadmiecs to over sell their work and a major reproducability crisis in deep learning. 
    * Graph nerual Networks can be used for protien folding. 
    * Equivariance to rotations - if the networks thinks the same instance rotates is a completely different structure, this is very inefficient. 
    * Instead we want rotation invariant representations for things like protiens. (Like we wan't time invariant representations for gas chromatography). 
    * Voxels are 3D pixels, these can be used to make a 3D representation of an instance, which then applies a 3D Convolutional Neural Network. 
    * We think that (1) message passing and (2) spatial convolution, are both well suited for different types of reasoning. 
    * In protein folding, their are chemical propoerties of protiens that simplify the combinatorial search space for the graphical neural network. 
    * This is similar to how the AI Feynman (Tegmark 2020, tegmark2020ai) used properties of physics equations to simplify symbolic regression. 
    * I would like to apply simplification using domain expertise in chemistry to my gas-chromatography and mass spectrometry data. 

2022-05-12 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Thursday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes: 
    * Audio-only private recordings of weekly meetings for study purposes is ok. 
    * Plant & Food collecting information on students (like myself) that have been working for them. 
    * Conferences: 
        * GECCO - Genetic Evolution Computation Conference. 
        * EvoStar (attended in 2022). 
        * IEEE CC - IEEE Congress of Evolutionary Computation. 

    * Camera ready? This is ready for print, a final version of the paper that has responded to feedback and been formatted for the journal. 
    * CDP - Candidate Development Plan, is due this month. Scope is for 6 months only. This is my first so I have no goals from previous CDP. 
    
TODO: 
    * [ ] Readings on Genetic Programming (GP)
        1. TranEtAl2015GPfsfc https://link.springer.com/article/10.1007/s12293-015-0173-y
        2. tran2019genetic https://www.sciencedirect.com/science/article/pii/S0031320319301815?dgcid=rss_sd_all
        3. tran2017new https://ieeexplore.ieee.org/document/7956226
    * [ ] Send a draft CDP to my supervisors for feedback. 

2022-05-12 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Cuie Yang, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Cuie suggested the talk "You are what you read" https://www.youtube.com/watch?v=Du7qLsToW-o

Notes: 
    * Light reading, figure fishing - 80% comprehension. 
    * Skip to pictures like a kid, we are all kids at heart. 
    * Deep read, creative/critical reading.
    * Shallow reading is important, a paper has four parts.
        1. Title 
        2. Abstract 
        3. Introduction 
        4. Rest 
    * Quick reading and shallow learning is most important for the majority of readers. 
    * Reviwers are busy people, bathroom reviews, a lot of reviwed is based on superficial details. 
    * Notes - a format for notes from skim reads of a paper: 
         1. About 
         2. Problem 
         3. Interesting 
         4. New
         5. Neat
    * Two word titles: "Snappy sampling". 
    * Find collegaues to collaborate with and share knowledge. 
        * Give and take, time is a resource. 
        * what NOT to read? 
    * Future work: 
        1. Good ideas
        2. Improvements
        3. Applications/Extensions
        4. Your opinion
    * Come up with your own: 
        * Other domains 
        * Moon-shit
        * Related ideas

TODO: 
    * [ ] How to get your SIGGRAPH paper rejected https://www.kormushev.com/public/How_to_Get_Your_SIGGRAPH_Paper_Rejected-by_Jim_Kajiya.pdf

2022-05-13 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

General notes: 
    * Possible to ahve future meetings in person sson - waiting on university to update their covid policy. 
    * Bastiaan's celbration is next week. 
    * Meng asks who would like to go first at the start of each meeting. BE READY! FIFO 
    * me: GA Tutorial, Evolutionary ML Survey, 3x Papers to read. 
    * Journals are encouraging people to publish their work in interactive ways. See obersvable for visualization techniques. 
    
Junhong Zhao from another department gave a talk on "AI Effects (AIX) in Computer Graphics (CG)". 

Notes: 
    * Reconstructing reflection maps using a stacked CNN for Mixed Reality Rendering. 
    * Automates/improved re4ndering practises with good estimation for reflection maps. 
    * Gist: "Get accurate reflection maps on artificial objects in Augmented Reality Environments". 
    * Challenges: wide range of sensors/lens - people have different phones with different quality camersas. 
    * A robust neural network is needed to handle out of distribution and real world data. 
    * DLNet - "Adaptive Light Eximtation using Dynamic Filtering Terms". 

TODO: 
    * [x] CDP is due this Monday, for part I. (finished!)

2022-05-06 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Daniel Braithwaithe, Jesse Wood, Bastiaan Kleign, et al.

Daniel Braithwaithe gave a talk on "Estimating KL Divergence with Kernal Estimators". 

Notes: 
    * MINE - Mutual Information Neural Estimation. 
    * GANs suffer from mode collapse, due to a lack of diversity in GAN generators. 
    * MINE labels as model and loss are not typically convex, so convergence is not guaraneteed. 
    * Non-trivial to implement a MINE model.
    * Maxmimizing entropy is an intractable problem. Instead estimate thi for a GAN. 
    * Instead use KKLE (KL Divergence using Kernal Estimators), KKLE is convex. 
    
Maxwell clarke gave a talk on "Why Deep Learning Works". 

Notes: 
    * Zero Eigenvalue, parameters can be shifted without affecting the loss. 
    * Compress the area of the search space for efficiency. 
    * Networks which used generalisable representations are "simpler" than networks that don't. 
    * Occam's razor, or, 'simple as possible but no simpler' - Enstein. 

2022-05-26 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Thursday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes: 
    * Add proposal to the CDP goals. 
    * End of October - proposal due. 
    * Set 18th June for Australassian AI paper submission as soft deadline for my first paper. 
    * IP is 1/3 creators, 1/3 university, 1/3 commercial (note: still far better than previous industry experience). 
    * Code is not IP, but is copy-writable.

2022-05-26 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Cuie Yang, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Bach (my supverisor) suggested the talk "The Master Algorithm" from Pedro Domnigos. https://www.youtube.com/watch?v=B8J4uefCQMc 

Notes: 
    * Knowledge: evolution, experience, culture, computers. 
    * Each knowledge acquistion is an order of magnitude faster than the previous. 
    * 5 tribes of AI: Symbolists, Connectionists, Evolutionaries, Bayesians, Analogizers. 
    * Each tribute is related to a different field of study outside of machine learning. 
    * Each 5 algorithms have a master algorithm, a agneral method that with enough data can learn anything. 
    * Robotic machine that can perform biological experiments in drug discovery (see Lee Cronin Chemputer https://www.youtube.com/watch?v=ZecQ64l-gKM)
    * Google's famout "cat" network, was an ANN trained on Youtube videos, not surpisingly it became very good at recognizing images of cats. 
    * Koza took Evolutionaries one step fouther, by inventing genetic programming, representing a candidate solution in program semantics. 
    * Bayesians are the most fanatical of the tributes. Strict adherence to statistical inference through Bayes Theorum. 
    * As we see more evidence, probablites of certrain hypothesis will become more likely. 
    * Weakness of AI, is it cannot predict events that are not explicitly given in training, things that have never happened have P(A) = 0. 
    * Analogizers, Douglas Hoftstader, author of Godel Escher Bach. 
    * Vladimir Vapnik is the creator of Support Vector Machines (SVM). 
    * Kernal machines are the master algorithm for analogizers. 
    * A master algorithm, for general intelligence, would include all 5 tribes of AI. 
    * Pedro prposed the Markov Logic Network, objective function indepdendent. 
    * All AI fits the evolutionary paradigm; Evaluation, Selection, Reproduction, Fitness. 

2022-05-30 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Daniel Braithwaithe, Jesse Wood, Bastiaan Kleign, et al.

Bastiaan gave a talk on "Distributed Processing"

Notes: 
    * Knowledge required: graph theory, probability, convex optimisation, domain optimization. 
    * Parallel vs Distributed Processing. 
    * Dual Gradient Ascent. 
    * ADMM, PPMM. 
    * Parallel processing - fast as slowest node, central coordination. 
    * Distributed processing - no central coordination, any connected topology, gossip consensus algorithm. 
    * Gossip - take the mean of each group of nodes and communicate that between groups of nodes. 
    * Duual Gradient Ascent - requires strong duality (e.g. Slater's condition). 
    * Dual Gradient Ascent is useful for distributed processing. 
      * We assume f(x) is seperable, get dis<unlegible> 
      * Instead of global optimization, we optimize locally. 
    * Alternating Direction Method of Multipliers (ADMM). 
    * Lasso (L1) regularization is the most common predecessor to ADMM. 
    * ADMM converges much faster because it is quadratic. 
    * Fast enough, their are faster algorithms, but this is fast enough in signal processing. 
    * ADMM is a distributed MSE. 
    * Primal-Dual Method of Multipliers (PDMM). 
    * Lifting - add a new variable, then constrin them, this allows us to associate variables with models only. 
    * PDMMM was developed for solving decomposible optimisation problems in a distributed fashion. 
    * Note: traditional somewhat ad-hoc deviations, nicer alternative is is <unlegible> greater based. 

2022-06-02 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Christian Raymond, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

(Misc) Heiter Gomes is a new lecturer in the department. 
Christian Raymond proposed a talk "Aritificial Intelligence 1980s-2021 and Beyond" by Professor Jurgen Schimidhunter https://www.youtube.com/watch?v=pGftUCTqaGg&t=1s 

Notes: 
    * Long-short-term memory (LSTM) 
    * Recurrent Neural Network (RNN)
    * What is predicatbale is compossable [sic]
    * RNN requires unsupervised pre-training. 
    * LSTM does not require unsupervised pre-training - an improvement on the RNN. 
    * DanNET fast deep CNN based image processing revolution in 2011. 
    * Highway nets (May 2015) - first nerual network with over 100 layers, e.g. ResNET. 
    * DL networks suffer from the problem of vanishing and exploding gradients. 
    * LSTM used for google speech recognition, Amazon Alexa, (Samsung Bixby xD )
    * Compressed network search was the first RL to learn policies from video for a controller. 
    * Reinforcement learning LSTM, 2007-2010. 
    * 2019, deep mind bet Starcraft player. 
    * 2018, OpenAI Five, bet competitive players in Dota 2. 
    * World models + RL + controller - e.g. RoboCop AI soccer. 
    * Motivate controller to design experiments to improve the world model. 
    * 1990 Generation Adversarial Networks (GANs). 
    * nnaisense - the dawn of AI - his company. 
    * AI in 3D printing - additive manufacturing. 

2022-06-03 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

General Notes: 
    * Mass exodus of academics from university (Nature article). 
    * New PhD students have to be in New Zealand to study at VUW. 
    * Christian Raymond had a very intuitive video about AI history yesterday at FASLIP. 
    * Make sure simiarlity score on Turntin is less thean 25%, bearing in mind we can plagarize ourselves. 
    * Me: reading (lehman 2020, lehman2020surprising), writing my first draft of my paper for Australassian AI. 

Fangfan Zhang gave a presentation on "Responding to Reviewers"

Note:   
    * Peer review processing - some papers are accepted with major/minor reviews. 
    * AS long as a paer is accpeted, this is a success, in the peer-review process. 
    * R1, R2, R3, ... -> Revision 1, Revision 2, Revision 3, ...
    * It is common to have up to 4-5 revisions when submitting a paper to a journal. 
    * 7 is the maximum number of revisions within the ECRG group. 
    * Important to take the revision process very seriously, when publishing as an academic. 
    * When first receiving feedback, keep calm and carry on, read all feedback before responding. 
    * Respond to each point when replying to a reviwers comment. 
    * Can disagree with reviwers comments, but be professional about it, be resepctful. 
    * Make is easy for your reviwers! 
        * Write a cover letter. 
        * Copy text directly for short changes. 
        * Reference for larger sections. 
        * Use color to highlight changes. 
    * A good revisions would mean the reviewer does not have to re-read your original paper to accept the changes. 
    * Summary: 
        * Tale a break. 
        * Point-by-point response. 
        * Well reasoned arguments. 
        * Pay attention to details. 
        * Appreciate reviewers work. 
    * Papers need to make a contribution to science/journal.  

2022-06-09 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Thursday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes:
    * Print a bigger vesion of the paper for next time, fit to page setting. 
    * Meng uses emacs, Pondy used to use emacs for everything (i.e. email). 
    * GP paper on GC-fish data. 
    * GP - set max tree depth to 8. 
    * Protected division - don't divide by 0 -> NAN. 
    * Later, use more compelx GP variations. 
    * Australassian AI, papers need novelty. 

2022-06-02 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Quinglan Fan, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Quinglan Fan shared a talk on "The Extremes of Interpretability" https://tads.research.iastate.edu/dr-cynthia-rudin-extremes-interpretability-machine-learning-sparse-decision-trees-scoring-systems

Notes: 
    * Interpretable machine learnings models objey a domain-specific set of constraints. 
    * Needed for high-stakes or troubleshooting, important to have interpretable models for real-world problems. 
    * Wildfires CA, Google Air Quality, breezeomoter ML model failed. 
    * Glenn Rodriguiz, COMPAS score, AI that bas borken left a man in prison. 
    * Florida COMPAS data on recidivim (similar to Andrew Lenson's talk from 2022-03-25 - ECRG).
    * Blackbox model that is used in the criminal justice system. 
    * Correctional Offender Management Profiling for Alternative Sanctions (COMPAS). 
    * Very basic decision tree, corels, was able to outperform the COMPAS model. 
    * c4.5 and Cart are greedy top down algorithms that often overfit the training data. 
    * 1990s: non-greedy algorithm, and, statistiscians improve splitting criteria. 
    * Genetic Programming has been tried to create fully optimal decision trees. 
    * GODST 2020, fastest algorithm by 3 orders of magnitude. 
    * Dynamic programming/ brnach and band 
        * Eliminate duplication. 
        * no need to solve pure leaves. 
        * reduce search space by theorum: 
             * minimum support bound. 
             * onestep lookahead. 
    * Generalized and Scalable Optical Sparse Decition Tree (GODST)
    * Improved representation: 
        * store only the leaves. 
        * use bitvectors -> crazy fast. 
        * Extened computation (avoid repetitions) - caching 
        * Consolidation of repeated sub-problems. 
    * Results: 
        * Similar classification range. 
        * For custom lost, much better than greedy algos. 
        * Sparser than all heuristic models. 
        * orders of magnitude faster than next best model. 
    * Explaining deep NNs with saliency maps does not work. 
    * i.e. given an image of a dog, the salience map for a Husky and a Music Instrument both examine the same areas. 
    * "This Looks Like That", that forces a blackbox NN to be intereptable with a prototype layer. 
    * CUB-200, a staple computer vision dataset on birds. 
    * Even for black-box NN, we can have an interpretable model. 
    * Interpetable AAI algorithm for Breat Lesions (IAIA-BL). 
    * An AI, that used domain specific apporach of radoilogy, and presents the radiologist an interpretable model, 
    * They can understand the reasons behind decisions, be skeptical, and disagree or agree based on reason-based interpretable models. 

2022-06-10 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Qurret Ul Ain, Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Qurret Ul Ain gave a talk on "Artificial Intelligence to Diagnose Skin Cancer"

Notes: 
    * Early detection of cancer is fundamental in saving lbies. 
    * Moles are an example of Skin Cancer. 
    * Combine knowledge of dermetology + computer vision. 
    * A multi-disciplinary research project (like mine). 
    * Skin cancer can spread to the body, in later stages, it can reach the blood vessels, and lead to tumours. 
    * Skin cancer is a disprportionately high cuase of mortality in New Zealand - 4,000 diagnoses, 300 deaths per year. 
    * Causes: radiation, modles, high altitude, immune system, hereditary. 
    * Diagosis: 
        * A - Assymetry 
        * B - Border 
        * C - Colour
        * D - Diameter 
        * E - Evolving
    * Tool: punch biopsy (skin biopsy) - manually check a mole (very painful procedure). 
    * Computer diagnostics let a dermetologist determine if a punch biopsy is needed. 
    * Punch biopsy (painfuly) barrier of entry to diagnosis. 
    * This motivates computer vision techniques for detection. 
    * Multi-stage classification systems requrie significant expertise; segementation, remove hair, ambient lighting, rotation. 
    * Existing methods rely on gray-pixel computer vision, but colour is important too. 
    * Lens (camera) callbiration varries due to different resolutions of camera. 
    * Limitations of NNs, reduce all to 256x256 pixel images, this distorts the aspect ratio.
    * Hair removal - is this because of the training data? Because a dermatologist shaves a patients hair before scanning. 
    * Multi-tree GP has achieved better performance than single tree. 
    * Multi-tree is similar to multi-cellular organism from biology. 
    * Local Binary Patterns (LBP) is a dense image descriptor. 
    * Uses a sliding window of foxed raidus, it computes the value of the central pixel based on the intensities of its neighbouring pixels. 
    * Can concatenate LBP frm multiple channels (i.e. RGB) to construct a new feature vector. 
    * Classifications: 
        * Benign (not harmful)
        * Malignant (harmful)
    * Can extract tabular data with domain specific knowledge, interpretablity "we know these work". 
    * Same-index-crossover-mutation - group together features for one method from constructed feature vector - i.e. Blue channel LBP. 
    * j48, NB, SVM, KNN. 
    * Analyze the feature apperance as feature frequency, then relate to domain expertise/OG features, to get interpretable models. 
    * Wavelet: different-scale information (mean/std summary statistics). 
    * "Wavelet" features have perormed well in the existing literature; Exsemble classifier (i.e. AdaBoost) performs very well. 
    * Emsemble balances the apporach of several classification algrotihms, voting/bagging, best performance. 
    * Wrapper: based on the performance accuracy on a subset of the training data labels (y). 
    * Filter: based only on the features (X). 
    * Important to use balanced accuracy for imbalanced datasets, otherwise results are biased towards the majority class. 
    * GP has the opportunity to generate new knowledge that can be verifiedand encorporated by domain experts into their field. 

TODO: 
    * READ: GP for Multi-FC in skin cancer image classification. 
    * READ: Two-stage GP for auotmated dianoging Skin Cancer from Multi-Modelity Images 

2022-06-17 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Cui Yang, Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

General notes: 
    * No meeting next week due to the (first) celebration of Matariki as a public holiday. 
    * Writing is an important part of research, we refine our ideas, and correct our incorrect assumptions, we organize our thoughts. 
    * For example, when re-writing my draft paper, I realized my understanding of treated fish biomass waste was incorrect.
    * Something I would have never realized if I did not explicitly have to re-write my introduction section. 

Cui Yang gave a talk on "Domain Adaptation". 

Notes: 
    * Machine learning learns to predict data from identical and indepedendly (iid) probability distributions. 
    * Transfer learning, use knowldge from one problem and apply this to another problem. 
    * Motivation: training ML models can be expensive, requires hundreds of GPU/TPU hours. 
    * Goal: improve performance of a model by training it on data from a source domain, to improve performance in the target domain. 
    * Source: domain to extract valuable information from. 
    * Target: area to improve performance in. 
    * Types: 
        * Instance 
        * Feature 
        * Paramater 
        * Rational-knowledge 
    * Instance based adaptation; KMM, AdaBoost. 
    * Feature based adaptation; CORAL, TCA, Class-conditional. 
    * Deep domain adaptation; MK-MMD, CORAL, GRL, HDA. 
    * Domain invariant and specific parts in prepresentations. 
    * Partial domain adaptation: classes in target domain are a subset of that in the source domain. 
    * Transfer learning in dynamic environments; concept-drift tolerance, hybrid ensemble approach. 

2022-06-20 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Jesse Wood, Bastiaan Kleign, et al.

Jesse Wood (me) gave a talk on the paper "Hierarchical Text-Conditional Image Generation with CLIP Latents" (ramesh 2022, ramesh2022hierarchical) https://arxiv.org/abs/2204.06125 

Notes: 
    * CLIP trains an auto-enocder to have minimize the distance between image and text embeddings in the latent space. 
    * Those image embeddings are fed to an autoregressive or diffusion prior to generate image embeddings. 
    * Then this embedding is used to condition a diffusion decoder which produces an image. 
    * The model is trained on 250 Million images, and has 3.5 billion parameters. 
    * We can use CLIP to interpolate between two images in the latent space. 
    * As we increase the dimensionality of the latent space we can represent more complex hierarchical structures. 
    * CLIP fails at producing text, and reconstruction can mix up objects and their attributes. 

2022-06-23 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Thursday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes: 
    * After finished Australassian AI paper, start writing the proposal. 
    * Write the motviation and research objectives. 
    * Good to finish proposal early - before Christmas! 
    * All important conference deadliens are in Jan - Feb. 
    * Eye contact on zoom, is not possible with cameras and remote audience. 

2022-06-23 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Jiablin Lin, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Jiablin Lin showed a talk on "The Three Magic Ingredients of Amazing Presentations" https://www.youtube.com/watch?v=yoD8RMq2OkU

Notes: 
    * Three things to consider: 
        1. The Audience 
        2. The Speaker 
        3. Transformation 
    * Call to actions (CTA), what can the audience do? 
    * Then we re-arrange the roadmap and can get the presentation order. 
    * Can use this technique for speeches, emails, proposals. 
    * Common mistake to put too much information into a presentation/paper, sometimes less is more, more important to change mind of audience.

Jiablin Lin suggested "The margical science of story-telling". https://www.youtube.com/watch?v=Nj-hdQMa3uA 

Notes: 
    * A 99 cent horse head was sold for $62.00. 
    * He bought random items of little value, then attactched a sory to each object written by authors, this increased their value a thousand-fold. 
    * The more emotionally invested in something, the less critical you are of that thing. 
    * 13 months after "honey-moon" period, our neo-cortex and objective reasoning comes back. 
    * The same thing ahppens during an advertisment that aims for brand experience - i.e. Coca Cola. 
    * All story-telling generates dopamine, because we are waiting for soemthing, i.e. cliff hanger. 
    * Oxytocin makes people bond towards a person, it makes them feel human, we can do this by telling a person a story that makes the audience empathetic. 
    * Endorphins, laughing makes the audience relax, comfortable and more open. 
    * Devils cocktail; high levels of cortisel and adrenaline. 
    * Functional story telling: 
        1. Everybody is a good story-teller from birth. 
        2. Write down your stories! 3-4x times more than you think. 
        3. Pick story to enduce emotions you want. 
 
 2022-06-27 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Daniel Braithwaite, Jesse Wood, Bastiaan Kleign, et al.

Daniel Braithwaite gave a presentation on "Audio Source Seperation - Using GAN/flow priors"

Notes: 
    * We have two unknown source signals, mixed together into a single signal. 
    * Minimize distance between the original source and generated source, create a latent space that accurately encodes the mixed audio signals and their individual source signals.
    * Proposed approach is low audio quality, but it can seperate two audio signals. 
    * Problem with GAN approach is Mode Collapse (not iid data) and low quality audio. 
    * Flow based generateds, train an AI model to generate instruments from an audo strack. 
    * These flow-based models use the inverse STFT. 
    * Easy to verify the accuracy of an audio seperation model by listening to its output. 
    * Problems with likelihood maxmimisation, highly discontinious, "deep generative models don't know what they don't know". 

Bastiaan Kelign breifly discussed a paper "Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models" (bao 2022, bao2022estimating) https://arxiv.org/abs/2206.07309

Notes: 
    * Diffusion Probabilistic Models (DPM) are special Markov Models with Gaussian Transitions. 
    * Paper shows how to go from noisy-to-clean with a deterministic process. 
    * A new approach to diffusion based models.

2022-06-30 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Junhong Huang, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al.

General notes:
    * FASLIP was started 5-6 years ago in September 2016. 
    * Ying Bi's last day in New Zealand - she was in FASLIP from the beginning. 

Junhong Huang shared a lectured from Andrew Ng "Lecture 8 - Career Advice / Reading Research Papers" https://www.youtube.com/watch?v=733m6qBH-jI

Notes: 
    * Two topics: (1) Reading research papers, and (2) career advice. 
    * Many PhD students learn to read papers by osmosis, from those around them, picking up tips and tricks. 
    * Andrew Ng outlines his method for reading papers. 
    * Start with a complete list of papers (including blogposts / Github).
    * Skip around the list. 
    * How many? 5-10 papers is good for implementation. 50-100 is needed for thorough understanding and research. 
    * Everywhere he goes, he has a stack of papers that he takes around with him.
    * He reads 6 papers a week, and presents 2 to a research group. 
    * Take multiple passes over a paper: 
        1. Title, abstract, figues. 
        2. Intro, conclusion, figures + skip rest. 
        3. Read paper + skip math.
        4. Whole things + skip parts that don't make sense. 
    * Questions (after readings): 
        - What did authors try to accomplish? 
        - What were the key elements? 
        - What can you use yourself? 
        - What other references do you want to follow? 
    * Math: 
        - Rederive from scratch. 
        - Art Gallery, we see art student's sitting on the floor copying the work of the greats. 
        - Do the same for Machine Learning, copy the greats, learn to dervice (and even invent) algorithms. 
    * Code: 
        - Run the open-source code. 
        - Re-implement from scratch. 
    * Steady reading, not short bursts! Sparse repetitions work better than cramming. 
    * Goal for most PhD students is a job (either big company or startup). 
    * Either way, we intend to do important work! Leave the world a better place than we found it! 
    * Recruiters are looking for skills (ML + coding), and meaningful work. 
    * The idea candidated is a "T" shaped individual, someone who has a breadth of knowledge in AIML, and depth of knowldege in one or two areas of expertise. 
    * Course work is an efficient way to gaina bredth of knowledge in AI field. Also +reading papers, +relevant projects. 
    * Saturday morning problem: (1) read paper, (2) open-source contribution, (3) project, (4) TV? 

2022-07-01 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Friday 12.30pm-1pm , **Attendees:** Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes: 
    * SIGEVO member for free virtual attendence to GECCO conference.
    * Conference is in July 9th - 13th 2022, Boston Masacheseutts. 
    * Focus on tutors, conference has different tracks - i.e. GP, Neural Evolution, GA, Swarm Intelligence. 
    * Big names in Evolutionary computations are coming to Wellington. Good opportunity to meet. 
    * Math is an important part of machine learning, different areas of math required for different tribes of AI. 
    * Bach is on holiday... 
    * Writing the paper is taking longer than I thought it would. But making slow progress... 
    * Deadline for Australassia AI is July 17th (just over two weeks away). 

TODO: 
    * [x] Email Yi to become a member.
    * [ ] Register for GECCO 2022.

2022-07-01 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Shaolin Wang, Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al.

General notes: 
    * Any time after 1 pm is actaully 10 monutes past that time, so 1 pm is 1.10 pm, to give students a 20 minute break for lunch. 
    * Impact factor for 2021 was released. Many top journals in AI increased their impact factor - i.e. IEEE TPAMI (>24 now). 
    * Register as a SIGEVO member ($10 USD), Yi will buy membership, free attendence virtually to GECCO. 
    * Wolfgang Baanzhaf https://www.cse.msu.edu/~banzhafw/, a foundational figure in Genetic Programming, is visiting the university soon. 
    * AI undergraduate major has been approved by the university, I am involved in advertising for this major. 

Shaolin Wang gave a talk on "LRE for GP-evolved Policies for UCARP". 

Notes: 
    * Uncertain Capacitated Arc Routing Problem (UCARP). 
    * Use genetic programming to provide hyper-heuristics. 
    * Benefits: flexible representation, less domain knowledge, potential interpretability. 
    * Existing work to increase the interpretability of routing policy. 
    * Hard to describe the full mapping learned by a model.
    * Local explanations, easier to give explanations to less complex solution subspaces. 
    * Local Ranking Explanations (LRE) method, is used to explain the behaviour of routing policy in a decision situation. 
    * Dataset - 6 representative UCARP problems.
    * Results - by error, correlation and consistency. 
    * Table shows a linear model can represent the LRE for a routing problem.
    * Proposed a new LRE method for UCARP problems. 
    * This presentations recieved a best paper nomination for GECCO 2022. 

 2022-07-06 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Jesse Wood, Bastiaan Kleign, et al.

Bastiaan Kleign discussed a paper "Denoising Diffusion Implicit Models" https://arxiv.org/abs/2010.02502

Notes: 
    * Other approaches: Flowbased, VAE, auto-regressive, WaveNext+ 
    * Diffusion; reverse forward process where we gradually add noise. 
    * Problems: over-denoise, extremely computationally complex, many steps - same number backwards as forwards. 
    * Can be derived from Langevin Diffusion equation from physics. 
    * Consider a forawrd Markov Process that gradually replaces the signal with Guassian noise.
        :math:`q(x_t|x{t-1})= N(x_t;\sqrt{1-\beta_t} x_t - 1, \beta_t I)`
        where :math:`x_t = \sqrt{\alpha_t} x_{t-1} + \beta_t \epsilon_t`
    * :math:`\beta_1,...,\beta_t` is the noise schedule (fixed by designer), but some new papers automate schedule selection. 
    * We assume we can approximate the inverse process with a reverse Markov Process. 
    * Naturally objective function is the cross entropy - negative log liklihood --> KL, which can be made tractable with Jensen's inequality. 
    * The symbol :math:`\epsilon` is the noise, the objective function can be reformulated to predict the noise :math:`\epsilon`. 
    * This diffusion method can generate images of people that do not exist, but this takes 100,000 steps, so diffusion models are very slow. 
    * Langevin's Equation: :math:`m\frac{d^2x}{dt^2} = - \gamma \frac{dx}{dt} = \nabla V_t + \eta(t)`, where :math:`x` is particle location, :math:`V` is a stiationary potential, :math:`\eta(t)` is a random force
    * From physics, requires stochastic calculus. 
    * Choise :math:`v(x) = - \log \pi(x)`, with :math:`\pi(x)` an equilibrium density, and no accelation term, results in random walk sampling from :math:`\pi(x)`. :math:`\nabla V` is then the score. 
    * A more generalized diffusion approach satisifed the equations, but is no longer a Markov Process. 
    * Forward process is no longer Markovian, but Backward process is. 
    * If its not Markovian, we donot need to add noise each step, we only add noise at the end. 
    * The actual forward steps can be derived from BAtes Rule, but we don't need them, we only need Eq. (9) for training. 
    * When :math:`\sigma_{t=0}` we end up with a deterministic relationship between the noise :math:`\epsilon` and the output. 
        * Backward process is deterministic. 
        * Map straight from noise to output. 
    * We can interpolate between two noise inputs, and get meaniful output, due to deterministic nature of noise-output mapping. 
    * In DDIM the initial state is the only place where stochasicity occurs --> meaningful interpolation. 
    * No requirement for number of steps foward and back to be the same, can choose fewer steps (the usually choose 100) for backward process. 
    * Neural Ordinary Differential Equations (NODE).
