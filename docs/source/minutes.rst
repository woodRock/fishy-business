Minutes
=======

This page contains the minutes for our weekly meetings. 

2022-02-23 - Planning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** C0355, **Time**: Monday 1pm-2pm , **Attendees:** Jesse Wood, Mengjie Zhang

Notes: 
    * Faculty of Graduate Research (FGR) - office on Kelburn Parade. 
    * Forms and information for enrollment is available at the FGR website. 
    * Booked a room for study in MARU101 - Desk 33
    * See Duncan in ECS for an account. 
    * Can work up to 12 hours per week. 
    * Let supervisors and faculty know about trips out of Wellington. 
    * Start as provisional registration, then candidate - write proposal, fully registered - proposal accepted. 
    * Two required meetings, FASLIP (Thursday 2pm-3pm), ECRG (Friday 3pm-5pm).

2022-02-28 - FGR
~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Matthew Vink, Jesse Wood

Notes: 
    * This meeting covers enrollment, we will be confirming details, forms, contacts.
    * PhD Supervisors: Bing Xue, Mengjie Zhang.
    * Documents: 
        1. Confirmation of study - AIML 692 code. 
        2. Fees assessment - two weeks to pay levees. 
    * Information sheet:
        1. Community for needs bank details. 
        2. Tony mcGloughin - School Administrator. 
        3. Confirmation of Proposal Registration Form (CoPR).
        4. Mathew Vink - helped me enroll today. 
        5. Student levees - 2 weeks due.  

2022-02-28 - FASLIP
~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Wednesay 2pm-3pm , **Attendees:** Matthew Vink, Jesse Wood

Neil Dodgeson - Cambridege lecture
    * How to not give a presentation. https://vimeo.com/51597270
    * How to present a paper. https://vimeo.com/7833850

Notes: 
    * Simular to ENGR401 stuff
        * Dont need slides.
        * Trip check technologies. 
        * Face audience. 
        * Relevant stuff only. 
        * No animations. 
    * Research Talks 
        * Don't type the script. 
        * Planning, a lot of time before writing slides. 
        * Audience, can change how you deliver a presentation. 
        * Highlight key points on the last slide. 

2022-03-07 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Bastiaan Kleign, Jesse Wood, et al. 

Papers: 
    1. Conditional Diffuction Probablistic Model for Speech Enhancment. https://arxiv.org/abs/2202.05256
    2. A Study on Speech enhancment on Diffusion Probabilistic Model. https://arxiv.org/abs/2107.11876

Notes:
    * Diffusion models got attention for synthesising images (i.e faces, animals). 
    * Later, it bet the GAN on standard benchmarks. https://proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html
    * Train it to add noise, the reverse the process. 
    * Diffusion: learning to denoise speech signal. 
    * Isotropic gaussian distribution? https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic
    * Learn the signal-to-noise difference, not the mean signal. 
    * Diffusion markov chain is intractable, so we use Elbo to from an approximate objective function. 
    * Ratios and constants to ensure the mean and variance don't explode or vanish. 
    * New Mailing list for DL updates. 
    * Next week: Bayesian Transformers. 

2022-03-08 - Induction
~~~~~~~~~~~~~~~~~~~~~~
**Location:** AM101, **Time**: Monday 2pm-4pm , **Attendees:** Neil Dodgeson, Jesse Wood, et al. 

Notes:
    * Bastiaan slide example for meetings. 
    * Neil Dodgeson - Faculty of Graduate Research Dean. 
    * Faculty of Graduate Research (FGR). 
    * Workshops, writing events, professional development. 
    * Website https://www.wgtn.ac.nz/fgr
    * Workshops are practical and hands-on. 
    * Thesis bootcamp - 20 writing hours. 
        * Aimed at final year students. 
        * June november 
    * Research room 
        * Review, tips, stories, events, resources. 
        * Updates monthly. 
    * Candidate progress form (CPF)
        * Report on 6 monthly progress in a report. 
        * May / November. 
        * Required, not academic, supporting evidence. 
    * 4 weeks annual leave, no formal process. 
    * Suspensions, for illness, bereavement, work. 
    * Forms for aforementioned available online. 
    * Proposal: first major milestone. 
        * 12 month deadline. 
        * no extensions available. 
    * Automatic re-registration for first 2 years. 
    * Constructive relationship with supervisor. 
    * PhD certificate: competent to do invidual research. 
    * Work expert in our PhD Research topic. 
    * Regular meetings times. 
    * Student/supervisor - same page for expectation.
    * Bring agenda to meeting.
    * Project management techniques - scrum, agile. 
    * 2pi rule for time estimation. 
    * Secondary supervisor - (usually) hands off role. 
    * "The only way through it, is to do it." 
    * Books, publications, thesis - different expectations for each course. 

2022-03-10 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-4pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood 

Notes:
    * Let Bing/Meng know about any financial difficulties. 
    * Topic ideas: 
        1. Multi-objective 
        2. Evolutionary computation
        3. Domain expertise. 
    * First two-weeks - extensive background reading. 
    * ECRG - meeting tomorrow from 3pm - 5pm. 
    * CoPR - fill out by the end of March. 
    * Individual induction - copy Bach in email for meeting. 
    * Add Bach to gitlab/github for the paper latex file. 

2022-03-10 - FASLIP
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-4pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Jeff Hawkins - Thousand Brains Theory: https://www.youtube.com/watch?v=O4geanMOsyM

Notes:
    * Voting, similar to droupout, bagged ensemble. 
    * Many models (sub-networks) for the same thing. 
    * Sparse networks, efficient -> noise tolerant. 
    * Only update in one area, without need for back-propagation, doesn't require a full training for each new instance. 
    * Builds a full world model, not a model for each task. 
    * Thousand brain theory - solution to No Free Lunch. 

2022-03-11 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Hui Ma gave presentation on Evolutionary Computation Approaches to Web Service Composition - https://link.springer.com/article/10.1007/s10732-017-9330-4

Notes:
    * Meng will discuss how to write a proposal. 
    * EuroGP conference - ask my supervisor to register. 
    * Introduced myself to the group 
        * paper - finish writing my Summer Research paper. 
        * enrolled - lots of paper work. 
        * Finish writing the paper properly. 
    * Abdullah (lab neighbour) first week in group.
    * Evolutionary Computation Approaches to Web Service Composition. 
    * Over 40 publications in the area. 
    * Holidy booking service used as an example. 
    * Organize services into re-usable modules. 
    * Service composition is a NP-hard problem. 
    * A global search is not possible, a heuristic based local search is required. 
    * Evolutionary principles and techniques - crossover, mutation.
    * Automatcally create hybrid services through composition. 
    * Don't reinvent the wheel, use existing libraries instead. 
    * Scheduling, routing, resource allocation, service composition - all possible for EC. 
    
2022-03-17 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood. 

Notes:
    * Daniel can provide domain expertise for writing the chemistry sections for the paper. 
    * Multi-objective - classify chemical compounds and their percentage.  
    * Multi-label - one instance can belong to multiple classes. 
    * copy Bing and Bach for induction email from Georgia. 
    * pymoo  - multi-objective python library. 
    * Read/write summaries for papers as I go - write content for second chapter iteratively. 
    * Send Daniel conclusions / contributions of paper in email, then organize a follow up meeting.

2022-03-17 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Ying, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Ying suggested a talk on Multi-objective Evolutionary Federated Learning https://vimeo.com/552900291

Notes: 
    * Yaochi Jin - University of Surrey. 
    * Multi-objective machine learning. 
    * Centralized and federated learning. 
    * Evolutionary multi-objecive federated learning. 
    * Evolutionary federated nerual architecture search. 
    * Multi-objective - gives a solution set, as their are tradeoffs between objectives. 
    * Dominance, no X is worse and Y, and X is strictly better than Y for object A. 
    * pareto front (See tegmark2020ai) set of optimal solutions.
        * accuracy, diversity. 
        * Inverse generational distance (IGD).
        * Hypervolume - nadir 
    * Optimize for minimal complexity implies interpretability. 
    * Centralized learning - one database. 
    * Localized learning - everyone trains their own model. 
    * Privacy techniques: 
        * Secure multi-party computation. 
        * Differential privacy. 
        * Homomorphic encyption. 
    * Federated learning 
        * train a high-quality centralized model with training dataq distributed over a large number of clients. 
        * Each with unreliable and relatively slow network connections. 
        * horizontal - all attributes, batches of data. 
        * vertical - trained on subset of attributes (i.e. security reasons). 
    * Federated learning objectives 
        1. Maximise learning performance. 
        2. Minimize communication cost. 
    * Their work efficiently reduce the number of connections while maintaining similar performance. 
    * Neural architecture search (TODO - watch the rest and take notes!!!)

2022-03-18 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

A talk on Geometric Semantic Genetic Programming by Qi Chen https://link.springer.com/chapter/10.1007/978-3-642-32937-1_3

Notes: 
    * We have published heaps of papers that are highly cites and hot papers according to https://www.webofscience.com/wos/woscc/basic-search toool that the university has access to.
    * Top 1% of papers cited per discipline for computer science journals. 
    * Evolving neural networks with evolutionary computation. 
    * Me: reading psychology papers on how the brain works with memory - hunting for relative simply neuro-science ideas to apply to machine learning. 
    * Geometric smenatic genetic programming (Morgalio 2012, moraglio2012geometric) https://link.springer.com/chapter/10.1007/978-3-642-32937-1_3 
    * Semantic genetic programming methods. 
    * Traditional GP ignores program semantics. 
    * Consequences - ragged gentype-phenotype mapping. 
    * Is it possible to make GP aware about the effects of the program execution? 
    * Semantics: 
        * Semantics differs from syntax. 
        * Semantics related to the problem domain. 
        * Semantics inform program design (Tegmark 2020, tegmark2020ai).
    * Measure semantic distance between current program and target output (multi-dimensional loss function). 
    * Genetic operators: 
        * Semantic aware cross-over (SAC) 
        * Semantic similarity-based cross-over (SSC)
        * Semantic similarity-based mutation (SSM)
        * Senantic tournament selection. 
            * t-test for statistical signfician with assessing selection. 
    * Search directly in the semantci space of the program. 
    * Semnatics of offsrping must sit in between the ntercept between its two parents in semantic space. 
    * Therefore each offspring minimized distance to target semantics. 
    * Each generation gets closer to the target semantics, or atleast closer than the furthest parent. 
    * Independent of data, good effect on improving generalization, althougt not actual claim made in paper. 
    * Geometric semantic programming leads to a unimodel fitness landscape - a cone where the apex is the target semantics. 
        * manhattan distance - square based pyramid.
        * euclidean distance - cone. 
    * Efficient implementation - only store changes to program tree, similar to git version control - except for GSGP. 
    * GSGR Red (reduce), simplify problems by expanding and recomputing. 
    * Locally geometric semantic crossover (LGSX).
        * Make offsrping similar to eachother than their parents. 
    * Random desired operator (RDO), exploit interoperability of instructions, + can be reversed with -, * can be reversed with division, + and * are communicative. 
    * Semantic backpropogation - decomposibility of the process if important for BP. 
    * Angle aware metrics - larger angle metrics iis more likely to generate offspring closer to target semantics. 
    * Permutations GSX and Random Segment Mutation 
    * Semenatic distance (euclidean) is the same as the loss, just looking at it from a different point of view. 
    * Can geometric smeantic programming work in an unsupervised or combinatorial problem? (Possibly not unimodel semantic space)
    
2022-03-21 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Hayden Dyne, Bastiaan Kleign, Jesse Wood, et al. 

Talk by Hayden on two papers: 
    * End-to-end driving via conditional imitation learning (Cai 2020, cai2020high) https://ieeexplore.ieee.org/abstract/document/8460487/
    * High-speed autonomous drifting with deep reinforcement learning (Codevilla 2018, codevilla2018end) https://ieeexplore.ieee.org/abstract/document/8961997/ 

Notes: 
    * Model-free reinforcement learning - does not rely on human understanding of world and design controllers. 
    * Human driver is the trajectory with is the goal, uses a professional driver playing the game with a steering wheel. 
    * Model performs on different track difficulties. 
    * Reward function is scaled by velocity, so faster lap times are rewarded. 
    * Works for 4 different kinds of vehicles, although the truck struggles to achieve same performance as lighter ones. 
    * Second paper - e2e 
    * Far easier to use real-world data on driving that has already been collected than generate simulation data. 
    * Data augmentation used to help network generalize to new scenarios and edge cases not in the training data. 

2022-03-24 - Faculty Induction
------------------------------
**Location:** Zoom, **Time**: Monday 10am-11am , **Attendees:** Georgia Dix, Jesse Wood, Bach Hoai Nguyen.

Induction to my PhD studies with supervisor and faculty. 

Notes: 
    * Expectations
        * Supervisor
            * Uni life 
            * Framework 
            * Networking 
            * Assessment 
        * Me
            * action plan 
            * identify problems 
            * administration 
            * CDP (6 monthly report)
    * Marking can thesis can take up to 6 months - can work during this time. 

2022-03-17 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood. 

Notes:
    * Daniel can draft the chemistry parts for the paper. 
    * Draft the full paper with Bach, then send to Daniel. 
    * Read "From evolutionary computation to the evolution of things" - Nature
    * Can start coding now - explore ideas for ENGR489 and EC on existing data. 
    * Transformers, LSTM, GAN - yet to be applied to GC-MS data in literature. 
    * CNNs for GC, likely due to libraries, hype, understanding, Diffusion of innovation. 
    * Scuba diver experiment for context-dependent memory is a good analogy for noise in ML models.
    * Came up with evolutionary ideas, like sexual selection, but (Miller 1994) did it quite some time ago.
    * Idea for EC, a dynamic environment where complexity increases, classes or features are added - similar to evolution IRL. 

2022-03-24 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Ying, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Ruwing Jiao suggested a video on Bayesian Optimization from Mark Deisenroth https://www.youtube.com/watch?v=_SC5_2vkgbA 

Notes: 
    * Recommended background reading on this topic: 
        1. A Tutorial on Bayesian Optimization of Expensive Cost Functions (Brochu 2010, brochu2010tutorial) https://arxiv.org/pdf/1012.2599.pdf
        2. Taking the Human Out of theLoop: A Review of Bayesian Optimization (Shahriari 2015, shahriari2015taking) https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306
    * Latent Structural Support Vector Machine (Miller 2012) - **TODO** find this paper/project. 
    * Deep learning often involves a lot of hyper-parameter tuning, this is usally done by the practitioner model. 
    * Alternative approaches: 
        * Manual tuning 
        * Grid search 
        * Random search 
        * Black magic (i.e. lr is 1e-3 is "good")
    * Computationally expensive to search for global maximum in hyper-parameter search space. 
    * Globally optimize a black-box approach to evaluate (e.g. cross validatio error for a massive neural network). 
    * Use a probabilistic model to approximate the black-box model for the hyper-parameter search. 
        * create a proxy model - this learns an approximation of the space - with less computational cost to query that space. 
        * referred to ass proxy / approximate / surrogate. 
    * The standard model for optimizing a bayesian model is a gaussian process. 
    * Evaluate the proxy function once, this saves computation. 
    * A gaussian process minimized the uncertainty of the proxy function.
    * It samples the feature space at the minimum value of the shaded area (== uncertainty).
    * It repeats this often, until the proxy function is close enough to the true objective. 
    * Exploration - sample areas with high uncertainty. 
    * Exploitation - sample places with low mean. 

2022-03-25 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Andrew Lenson, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Andrew gave a talk on Genetic Programming, Explainability and Interdisciplinary AI.

Notes: 
    * Heaps of students successfully submitted papers to the Gecko conference. 
    * Possible to publish in conference at different levels; paper, poster, etc. 
    * If a paper is declined, revise with reviewer comments, and resubmit as poster. 
    * Qurrat Al Ain - Cancer research in AI. 
    * Swiss roll manifold problem
        * Reduce a manifold to a 2D visual representation. 
        * 2D path is representation of non-linear dimensionality reduction (NLDR).
    * Geo-desic, shortest path from A to B, not shortest euclidean distance. 
    * Lower dimensional space is referred to as an embedding. 
    * We can use AI to learn or approximate this embedding (if the problem is intractable).
    * Ways to estimate the intrinsic dimensionality of the dataset - statistical techniques. 
    * Kaka - count distinct nnumber of birds at Zealandia. 
    * GoPro for data collection combined with crack for Kaka. 
    * Law - predicting sentencing lengths with PLSR on judge summaries. 
    * Names with high/low probabilities are often historic cases referred to as 'guidance judgements'. 
    * Combine data analysis and domain expertise to infer knowledge about sentencing lengths. 
    * Home detention or communtiy service are associated with shorter sentences. 
    * Future work, take humans out of the loop, and make sentencing deterministic.
    * ^ This can be done, because their are extenuating circumstances that require a judges opinion.
    * Also, if all sentences are automated, there would no longer be guidance judgements being set historically. 
    * Law is a dynamic and decentralized system, unique and specialized for each country, case, individual, etc... 
    * Research more productive on letting judges analysis their blindspots, and identify bias.

2022-03-28 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Daniel Braithwaite, Bastiaan Kleign, Jesse Wood, et al. 

Daniel Braithwaite talked about two papers related to machine learning for audio wave construction:
    1. Deep Audio Priors Emerge from Harmonic Convolutional Networks https://openreview.net/pdf?id=rygjHxrYDB
    2. Harmonic WaveGAN https://www.isca-speech.org/archive/pdfs/interspeech_2021/mizuta21_interspeech.pdf

Notes: 
    * The idea is to look at harmonic convolutions, think convolution layer but designed for audio. 
    * WaveGAN and Harmonic WaveGAN use deep learning on audio signals. 
    * Harmonic, is better suited towards audio signals, than wave alone. 
    * Harmonic considers local connections / adjaceny better. 
    * **TODO** Read these papers and add to notes. 

2022-03-31 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Jesse Wood. 

Bing and Meng were both unwell this week. Important to send minutes for this meeting to them.

Notes: 
    * Augmentation - boost performance on the fish part dataset. 
        * Time-shift, shift data backwards and forward, to get time-invariant generalized model (may not work well).
        * Impute data, combine existing samples, add noise, etc... 
    * Worked on CNN from ENGR489 for classification task.
        * Issues with keras and sklearn libraries, stratified cross-fold validation and one hot encoding don't play nice together. 
        * CNNs, we use 1D convolution and pooling layers on time-series data. 
        * Existing ML + GC literature also use CNN for classification and regression tasks. 
        * These models are powerful for extracting features in spaces with local connectivity. 
        * Aim to use EC to perform neural architecture search for CNN hyperparamters - these differ for each dataset. 
    * Both EC and Bayesian Optimization approaches work for neural architecture search. 
        * However, EC has more interpretable results, e.g. a genetic algorithm produces an explainable tree. 
        * Neural networks are black-box and esoteric, we understand how (i.e. back-prop, SGD), but not why? 
        * EC produces simpler representations, that can be prodded with domain expertise.
    * Important to read heaps for first few months of PHD. 
        * Take original notes that can contribute toward a backgroup chapter of my proposal. 
        * Get an idea of what has been done, and what I want to do. 
        * Still reading psychology textbook on memory and the brain to establish conceptual framework for learning.
     
2022-03-31 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Fangfang Zhang, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Fangfang suggested a video called the Big Reset 2.0 https://www.youtube.com/watch?v=-ePZ7OdY-Dw

Notes: 
    * Reinforcment learning for Robotic Arms. 
    * Deep blue beat Kasparov, but no AI can set up the chess board, a 6 year old can do that. 
    * Hugh Herr designed his own AI legs https://www.youtube.com/watch?v=CDsNZJTWw0w
        * AI prosthesis is cost prohibitive for the masses, but may work with diffusion of innovation in future. 
        * Prosthesis can up upgraded over time, biological body parts cannot, hardware/software updates for legs. 
    * Fake news - Jon Stewart said MSM has more trackers than ANY other media (adult entertainment, torrent sites, social media included).
    * Chomskey, MSMs job is to sell the educated privelaged wealthy elites as an audience to the corporations advertising. 
    * AI algorithms - social media, fake news, incentives. 
    * AI autonomous warfare proliferation - we need to ban slaughter bots https://www.youtube.com/watch?v=pOv_9DNoDRY
    * AI used for traffic management, screen-time punishment - pick up phone at cafe and pay the bill. 
    * RoboMaster - robot warfare, mechatronics, AI - physical robot warfare as a game/competition. 
    * Cosmo - Boris Sofman https://www.youtube.com/watch?v=U_AREIyd0Fc 
    * Narrow-AI and no free lunch problem - AI is good at solving very specific tasks, but not general intelligence. 
    * I have an industry project, that has real-world applications in a factory settings - i.e. reduce bycatch and maximize efficiency of food processing for fish. 

2022-04-01 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Andrew Lenson, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Meng and Bing were unwell, so Yi chaired the research group meeting. 

Notes: 
    * Bach (my supervisors) first day lecturing for COMP102. 
    * Me: I got 98% accuracy on the fish species dataset using a 1D CNN.
    * Shorter meeting, workshop cancelled, due to Meng being unwell.  

2022-04-04 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Ciaran King, Bastiaan Kleign, Jesse Wood, et al. 

Ciaran King was gave a talk on "Experiences using Github Copilot".
    * Understands the context of code, can make abstractions for helper methods. 
    * Can write documentation for codebases.
    * Not software "correct" code, but (likely) the code we were going to write. 
    * Can write tests for codebases with very little leading. 
Daniel Braithwaite on "Fixed Neural Network for Stenography"
    * Hide messages in adversarial neural network. 
    * Pre-trained stenograph, results in non-zero error, we need perfect reconstruction for encryption.
    * Face anonymization, post a persons face online, then regenerate the face, but encrypt the private face. 
    * This lets friends anonmyously share images with their face online, without revealing their identity.
Bastian - contractivity of neural networks.  
    * Signal processing worries about getting non-stable linear filters for signals. 
Jesse Wood 
    * Evaluating Large Language Models Trained on Code https://arxiv.org/abs/2107.03374
        * 70% accuracy for basic DSA problems. 
        * Can't solve more difficult problems - doesn't optimize solutions for performance. 
        * CoPilot outperforms other state-of-the-art NLP code generation models. 
        * Requires "fine-tuning", supervised human intervention to hint towards correct answer. 
    * Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions https://arxiv.org/abs/2108.09293
        * 40% of code written with CoPilot has cybersecurity vulnerabilities.
        * CodeQL and other static analysis tools used to define the security of the code.
        * Security is a shifting landscape, WannaCry, Log4J - zero days kept secret by intelligence agencies. 
        * This is true of all code, the training data was written by humans. 
        * Potential vulnerability for future attacks if hackers know open-source repos are training data.  
        * Don't treat copilot as a "glass cannon", it doesn't deserve a false sense of security.

2022-04-07 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bing Xue, Jesse Wood.

Notes:
    * Use an existing neural network architecture search algorithm - application analysis. 
    * Callaghan may have extra data work with - arrange a meeting with Daniel. 
    * Pre-traning, tranfer learning, NIST dataset for GC refraction index. 
    * Look at existing proposals, get an idea for mine - possible to submit proposal early. 
    * State-of-the-art, is 50-50 whether it works or is a bust - good to have a backup based in existing literature. 
    * Pareto front with tradeoff between complexity and accuracy. 
    * Proposal does not lock me into using a particular method (i.e. SVM, EC, PSO). 
    * Idea: make sure students have a decent grasp of the field before conducting their own research, if not then read more.
    * Later try out ideas in the proposal, and see if they work. If they don't change tact.

2022-04-07 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Qi Chen, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Qi Chen showed a talk "Artificial Intelligence: A Guide for Thinking Humans" by Melanie Mitchell from https://www.youtube.com/watch?v=NMUqvhuDZtQ

Notes:
    * Shannon, Simon, Minsky - all though AGI was 15 years off in their own time. 
    * Andrew Ng - AI is the new electricity. 
    * Elon Musk - nobody would listen - https://www.youtube.com/watch?v=4RMKLyaqh_8 
    * Deep learning brought back the hype for AGI. 
    * "An Anarchy of Methods" - Joel Lehman 2014. 
    * AI, Machine Learning, Deep Learning, onotologies of fields and their popularity change over time. 
    * Deep learning looks at AI as an aritficial brain - enter the Artificial Neural Network (ANN) - the connectionists.
    * CNN based on the limited understanding of the human brain in 1950s neuroscience. 
    * Facebook used CNN AI for facial recognition when a user uploads a photo. 
    * ImageNet is a famous supvervised classification task that was generated through crowdfunding internet "slave" labour. 
    * Famous 94% result for image classification has a sample size of PhD student (Andrew Kaparthy) - the fake news embellished the story. 
    * Self-driving, stopped fire truck on the highway, the long tail of AI, edge cases. 
    * Adversarial attacks on neural networks, crack networks to make wrong predictions based off of their flaws. 
        * "Intruiging properties of Neural Networks" https://arxiv.org/abs/1312.6199
    * Trick self-driving cars into driving through stop signs with stickers, that make it think it is a speed limit sign. 
    * "I wonder whether or not AI will every crash the barrier of meaning." - Glen Carlo Rote 1988. 
    * "common-sense" machine learning, WinnaGap NLP problem. 
    * DARPA - competition to design a machine with the common-sense of an 18 month old baby. 

2022-04-08 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Harith Al-Sahaf, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

General: 
    * University drops their vaccine mandate https://www.wgtn.ac.nz/covid-19/settings-and-mandates/removal-of-vaccine-mandate 
    * EvoStar conference is coming up soon - April 20-22nd, Spain Madrid http://www.evostar.org/2022/eurogp/
    * Me: 
        * CNN performs relatively well on fish part dataset with manually tuned hyper-parameters. 
        * Also, mentioned MegaSYN annecdote about adversarial attacks on machine learning models for protiens to manufacture deadly nerve toxins.

Harith Al-Sahaf gave a talk on Malware Analysis https://www.al-sahaf.com/harith/ 

Notes: 
    * Malware analysis determines the functionality, origin and potential impact of a given malware. 
    * Applying EC techniques to malware anaylsis. 
    * Harith and the university have a lot of publications in this area https://www.al-sahaf.com/harith/publications.html
    * Siemese neural networks used to identify an unknown instance to a known malware for similarity. 
    * Yann LeCunn invented the idea for Siemese neural networks in the 1980s. 

2022-04-11 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-5pm , **Attendees:** Madhurjya Dev Choudhury, Bastiaan Kleign, Jesse Wood et al.

Madhurjya gave a talk on "Time Series analysis for Machine Health and Diagnosis". 

Notes:
    * "Image-<MUFFLED> tranlation with Conditional Adversarial Networks". 
    * pix2pix 
    * Nuisance parameters is any parameter which is not of immediate interest, but must be accounted for in those parameters which are of interest. 

2022-04-18 - EvoStar #1
~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Madrid, **Time**: Monday 7pm - 6 am, **Attendees:** Jesse Wood, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al.

More information can be found on the full conference program - https://easychair.org/smart-program/Evo2022/

Gabriella gave a keynote address on "Illuminating Computational Search Spaces" https://www.youtube.com/watch?v=EyynDbXnwic

Notes: 
    * Visualisation, explanation, informed configuration. 
    * Complex networks, Local Optima Networks (LON), Search Trajectory Networks (STNs). 
    * Graphs exist since 1800s with Eulers famous bridge problem. 
    * Networks coming from different systems share topological structure. 
    * Force-directed graph layout algorithm, borrows ideas from physics (i.e. simulated annealing). 
    * Difference between Euclidian distance and traversal path distance between nodes (see Andrew's manifold talk from 2022-03-25 - ECRG)
    * Fitness landscpaes f(S,N,F) , search space (S), n<unreadable> (N), fitness (F).
    * Funel - local optima in a course grain structure that can minimize energy. 
    * Local optima Network, nodes - local optima from hill climbing heuristic, edges - transition between local optima. 
    * Number partitioner, given a random set of numbers find a partition at value k, such that the two disjoint sets have equal sums. 
    * Map fitness landscape as a graph, compress the representation, to get an explainable visualisation for a funnel. 
    * Travelling Salesman Problem (TSP) - apply pertubations to existing solutions to find new solutions in the fitness landscape. 
    * CMLON, compress a Local Optima Network into a smaller representation that is easier to understand. 
    * Genetic improvement to CMLON. 
    * STN, allow for representative solujtions. 

Christian Raymond gave a talk on "Multi-objective Genetic Programming for Symbolic Regression with the Adaptive Splines Representation" 

Notes: 
    * Overfitting if a difficult problem for GP, because of flexibility of representation. 
    * Difficult to regularize overfitting in GP. 
    * Parisomony pressure, population distribution control, order of non-linearity. 
    * Limitations: structural complexity has minimal correlation with generalization. 
    * Estimate complexity of a model by estimating a model's behaviour over a subset of training space. 
    * Limitations: difficult to reliably estimate, complexity vs error is a trade-off. 
    * Semi structured representation - a solution to both limitations above. 
    * Spline: defined by multipled low degree k (cubic) polynomials smoothly. 
    * GG-AWS-PP - Adaptive wieght Splines with parsimony Pressure. 
    * Apply multi-objective optimisation, for loss and complexity objectives, both considered. 
    * Training paretor fronts, 2D rperesntation of fintess landscape between two objectives. 
    * Creates interpretable representations which are easier to understand than genetic algorithms. 
    * Works well on low to medium dimension feature sets. 

Another talk on "Morphologicl development of Robots [...]" 

Notes: 
    * Development of agents, aging, is beneficial for generating complex agents. 
    * Voxel based soft robots (VBS). 
    * Aggregates of soft cubes (voxels). 
    * Neural networks for voxel controllers, a net for each voxel. 
    * New voxels are added to the model over time. 
    * This is a scheduling function, that must alloclate the correct time to increase the complexity of the model. 
    * <unlegible> based morphology representation. 
    * Different development schedules. Early development, uniform development, no development. 
    * Early development seems more beneficial than uniform development, the artificial mimics the biological. 
    * No development, shows large deviation in the results, fuzzy accuracy. 
  
"A new evoltionary algorithm based home monitoring device for Parkinsons Dyskinesei"

Notes: 
    * AUC based fitness. 
    * Adaptive size fitness <unlegible> - allocate fitness absed on a subset of training data. 
    * Aaply different representation width lengths using GP to see [...] (coffee break for me)

Zhixing gave a best paper nominee talk "An investigation of Multi-task Linear Genetic Programming for Dynamic Job Shop Scheduling"

Notes: 
    * Job shop scheduling problem. 
    * Complicated dynamic NP-hard combinatorial problem. 
    * Make decisions based on imperfect information. Instead, we use heuristics to decide schedule. 
    * We can't rely on a single heuristic alone, dynamic environment means we must change heuristics in real-time. 
    * Hyper heuristic - a search mechanism to find a heuristic selecting model. 
    * GP has good interpretability, a tree can easily be understood by humans. 
    * Seasonal variance - more demand for ice-cream in summer for an ice cream factory. 
    * Multi-task, conflicting or tradeoff between goals of different stakeholders. 
    * Multi-task model - one ring to rule them all - one model that can balance multiple tasks at once. 
    * Linear gentic programiing, register based instructions, creates directed acyclic graph (DAG). 
    * DAG > Tree; can use diffrent topological structures to perform cross over for DAG, tree reperesentation is limiting. 
    * Operators: linear crossover, macro mutation, micro mutation. 
    * Multi-population based genetic programming (GP) - sub populations that develop in isolation with crossover (migration) allowed. 
    * homogenous/heterogenous - diff-same / same-diff - utilization/objective functions. 
    * M^2GP does not perform the same as LGp methods, M^2GP (tree based) state-of-the-art does better. 
    * Likely because: (1) too large variation step sive, (2) ineffective initializaiton strategy. 
    * One and multi-objective population methods have similar performance. 
    * Graph based crossover is a <unlegible> genetic oeprator 
    * But not used in this work, but is a research direction in ECRG. 

David Wittenberg gave a talk on "Using a denoising autoencover genetic programming to Control Exploration and Explotitation in Search"

Notes: 
    * Capture relevant properties of parent population in a latent representation. 
    * Model (auto-encoder) is trained to reconstruct the input. 
    * Problem: don't want to learn the identity function. 
    * Solution: denoise (slightly mutate) parent to avoid overfitting anf force the auto-encoder to generalize. 
    * Level of corruption can be used to control the exploration and exploitation of genetic algorithm. 
    * Paper explores this idea, we want a latent representation that is a lower resolution to let the model generalize better. 
    * Subtree mutation: can't control corruption. 
    * They propose Levenshtein edit, a genetic operator for mutation on the representation string. 
    * Convert tree into infix string reperesentation - then perform mutation operations on that string. 
    * Levenshtein edit, insert, delete, mutate - with an edit percentage (this determines level of corruption). 
    * The  stronger the corruption, the shtonger the exploration. 

Nicholas Fontbonne gave talk "Coperatative Co-Evolution and Adaptve Tree Composition for a Multi-Rover Resources Allocation Problem"

Notes: 
    * Multi-agent - agents act indepdentdenlt. 
    * Competetive, mixed, co-operative sum games from game theory. 
    * Zero-sum, shared-sum, shared-fitness agents. 
    * In cooperative case, all information about the individual is lost (collectivist idealogies "the greater good"). 
    * A shared fitness that promotes social welfare is not a good learning signal. 
    * Marginal contribution - contribution of an individual agent to the team. 
    * Evolutionary algorithm for multi-agent problems. 
    * issues: stuck in local optima, high computation cost. 
    * solution: grouping mechanism. 
    * reousrce seelction problem: each resource has a satisfcation score. 
    * k is the group size for the grouping mechanism. 
    * Co-operative co-veolutionary algorithm for adhoc autonmous agents. 
    * Efficient anytime larning without apriori knowledge of the problem. 

2022-04-21 - Weekly
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Wednesay 12pm-1pm , **Attendees:** Bing Xue, Bach Hoai Nguyen, Mengjie Zhang, Jesse Wood.

Notes:
    * EvoStar conference yesterday, not much work for covid isolation week. 
    * Demelza is giving a talk tongight for EvoStar. 
    * Likely to attend EuroGP in-person next year. 
    * EvoCNN - encodes basic CNN components. 
    * Apply GP to classifier problem directly. 
    * Try several techniques (initially). 
    * Also later what new tasks Daniel may want.  

2022-04-21 - EvoStar #2 
~~~~~~~~~~~~~~~~~~~~~~
**Location:** Madrid, **Time**: Tuesday 7pm - 6 am, **Attendees:** Jesse Wood, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al.

More information can be found on the full conference program - https://easychair.org/smart-program/Evo2022/

Demelza gave a talk on "Genetic Algorithm for Automated Spectral Pre-processing in Nutrient Assessment"

Notes:
    * Rapid collection, non-destructive, construct a model. 
    * Partial Least Squares Regression (PLSR). 
    * Spectral data is easy to collect, but the pre-procsing is a bottleneck. 
    * PLSR tranlsates the feature space into latent varaibles. 
    * Spectral pre-processing, remove noise and redundant intensity values. 
    * Manual process for pre[rpcessing spectra is laborious and lacks standardization. 
    * Automate pre-processing for choosing appropriate techniques and their order of application. 
    * Representation, a two chromosome structure that encodes objectives for: 
    * Fitness function: combination of MSE and number of features. 
    * GA-PLSR-PPS perform btter for most cases and has a smaller standard devition for IR and Raman. 

Zakaria Dehi gave a talk on "A machine learning based approach for Economics-Tailored Applications: The Spanish Case Study". 

Notes: 
    * Use AI for dynamic budget allocations in governmental bodies. 
    * (1) ETL, (2) Profiling, (3) Predicting. 
    * ETL - gathered 30,000 economic features, and identified 50 types of related economic metrics. 
    * Pre-processing required to make the building blocks for the ETL model. 
    * Different metrics can have the same profiles for all cities (i.e. unemployment/retirement).
    * Eliminate the redudnat metrics (duplicate class profiles) - for concise metric space. 
    * DCGA-kMeans - unsupervised clustering algorithm to determine profiles (k). 
    * Profiling deals with indentifying groups og cities based on an economic profile dervied from metrics. 
    * Prediction: Long Short-Term Memory (LSTM) - recurrent neural network. 
    * This unlocks the time series component of ecnomic data, persistance of time observed by the model. 
    * The ideal number of profiles for most metrics was 3. 
    * Massive feature and metric reduction - creates meaninful data through feature construction. 
    * Can decrease complexity of number of cases when officials must make complex politicy decisions based on near-infinite combinatorial search spaces. 
  
Daniel Lopez gave a talk on "EvoDesigner: Towards Aiding Graphic Design"

Notes: 
    * Readability, balance, innovation, style - all measured by Mean Squared Error (MSE). 
    * Adobe plugin to produce variations of a design based on rough input using evolutionary computation. 
    * Intitial input is a rought sketch, that paints an idea of a possible layout. 
    * Evolutionary engine creates a good design graphic based on that initial input. 
    * Then the engine generates variations of that good design using evolutionary computation. 
    * Still needs work, but offered a good tool for aiding ideation in graphic design. 

Julia Reuter gave a talk on "Genetic Programming-Based Inverse-Kinematics for Robotic Manipulators... <unlegible>".

Notes: 
    * Develop prototypical solution for Inverse Kinematics (IK) problems. 
    * We want closed-form solutions, apply to non-standard SCARA, with explainable representations (equations). 
    * Kuke koubot - was the brand of SCARA they ran their simulations on. 
    * Different objective functions + co-creation / co-evolutionary approaches.
    * IK-CCGP was compared to an ANN. It did better than the ANN. 
    * Co-evoltuion for the two-joints, performed better than for 3 (a more complex task). 
    * The equations were tested on simlated - not real world - robotic arms. 
    * But these equations can be verified later, by applying them to real-wrold systems, and testing for collisions/singularities. 
  
Partick Indri gave a talk on "One-shot learning of Ensembles of Temporal Cage Fomrulaes for Anomaly Detection in Cyber-Physical Systems".

Notes: 
    * Monitor behaviour of CPS, e.g. AV, power plant, medical monitoring, security systems, smart house. 
    * CPS are dynamic - need to quantify the system in terms of time. 
    * Signal Time Logic (STL), is an expression grammer for time based operations. 
    * GP oeprators to construct STL for optimized performiang at controlling a CPS. 
    * One-shot algorithm, GP is population based, we can use the population to build ensembles.
    * Water treatment CPS was used as dataset for training/test. 
    * Their method performs well when comapred to other state-of-the-art methods. 
    * One-shot G3P achieves more complext formulates than the standard G3P. 
    * One-shot approach cpatures wave temporal operators better than standard G3P.
    * It can learn more complext models, that include "time-based" operators (i.e. STL). 
  
Gloria Pietropoli gave a talk on "Combining Geometric Semantic GP with Gradient-descent Optimisation"

Notes: 
    * Geometric smentaic metric programiing (GSCP) is a well known variant of genetic programming (GP), 
    * GSCP used recombination and mutation operators that have a clear semantic effect. 
    * Combine GSCP with Adam Optimizer. 
    * GP, we can presresent an individual as a point a real n-dimensioanl semantic space. 
    * Geometric smenatic operators: 
        * Geometric semantic crossover (GSC). 
        * Geometric semantic mutation (GSM). 
    * Adapate Moment estimation (Adam, kingma2014adam), is a first order gradient-based optimization of stochastic functions. 
    * GSCP makes a good jump in the solution space, then adam can refine a candidate solution. 
    * Two approaches: (1) one-step GSCP then one-step ADAM (HYB-GSCP), or, (2) full GSCP then full ADAM (HCH-GSCP).
    * HYB-HSCP does better than HCH-GSCP. 

Dominik Sobaria gave a talk on "Program Synthesis with Genetic Programming: The Influence of Batch Size"

Notes: 
    * Program synthesis with genetic programming. 
    * Anayluse perfromance and generalization ability of programs generated by GP. 
    * Batch size effects programming synthesis. 
  
2022-04-22 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Notes:
    * Small editing (layout) is allowed when responding to referee feedback on a paper. 
    * Me: "I worked on catching COVID his week"
    * New datasets from Daniel, he proposed some new research objectives https://mail.google.com/mail/u/0/?hl=en/#inbox/FMfcgzGpFWTMDnsjKxswvdtWdrhKzKdH 
  
2022-04-22 - Proposal Workshop 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Mengjie Zhang, Jesse Wood et al.

We had a proposal workshop hosted by my former supervisor Mengjie Zhang. 

Notes: 
    * First (of many) proposal workshops about preparing this document. 
    * Target adueince for workshops are first year PhD students (me). 
    * PhD is a great way to go for a very good job later in life. 
    * Don't have to do theoretical work, but focus on application, key point is to make a major contribution to the field. 
    * First year of PhD, we are a student not a candidate. A candidate has passed their proposal. 
    * Bad PhDs can be converted into a masters degree - this happens if the proposal is very bad (try to avoid). 
    * Read 50-100 papers in order to get a good grounding for a PhD proposal. 
    * Some people do not have the capability to do a PhD, they can propose, but must demostrate their capability with preliminary work. 
    * Structure: 
       1. Introduction 
       2. Literature survey (read) + 
       3. preliminary work (code) + 
       4. contributiosn (major) / miletones / thesis outline / resources. 
    * Overall goal: a single over-arching scientific/engineering argument to unify my PhD stydies as one body of work. 
    * 100,000 words (is a lot of words) is the expectation for a PhD thesis. 
    * Minimize dependencies - encourage modularity for research objectives. 
    * Coherence is very important for choosing research objectives. 
    * Literature review should cover most recent work and domain specific (biology/chemistry) papers. 
    * Honours work can't be counted as preliminary work (but can be references as a citation). 
    * Not required to publish - but encouraged to do so (prevents concurrent thinking issues later down the line). 

2022-04-21 - EvoStar #3
~~~~~~~~~~~~~~~~~~~~~~
**Location:** Madrid, **Time**: Tuesday 9.30pm - 11.30pm, **Attendees:** Jesse Wood, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al.

The final day of the conference, involved a plenary talk and prize giving ceremonies. 
More information can be found on the full conference program - https://easychair.org/smart-program/Evo2022/

Pablo Geruas gave a EvoMUSART talk on "Evolutionary construction of Stories that Combine Several Story Lines"

Notes: 
    * Star Wars used as an example during the talk for storylines (kudos :D). 
    * Use evolutionary algorithms to evolve <unlegible> <unlegible> plot lines for a story. 
    * Made up of 20 popular plot lines from a textbook, assign a plot line to each character. 
    * Tricky to come up with a genetic representation for a story plotline.
    * Genetic representation must be manipulatable with genetic operators. 
    * Difficult to design an objective/fitness function. 
    * Fitness: validity - continuity of life and death, each character falls in love once (simplification). 
    * Fitness function averages over all validity metrics to evluate fitness. 
    * Eolvutionary approach good for creating multi-plotline stories with semantically valid discourse. 
    * Speaker gave a similar application e.g. a robot that generates plot line scripts for a leage of legends game. 
    * https://nil.fdi.ucm.ec/ 

Pedro Larrange gave the plenary talk on "Estimator of Distribution Algorithms In Machine Learning" 

Notes: 
    * Machine learning is a large focus of artificial intelligence nowadays. 
    * Construct a model from data, perform (non)-parametric optimization. 
    * Estimation of Distribution Algorithm (EDA). 
    * Bayesian Networks  (DAG + CPT), Directed Acyclic Graph (DAG), Conditional Probability Table (CPT).
    * Feature subset selection, Filter - only consider features, Wrapper - evaluate performance at machine learning task. 
    * Classification - a greedy algorith. 
    * Artifical Neural Network (ANN) - (Baluja 1995). 
    * Logistic regresion (Roles et al 2008) - led to intrepretable results for complex models. 
    * AdaBoost (Cagninin et al. 2018) - aggregated voting system between classifiers. 
    * Hierarchcial clustering (Fan 2019). 
    * k-Means (Forgy 1965), most popular culstering algorithnm, centroids shiftwed with hill climbing strategy. 
    * Reinfrocment learning (Honda and Nishive 2008) - relies on conditional markov fields. 
    * EDAs have not yet been applied to Support Vector Machines (SVM) - possible future work here (for me). 

2022-04-26 - Thesis Workshop 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Kirsten Reid, Jesse Wood et al.

Kirsten Reid, from learning support, hosted a Thesis Workshop for the Faculty of Graduate Research (FGR). 

Notes: 
    * Shape/formate appropriate for the field - i.e. Engineering / Artifical Intelligence. 
    * Linking ideas between chapters - see About Face 3 for great example. 
    * (Sub)headings are used effectively - maximum three levels of indentation for understandability by reader. 
    * Topic sentence for each new paragraph. 
    * If needed - glossary, acronyms, abbreviations - up front. 
    * Reminder of key concepts when needed, jog the readers memory, guide the reader along (a thesis is long). 
    * "Reader-friendly", avoid jargon, use plain text. 
    * Sentence length should vary to avoid monontonous tone (see https://bit.ly/38mTmAE)
    * Avoid too many nominisations - e.g. "we decided" rather than "we came to the decision" - focus on action verbs over their nouns. 
    * Don't have front-ended setnences, cut the wheat from the chaff, 1/3 of writing can be removed for brevity usually. 
    * Make appointment with Student Learning - limited to 50 minutes. 
    * Sentence cohesion worksop coming up soon. 
    * Kirsten happy to reply to Emails for further advice on this workshop. 
    * Three voices: research, data, researcher. 
    * From FGR: 4 workshops coming up for PhD students in May.

2022-04-28 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO 352, Cotton Building, **Time**: Monday 12pm-1pm , **Attendees:** Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Notes: 
    * Can claim GC dataset research as preliminary work for the REIMS data in my proposal. 
    * Transfer models from GC to REIMS data (likely) - this supports the preliminary claim. 
    * Pre-training on NIST GC refraction index data - needs a parametric (neural network) model for this technique. 
    * TODO: Apply EvoCNN to GC data. 
    * TODO: Apply Genetic Programming (GP) data to GC data. 

2022-04-28 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Jesse Wood (me) showed a talk "Telsa AI Day (supercut)". 

Notes: 
    * Disclaimer: I submitted this talk request on the 21st of April (7 days ago) - this is not political, just a coincidence. 
    * "Iron Man in Real life" - a comment from the Zoom chat. 
    * If you are interested in another high level summary:
    * Lex Fridman AI Day (summary by MIT researcher) - https://www.youtube.com/watch?v=ABbDB6xri8o
    * Here is the full video, it includes references to academic papers:
    * Full AI Day (3 hours long) - https://www.youtube.com/watch?v=j0z4FweCy4M
    * Supervisor liked my running commentary and links to further watching. 

2022-04-22 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

General Notes:
    * Zhixing won best paper for EuroGP. 
    * Shaolin had a TEVC paper revision. 
    * Vincent and Tao had a paper accepted by CEC. 

Talk from Xioying Sharon Gee on "Text Representation" 

Notes:
    * Inverse document frequency - use to elminate words that are too common - e.g. "the". 
    * Word emeddings, analyse similarity (relateness) with PCA - reduce to lower dimensional space (2d / 3d). 
    * CNN, Transformers, Attention, BERT, Pre-training. 
    * Pre-training of deep bidirectional transformers for language understading (BERT). 
    * Word masking, pre-training, next sentence prediction (NSP). 

2022-04-02 - Research Workshop 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 11pm-1pm , **Attendees:** Jesse Wood 

FGR hosted a workshop "How do I publish and disseminate my research". 

Notes: 
    * https://scopus.com - see citation statistics for papers/journals - useful metrics to judge quality. 
    * Can see stats for a journal, and comapre this to other jounrals, to assess the credibility. 
    * These tools are useful for making/measuring goals as a researcher. 
    * http://login.webofknowledge.com/ -  useful tool for mainly STEM disciplines. 
    * Eslever Journal Finder - provices acceptance nodes. turn around times - useful for finding which journal is appropriate. 
    * www2.cabells.com/jouranlytics - see a journal's publishing frequency, may be anually, this is a long wait. 
    * Consequences: 
        * Who reads it? 
        * Does it publish articles like yours? 
        * Does your style match? 
        * Would you need to change to submit? 
        * Peer-reviwed? 
        * Time to publish? 
        * Substantial paper. 
        * Tolerable rejection rate. 
        * Preferred type of journal. 
        * Solid reputation/metrics. 
        * Many articles a year. 
    * Keywords are good for SEO, they ensure the discoverability of work, and extend your audience. 
    * Rejection happens, may provide feedback (may not). 
    * Query letter - send an abstract to a journal to test the waters - see if a paper is appropriate. 


2022-04-11 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-5pm , **Attendees:** Mathew O'Connor, Bastiaan Kleign, Jesse Wood et al.

Matt O'Connoer gave a talk "Unsampling Artifacts in Neural Audio Synthesis" https://ieeexplore.ieee.org/abstract/document/9414913

Notes: 
    * Imrpove the quality of audio using a neural network approach. 
    * CNN is for images, but we need an architecture for audio specifically - this must handle audio specific problems (e.g. time, harmonics). 
    * Convolution (collapse), transposed (expand). 
    * Transposed convolutions are widely used. 
    * Upsampling methods: 
        * Stretch (insert zeros) 
        * NN (nearest neighbours)
        * Linear 
    * The unsampling algorithm leaves artifacts in the output sample. 
    * Spectral replicas emerge when sampling/discretizing the signal. 
    * All up sampling methods sound very similar - in the human audible range. 
    * NN, no artifacts, but frequency filter at zero frequency. 
    * Out of distribution test, shows major artifacts, since it wasn't in training. 
    
2022-05-05 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO 352, Cotton Building, **Time**: Monday 12pm-1pm , **Attendees:** Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Notes: 
    * Reading this week - Evolutionary ML Survey (60% done). 
    * Apply GP to the GC fish dataset. 
    * Later consider the time domain: terminal set, symbolic regression. 
    * GP Hello World! Try find a tutorial for this. 
    * DEAP does not have "Elitism" - I will need to implement my own.
    * Elitism:
        * Keep the top solutions between generations. 
        * Ensures performance can't decrease. 
        * Does not guarantee performance will increase. 
    * Future work - transfer learning can be: 
        * Paramters 
        * Model 
        * Feature (selected/constructed)
    * Terminology; domain adaptation, domain generalisation. 
    * N.B. I shoudl record my tutor meetings on zoom (even if they are in person) to make use of Panopto's free transcription software while I am a staff member. 

2022-05-05 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Peng Wang, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Peng Wang proposed a talk from Prof. Zhihua Zhou on "From Pure Learning to Learning+Reasoning" https://www.youtube.com/watch?v=LAvRDCcXCMc 

Notes: 
    * Most machine learning techniques rely on large database of labelled training data. 
    * We can leverage unlablled data, to improve performance on labelled data. 
    * Self-learning, use a classifier to apply psuedo classes to unlabelled data. 
    * SETRED 2005 - data editing, cleans up self learning. 
    * Active learning (AL); uses an oracle to query (label) unlabelled data, rely on minimizing queries to oracle (this requires human supervision). 
    * Representative AL approach - informative/representative. 
    * AL requires human-in-the-loop. 
    * Semi-supervised learning (SSL) - see Lex podcast for more details https://www.youtube.com/watch?v=FUS6ceIvUnI 
    * Indepedndent and Identically Distributed (IID). 
    * Semi-supervised SVMs (S3VMs) (Zemma 201u6, zemmal2016adaptative).
    * Using inlabeleld data to ensure the decision boundaries are drawn through low density areas. 
    * Tri-training approach, three learners, tha can teach eacother, and perform ensemble learning. 
    * Ensemble learning - uses multiple models and combines them to make a prediction. 
    * Holy grail: machine learning + logical reasoning. 
    * Probabilistic Logic Programming (PLP) - heavy-reasoning light-learning.  
    * Statistical Relation Learning (SRL) - light-reasoning heavy-learning.
    * Proposal: abductive learning: 
        * Deductive 
        * Inductive 
        * Abductive - Inversly embed deductive reasoning into inductive reasoning. 
    * Knowledge Base (KB), a series of first-order logic predicates. 
    * Instance --> Psuedo lables --> Psuedo groundings --> KB. 
    * Optimize minimzed inconsistency in the system. 
    * ABL does not rely on ground-truth labels. 
    * SSL for court sentences in China, similar to Andrews work (see "2022-03-25 - ECRG" above). 

2022-05-06 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Notes:
    * Celebtration for AI group since Bastiaan recieved his RSNZ Fellowship - Wednesday 18th May from 12 pm - 1 pm in CO3250. 
    * Hayden, camera-ready (?) paper for Gecko (?) workshop. 
    * Candidate Development Plan (CDP) is due by the end of May - this is due every 6 months (see email https://mail.google.com/mail/u/0/#search/CDP/FMfcgzGpFgvPXpqBwmBtBvqBZPxxQFtB).
    * University policy for thesis students does not require use to publish any papers. 
    * STEM has a publishing-forward culture when compared to other fields (e.g. law or humanities). 
    * Evolutionary Algorithm:  
        * Initialisation
        * Cycle 
            1. Evaluation. 
            2. Selection. 
            3. Reproduction. 
            4. Repeat 1-3 until termination condition. 
    * Meng has not seen anyone miss genetic operators, but a fair number of research papers ommit the "evaluation" and "selection" sections. 
    * DON'T OMMIT THESE SECTIONS! (Unless you have a very good reason not to). 
    * Ideally, have a nice flow diagram in a paper, to explain the training process for the model (this figure is a good use of space!!!). 

2022-05-09 - Deep Learning 
~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Ciaran King, Jesse Wood, Bastiaan Kleign, et al.

Ciaran King gave a talk on "Learning from Protein Structure with Geometric Vector Perceptrons" (Jing 2020, jing2020learning) https://openreview.net/forum?id=1YLJDvSx6J4 

Notes: 
    * Perverse incenstives for acadmiecs to over sell their work and a major reproducability crisis in deep learning. 
    * Graph nerual Networks can be used for protien folding. 
    * Equivariance to rotations - if the networks thinks the same instance rotates is a completely different structure, this is very inefficient. 
    * Instead we want rotation invariant representations for things like protiens. (Like we wan't time invariant representations for gas chromatography). 
    * Voxels are 3D pixels, these can be used to make a 3D representation of an instance, which then applies a 3D Convolutional Neural Network. 
    * We think that (1) message passing and (2) spatial convolution, are both well suited for different types of reasoning. 
    * In protein folding, their are chemical propoerties of protiens that simplify the combinatorial search space for the graphical neural network. 
    * This is similar to how the AI Feynman (Tegmark 2020, tegmark2020ai) used properties of physics equations to simplify symbolic regression. 
    * I would like to apply simplification using domain expertise in chemistry to my gas-chromatography and mass spectrometry data. 

2022-05-12 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** CO352, **Time**: Thursday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Bing Xue, Mengjie Zhang, Jesse Wood. 

Notes: 
    * Audio-only private recordings of weekly meetings for study purposes is ok. 
    * Plant & Food collecting information on students (like myself) that have been working for them. 
    * Conferences: 
        * GECCO - Genetic Evolution Computation Conference. 
        * EvoStar (attended in 2022). 
        * IEEE CC - IEEE Congress of Evolutionary Computation. 

    * Camera ready? This is ready for print, a final version of the paper that has responded to feedback and been formatted for the journal. 
    * CDP - Candidate Development Plan, is due this month. Scope is for 6 months only. This is my first so I have no goals from previous CDP. 
    
TODO: 
    * [ ] Readings on Genetic Programming (GP)
        1. TranEtAl2015GPfsfc https://link.springer.com/article/10.1007/s12293-015-0173-y
        2. tran2019genetic https://www.sciencedirect.com/science/article/pii/S0031320319301815?dgcid=rss_sd_all
        3. tran2017new https://ieeexplore.ieee.org/document/7956226
    * [ ] Send a draft CDP to my supervisors for feedback. 

2022-05-12 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Thursday 2pm-3pm , **Attendees:** Cuie Yang, Jesse Wood, Bing Xue, Bach Hoai Nguyen, et al. 

Cuie suggested the talk "You are what you read" https://www.youtube.com/watch?v=Du7qLsToW-o

Notes: 
    * Light reading, figure fishing - 80% comprehension. 
    * Skip to pictures like a kid, we are all kids at heart. 
    * Deep read, creative/critical reading.
    * Shallow reading is important, a paper has four parts.
        1. Title 
        2. Abstract 
        3. Introduction 
        4. Rest 
    * Quick reading and shallow learning is most important for the majority of readers. 
    * Reviwers are busy people, bathroom reviews, a lot of reviwed is based on superficial details. 
    * Notes - a format for notes from skim reads of a paper: 
         1. About 
         2. Problem 
         3. Interesting 
         4. New
         5. Neat
    * Two word titles: "Snappy sampling". 
    * Find collegaues to collaborate with and share knowledge. 
        * Give and take, time is a resource. 
        * what NOT to read? 
    * Future work: 
        1. Good ideas
        2. Improvements
        3. Applications/Extensions
        4. Your opinion
    * Come up with your own: 
        * Other domains 
        * Moon-shit
        * Related ideas

TODO: 
    * [ ] How to get your SIGGRAPH paper rejected https://www.kormushev.com/public/How_to_Get_Your_SIGGRAPH_Paper_Rejected-by_Jim_Kajiya.pdf