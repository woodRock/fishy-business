Minutes
=======

This page contains the minutes for our weekly meetings. 

2022-02-23 - Planning 
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** C0355, **Time**: Monday 1pm-2pm , **Attendees:** Jesse Wood, Mengjie Zhang

Notes: 
    * Faculty of Graduate Research (FGR) - office on Kelburn Parade. 
    * Forms and information for enrollment is available at the FGR website. 
    * Booked a room for study in MARU101 - Desk 33
    * See Duncan in ECS for an account. 
    * Can work up to 12 hours per week. 
    * Let supervisors and faculty know about trips out of Wellington. 
    * Start as provisional registration, then candidate - write proposal, fully registered - proposal accepted. 
    * Two required meetings, FASLIP (Thursday 2pm-3pm), ECRG (Friday 3pm-5pm).

2022-02-28 - FGR
~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Matthew Vink, Jesse Wood

Notes: 
    * This meeting covers enrollment, we will be confirming details, forms, contacts.
    * PhD Supervisors: Bing Xue, Mengjie Zhang.
    * Documents: 
        1. Confirmation of study - AIML 692 code. 
        2. Fees assessment - two weeks to pay levees. 
    * Information sheet:
        1. Community for needs bank details. 
        2. Tony mcGloughin - School Administrator. 
        3. Confirmation of Proposal Registration Form (CoPR).
        4. Mathew Vink - helped me enroll today. 
        5. Student levees - 2 weeks due.  

2022-02-28 - FASLIP
~~~~~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Wednesay 2pm-3pm , **Attendees:** Matthew Vink, Jesse Wood

Neil Dodgeson - Cambridege lecture
    * How to not give a presentation. https://vimeo.com/51597270
    * How to present a paper. https://vimeo.com/7833850

Notes: 
    * Simular to ENGR401 stuff
        * Dont need slides.
        * Trip check technologies. 
        * Face audience. 
        * Relevant stuff only. 
        * No animations. 
    * Research Talks 
        * Don't type the script. 
        * Planning, a lot of time before writing slides. 
        * Audience, can change how you deliver a presentation. 
        * Highlight key points on the last slide. 

2022-03-07 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Bastiaan Kleign, Jesse Wood, et al. 

Papers: 
    1. Conditional Diffuction Probablistic Model for Speech Enhancment. https://arxiv.org/abs/2202.05256
    2. A Study on Speech enhancment on Diffusion Probabilistic Model. https://arxiv.org/abs/2107.11876

Notes:
    * Diffusion models got attention for synthesising images (i.e faces, animals). 
    * Later, it bet the GAN on standard benchmarks. https://proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html
    * Train it to add noise, the reverse the process. 
    * Diffusion: learning to denoise speech signal. 
    * Isotropic gaussian distribution? https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic
    * Learn the signal-to-noise difference, not the mean signal. 
    * Diffusion markov chain is intractable, so we use Elbo to from an approximate objective function. 
    * Ratios and constants to ensure the mean and variance don't explode or vanish. 
    * New Mailing list for DL updates. 
    * Next week: Bayesian Transformers. 

2022-03-08 - Induction
~~~~~~~~~~~~~~~~~~~~~~
**Location:** AM101, **Time**: Monday 2pm-4pm , **Attendees:** Neil Dodgeson, Jesse Wood, et al. 

Notes:
    * Bastiaan slide example for meetings. 
    * Neil Dodgeson - Faculty of Graduate Research Dean. 
    * Faculty of Graduate Research (FGR). 
    * Workshops, writing events, professional development. 
    * Website https://www.wgtn.ac.nz/fgr
    * Workshops are practical and hands-on. 
    * Thesis bootcamp - 20 writing hours. 
        * Aimed at final year students. 
        * June november 
    * Research room 
        * Review, tips, stories, events, resources. 
        * Updates monthly. 
    * Candidate progress form (CPF)
        * Report on 6 monthly progress in a report. 
        * May / November. 
        * Required, not academic, supporting evidence. 
    * 4 weeks annual leave, no formal process. 
    * Suspensions, for illness, bereavement, work. 
    * Forms for aforementioned available online. 
    * Proposal: first major milestone. 
        * 12 month deadline. 
        * no extensions available. 
    * Automatic re-registration for first 2 years. 
    * Constructive relationship with supervisor. 
    * PhD certificate: competent to do invidual research. 
    * Work expert in our PhD Research topic. 
    * Regular meetings times. 
    * Student/supervisor - same page for expectation.
    * Bring agenda to meeting.
    * Project management techniques - scrum, agile. 
    * 2pi rule for time estimation. 
    * Secondary supervisor - (usually) hands off role. 
    * "The only way through it, is to do it." 
    * Books, publications, thesis - different expectations for each course. 

2022-03-10 - Weekly 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-4pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood 

Notes:
    * Let Bing/Meng know about any financial difficulties. 
    * Topic ideas: 
        1. Multi-objective 
        2. Evolutionary computation
        3. Domain expertise. 
    * First two-weeks - extensive background reading. 
    * ECRG - meeting tomorrow from 3pm - 5pm. 
    * CoPR - fill out by the end of March. 
    * Individual induction - copy Bach in email for meeting. 
    * Add Bach to gitlab/github for the paper latex file. 

2022-03-10 - FASLIP
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-4pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Jeff Hawkins - Thousand Brains Theory: https://www.youtube.com/watch?v=O4geanMOsyM

Notes:
    * Voting, similar to droupout, bagged ensemble. 
    * Many models (sub-networks) for the same thing. 
    * Sparse networks, efficient -> noise tolerant. 
    * Only update in one area, without need for back-propagation, doesn't require a full training for each new instance. 
    * Builds a full world model, not a model for each task. 
    * Thousand brain theory - solution to No Free Lunch. 

2022-03-11 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Hui Ma, Bach Hoai Nguyen, Jesse Wood et al. 

Hui Ma gave presentation on Evolutionary Computation Approaches to Web Service Composition - https://link.springer.com/article/10.1007/s10732-017-9330-4

Notes:
    * Meng will discuss how to write a proposal. 
    * EuroGP conference - ask my supervisor to register. 
    * Introduced myself to the group 
        * paper - finish writing my Summer Research paper. 
        * enrolled - lots of paper work. 
        * Finish writing the paper properly. 
    * Abdullah (lab neighbour) first week in group.
    * Evolutionary Computation Approaches to Web Service Composition. 
    * Over 40 publications in the area. 
    * Holidy booking service used as an example. 
    * Organize services into re-usable modules. 
    * Service composition is a NP-hard problem. 
    * A global search is not possible, a heuristic based local search is required. 
    * Evolutionary principles and techniques - crossover, mutation.
    * Automatcally create hybrid services through composition. 
    * Don't reinvent the wheel, use existing libraries instead. 
    * Scheduling, routing, resource allocation, service composition - all possible for EC. 
    
2022-03-17 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood. 

Notes:
    * Daniel can provide domain expertise for writing the chemistry sections for the paper. 
    * Multi-objective - classify chemical compounds and their percentage.  
    * Multi-label - one instance can belong to multiple classes. 
    * copy Bing and Bach for induction email from Georgia. 
    * pymoo  - multi-objective python library. 
    * Read/write summaries for papers as I go - write content for second chapter iteratively. 
    * Send Daniel conclusions / contributions of paper in email, then organize a follow up meeting.

2022-03-17 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Ying, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Ying suggested a talk on Multi-objective Evolutionary Federated Learning https://vimeo.com/552900291

Notes: 
    * Yaochi Jin - University of Surrey. 
    * Multi-objective machine learning. 
    * Centralized and federated learning. 
    * Evolutionary multi-objecive federated learning. 
    * Evolutionary federated nerual architecture search. 
    * Multi-objective - gives a solution set, as their are tradeoffs between objectives. 
    * Dominance, no X is worse and Y, and X is strictly better than Y for object A. 
    * pareto front (See tegmark2020ai) set of optimal solutions.
        * accuracy, diversity. 
        * Inverse generational distance (IGD).
        * Hypervolume - nadir 
    * Optimize for minimal complexity implies interpretability. 
    * Centralized learning - one database. 
    * Localized learning - everyone trains their own model. 
    * Privacy techniques: 
        * Secure multi-party computation. 
        * Differential privacy. 
        * Homomorphic encyption. 
    * Federated learning 
        * train a high-quality centralized model with training dataq distributed over a large number of clients. 
        * Each with unreliable and relatively slow network connections. 
        * horizontal - all attributes, batches of data. 
        * vertical - trained on subset of attributes (i.e. security reasons). 
    * Federated learning objectives 
        1. Maximise learning performance. 
        2. Minimize communication cost. 
    * Their work efficiently reduce the number of connections while maintaining similar performance. 
    * Neural architecture search (TODO - watch the rest and take notes!!!)

2022-03-18 - ECRG 
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

A talk on Geometric Semantic Genetic Programming by Qi Chen https://link.springer.com/chapter/10.1007/978-3-642-32937-1_3

Notes: 
    * We have published heaps of papers that are highly cites and hot papers according to https://www.webofscience.com/wos/woscc/basic-search toool that the university has access to.
    * Top 1% of papers cited per discipline for computer science journals. 
    * Evolving neural networks with evolutionary computation. 
    * Me: reading psychology papers on how the brain works with memory - hunting for relative simply neuro-science ideas to apply to machine learning. 
    * Geometric smenatic genetic programming (Morgalio 2012, moraglio2012geometric) https://link.springer.com/chapter/10.1007/978-3-642-32937-1_3 
    * Semantic genetic programming methods. 
    * Traditional GP ignores program semantics. 
    * Consequences - ragged gentype-phenotype mapping. 
    * Is it possible to make GP aware about the effects of the program execution? 
    * Semantics: 
        * Semantics differs from syntax. 
        * Semantics related to the problem domain. 
        * Semantics inform program design (Tegmark 2020, tegmark2020ai).
    * Measure semantic distance between current program and target output (multi-dimensional loss function). 
    * Genetic operators: 
        * Semantic aware cross-over (SAC) 
        * Semantic similarity-based cross-over (SSC)
        * Semantic similarity-based mutation (SSM)
        * Senantic tournament selection. 
            * t-test for statistical signfician with assessing selection. 
    * Search directly in the semantci space of the program. 
    * Semnatics of offsrping must sit in between the ntercept between its two parents in semantic space. 
    * Therefore each offspring minimized distance to target semantics. 
    * Each generation gets closer to the target semantics, or atleast closer than the furthest parent. 
    * Independent of data, good effect on improving generalization, althougt not actual claim made in paper. 
    * Geometric semantic programming leads to a unimodel fitness landscape - a cone where the apex is the target semantics. 
        * manhattan distance - square based pyramid.
        * euclidean distance - cone. 
    * Efficient implementation - only store changes to program tree, similar to git version control - except for GSGP. 
    * GSGR Red (reduce), simplify problems by expanding and recomputing. 
    * Locally geometric semantic crossover (LGSX).
        * Make offsrping similar to eachother than their parents. 
    * Random desired operator (RDO), exploit interoperability of instructions, + can be reversed with -, * can be reversed with division, + and * are communicative. 
    * Semantic backpropogation - decomposibility of the process if important for BP. 
    * Angle aware metrics - larger angle metrics iis more likely to generate offspring closer to target semantics. 
    * Permutations GSX and Random Segment Mutation 
    * Semenatic distance (euclidean) is the same as the loss, just looking at it from a different point of view. 
    * Can geometric smeantic programming work in an unsupervised or combinatorial problem? (Possibly not unimodel semantic space)
    
2022-03-21 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Hayden Dyne, Bastiaan Kleign, Jesse Wood, et al. 

Talk by Hayden on two papers: 
    * End-to-end driving via conditional imitation learning (Cai 2020, cai2020high) https://ieeexplore.ieee.org/abstract/document/8460487/
    * High-speed autonomous drifting with deep reinforcement learning (Codevilla 2018, codevilla2018end) https://ieeexplore.ieee.org/abstract/document/8961997/ 

Notes: 
    * Model-free reinforcement learning - does not rely on human understanding of world and design controllers. 
    * Human driver is the trajectory with is the goal, uses a professional driver playing the game with a steering wheel. 
    * Model performs on different track difficulties. 
    * Reward function is scaled by velocity, so faster lap times are rewarded. 
    * Works for 4 different kinds of vehicles, although the truck struggles to achieve same performance as lighter ones. 
    * Second paper - e2e 
    * Far easier to use real-world data on driving that has already been collected than generate simulation data. 
    * Data augmentation used to help network generalize to new scenarios and edge cases not in the training data. 

2022-03-24 - Faculty Induction
------------------------------
**Location:** Zoom, **Time**: Monday 10am-11am , **Attendees:** Georgia Dix, Jesse Wood, Bach Hoai Nguyen.

Induction to my PhD studies with supervisor and faculty. 

Notes: 
    * Expectations
        * Supervisor
            * Uni life 
            * Framework 
            * Networking 
            * Assessment 
        * Me
            * action plan 
            * identify problems 
            * administration 
            * CDP (6 monthly report)
    * Marking can thesis can take up to 6 months - can work during this time. 

2022-03-17 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood. 

Notes:
    * Daniel can draft the chemistry parts for the paper. 
    * Draft the full paper with Bach, then send to Daniel. 
    * Read "From evolutionary computation to the evolution of things" - Nature
    * Can start coding now - explore ideas for ENGR489 and EC on existing data. 
    * Transformers, LSTM, GAN - yet to be applied to GC-MS data in literature. 
    * CNNs for GC, likely due to libraries, hype, understanding, Diffusion of innovation. 
    * Scuba diver experiment for context-dependent memory is a good analogy for noise in ML models.
    * Came up with evolutionary ideas, like sexual selection, but (Miller 1994) did it quite some time ago.
    * Idea for EC, a dynamic environment where complexity increases, classes or features are added - similar to evolution IRL. 

2022-03-24 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Ying, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Ruwing Jiao suggested a video on Bayesian Optimization from Mark Deisenroth https://www.youtube.com/watch?v=_SC5_2vkgbA 

Notes: 
    * Recommended background reading on this topic: 
        1. A Tutorial on Bayesian Optimization of Expensive Cost Functions (Brochu 2010, brochu2010tutorial) https://arxiv.org/pdf/1012.2599.pdf
        2. Taking the Human Out of theLoop: A Review of Bayesian Optimization (Shahriari 2015, shahriari2015taking) https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306
    * Latent Structural Support Vector Machine (Miller 2012) - **TODO** find this paper/project. 
    * Deep learning often involves a lot of hyper-parameter tuning, this is usally done by the practitioner model. 
    * Alternative approaches: 
        * Manual tuning 
        * Grid search 
        * Random search 
        * Black magic (i.e. lr is 1e-3 is "good")
    * Computationally expensive to search for global maximum in hyper-parameter search space. 
    * Globally optimize a black-box approach to evaluate (e.g. cross validatio error for a massive neural network). 
    * Use a probabilistic model to approximate the black-box model for the hyper-parameter search. 
        * create a proxy model - this learns an approximation of the space - with less computational cost to query that space. 
        * referred to ass proxy / approximate / surrogate. 
    * The standard model for optimizing a bayesian model is a gaussian process. 
    * Evaluate the proxy function once, this saves computation. 
    * A gaussian process minimized the uncertainty of the proxy function.
    * It samples the feature space at the minimum value of the shaded area (== uncertainty).
    * It repeats this often, until the proxy function is close enough to the true objective. 
    * Exploration - sample areas with high uncertainty. 
    * Exploitation - sample places with low mean. 

2022-03-25 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Andrew Lenson, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Andrew gave a talk on Genetic Programming, Explainability and Interdisciplinary AI.

Notes: 
    * Heaps of students successfully submitted papers to the Gecko conference. 
    * Possible to publish in conference at different levels; paper, poster, etc. 
    * If a paper is declined, revise with reviewer comments, and resubmit as poster. 
    * Qurrat Al Ain - Cancer research in AI. 
    * Swiss roll manifold problem
        * Reduce a manifold to a 2D visual representation. 
        * 2D path is representation of non-linear dimensionality reduction (NLDR).
    * Geo-desic, shortest path from A to B, not shortest euclidean distance. 
    * Lower dimensional space is referred to as an embedding. 
    * We can use AI to learn or approximate this embedding (if the problem is intractable).
    * Ways to estimate the intrinsic dimensionality of the dataset - statistical techniques. 
    * Kaka - count distinct nnumber of birds at Zealandia. 
    * GoPro for data collection combined with crack for Kaka. 
    * Law - predicting sentencing lengths with PLSR on judge summaries. 
    * Names with high/low probabilities are often historic cases referred to as 'guidance judgements'. 
    * Combine data analysis and domain expertise to infer knowledge about sentencing lengths. 
    * Home detention or communtiy service are associated with shorter sentences. 
    * Future work, take humans out of the loop, and make sentencing deterministic.
    * ^ This can be done, because their are extenuating circumstances that require a judges opinion.
    * Also, if all sentences are automated, there would no longer be guidance judgements being set historically. 
    * Law is a dynamic and decentralized system, unique and specialized for each country, case, individual, etc... 
    * Research more productive on letting judges analysis their blindspots, and identify bias.

2022-03-28 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Daniel Braithwaite, Bastiaan Kleign, Jesse Wood, et al. 

Daniel Braithwaite talked about two papers related to machine learning for audio wave construction:
    1. Deep Audio Priors Emerge from Harmonic Convolutional Networks https://openreview.net/pdf?id=rygjHxrYDB
    2. Harmonic WaveGAN https://www.isca-speech.org/archive/pdfs/interspeech_2021/mizuta21_interspeech.pdf

Notes: 
    * The idea is to look at harmonic convolutions, think convolution layer but designed for audio. 
    * WaveGAN and Harmonic WaveGAN use deep learning on audio signals. 
    * Harmonic, is better suited towards audio signals, than wave alone. 
    * Harmonic considers local connections / adjaceny better. 
    * **TODO** Read these papers and add to notes. 

2022-03-31 - Weekly   
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 12pm-1pm , **Attendees:** Bach Hoai Nguyen, Jesse Wood. 

Bing and Meng were both unwell this week. Important to send minutes for this meeting to them.

Notes: 
    * Augmentation - boost performance on the fish part dataset. 
        * Time-shift, shift data backwards and forward, to get time-invariant generalized model (may not work well).
        * Impute data, combine existing samples, add noise, etc... 
    * Worked on CNN from ENGR489 for classification task.
        * Issues with keras and sklearn libraries, stratified cross-fold validation and one hot encoding don't play nice together. 
        * CNNs, we use 1D convolution and pooling layers on time-series data. 
        * Existing ML + GC literature also use CNN for classification and regression tasks. 
        * These models are powerful for extracting features in spaces with local connectivity. 
        * Aim to use EC to perform neural architecture search for CNN hyperparamters - these differ for each dataset. 
    * Both EC and Bayesian Optimization approaches work for neural architecture search. 
        * However, EC has more interpretable results, e.g. a genetic algorithm produces an explainable tree. 
        * Neural networks are black-box and esoteric, we understand how (i.e. back-prop, SGD), but not why? 
        * EC produces simpler representations, that can be prodded with domain expertise.
    * Important to read heaps for first few months of PHD. 
        * Take original notes that can contribute toward a backgroup chapter of my proposal. 
        * Get an idea of what has been done, and what I want to do. 
        * Still reading psychology textbook on memory and the brain to establish conceptual framework for learning.
     
2022-03-31 - FASLIP 
~~~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 2pm-3pm , **Attendees:** Fangfang Zhang, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Fangfang suggested a video called the Big Reset 2.0 https://www.youtube.com/watch?v=-ePZ7OdY-Dw

Notes: 
    * Reinforcment learning for Robotic Arms. 
    * Deep blue beat Kasparov, but no AI can set up the chess board, a 6 year old can do that. 
    * Hugh Herr designed his own AI legs https://www.youtube.com/watch?v=CDsNZJTWw0w
        * AI prosthesis is cost prohibitive for the masses, but may work with diffusion of innovation in future. 
        * Prosthesis can up upgraded over time, biological body parts cannot, hardware/software updates for legs. 
    * Fake news - Jon Stewart said MSM has more trackers than ANY other media (adult entertainment, torrent sites, social media included).
    * Chomskey, MSMs job is to sell the educated privelaged wealthy elites as an audience to the corporations advertising. 
    * AI algorithms - social media, fake news, incentives. 
    * AI autonomous warfare proliferation - we need to ban slaughter bots https://www.youtube.com/watch?v=pOv_9DNoDRY
    * AI used for traffic management, screen-time punishment - pick up phone at cafe and pay the bill. 
    * RoboMaster - robot warfare, mechatronics, AI - physical robot warfare as a game/competition. 
    * Cosmo - Boris Sofman https://www.youtube.com/watch?v=U_AREIyd0Fc 
    * Narrow-AI and no free lunch problem - AI is good at solving very specific tasks, but not general intelligence. 
    * I have an industry project, that has real-world applications in a factory settings - i.e. reduce bycatch and maximize efficiency of food processing for fish. 

2022-04-01 - ECRG
~~~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Friday 3pm-5pm , **Attendees:** Andrew Lenson, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Jesse Wood et al. 

Meng and Bing were unwell, so Yi chaired the research group meeting. 

Notes: 
    * Bach (my supervisors) first day lecturing for COMP102. 
    * Me: I got 98% accuracy on the fish species dataset using a 1D CNN.
    * Shorter meeting, workshop cancelled, due to Meng being unwell.  

2022-04-04 - DL
~~~~~~~~~~~~~~~
**Location:** Zoom, **Time**: Monday 3pm-4pm , **Attendees:** Ciaran King, Bastiaan Kleign, Jesse Wood, et al. 

Ciaran King was gave a talk on "Experiences using Github Copilot".
    * Understands the context of code, can make abstractions for helper methods. 
    * Can write documentation for codebases.
    * Not software "correct" code, but (likely) the code we were going to write. 
    * Can write tests for codebases with very little leading. 
Daniel Braithwaite on "Fixed Neural Network for Stenography"
    * Hide messages in adversarial neural network. 
    * Pre-trained stenograph, results in non-zero error, we need perfect reconstruction for encryption.
    * Face anonymization, post a persons face online, then regenerate the face, but encrypt the private face. 
    * This lets friends anonmyously share images with their face online, without revealing their identity.
Bastian - contractivity of neural networks.  
    * Signal processing worries about getting non-stable linear filters for signals. 
Jesse Wood 
    * Evaluating Large Language Models Trained on Code https://arxiv.org/abs/2107.03374
        * 70% accuracy for basic DSA problems. 
        * Can't solve more difficult problems - doesn't optimize solutions for performance. 
        * CoPilot outperforms other state-of-the-art NLP code generation models. 
        * Requires "fine-tuning", supervised human intervention to hint towards correct answer. 
    * Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions https://arxiv.org/abs/2108.09293
        * 40% of code written with CoPilot has cybersecurity vulnerabilities.
        * CodeQL and other static analysis tools used to define the security of the code.
        * Security is a shifting landscape, WannaCry, Log4J - zero days kept secret by intelligence agencies. 
        * This is true of all code, the training data was written by humans. 
        * Potential vulnerability for future attacks if hackers know open-source repos are training data.  
        * Don't treat copilot as a "glass cannon", it doesn't deserve a false sense of security.
