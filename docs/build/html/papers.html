

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Literature Review &mdash; fish 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Thoughts" href="thoughts.html" />
    <link rel="prev" title="Minutes" href="minutes.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> fish
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="about.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="minutes.html">Minutes</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Literature Review</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#aizerman1964theoretical">aizerman1964theoretical</a></li>
<li class="toctree-l2"><a class="reference internal" href="#akkaya2019solving">akkaya2019solving</a></li>
<li class="toctree-l2"><a class="reference internal" href="#al2019survey">al2019survey</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bi2020gc">bi2020gc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#boser1992training">boser1992training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#brewer2006brown">brewer2006brown</a></li>
<li class="toctree-l2"><a class="reference internal" href="#brochu2010tutorial">brochu2010tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cai2020high">cai2020high</a></li>
<li class="toctree-l2"><a class="reference internal" href="#chase1973perception">chase1973perception</a></li>
<li class="toctree-l2"><a class="reference internal" href="#chen2021evaluating">chen2021evaluating</a></li>
<li class="toctree-l2"><a class="reference internal" href="#codevilla2018end">codevilla2018end</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cortes1995support">cortes1995support</a></li>
<li class="toctree-l2"><a class="reference internal" href="#craik1972levels">craik1972levels</a></li>
<li class="toctree-l2"><a class="reference internal" href="#craik1975depth">craik1975depth</a></li>
<li class="toctree-l2"><a class="reference internal" href="#da2018evolutionary">da2018evolutionary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ding2005minimum">ding2005minimum</a></li>
<li class="toctree-l2"><a class="reference internal" href="#eder1995gas">eder1995gas</a></li>
<li class="toctree-l2"><a class="reference internal" href="#eiben2015evolutionary">eiben2015evolutionary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#eich1975state">eich1975state</a></li>
<li class="toctree-l2"><a class="reference internal" href="#eyesenck1980effects">eyesenck1980effects</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fix1989discriminatory">fix1989discriminatory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fukushima1982neocognitron">fukushima1982neocognitron</a></li>
<li class="toctree-l2"><a class="reference internal" href="#garnelo2018conditional">garnelo2018conditional</a></li>
<li class="toctree-l2"><a class="reference internal" href="#godden1975context">godden1975context</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hand2001idiot">hand2001idiot</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ho1995random">ho1995random</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hofstadter1979godel">Hofstadter1979godel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#karras2020analyzing">karras2020analyzing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kennedy1995particle">kennedy1995particle</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kennedy1997discrete">kennedy1997discrete</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kingma2014adam">kingma2014adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kishore2021fixed">kishore2021fixed</a></li>
<li class="toctree-l2"><a class="reference internal" href="#koppen2000curse">koppen2000curse</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kullback1951information">kullback1951information</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lecun1989generalization">lecun1989generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#liu1995chi2">liu1995chi2</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loh2011classification">loh2011classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mantyla1998cue">mantyla1998cue</a></li>
<li class="toctree-l2"><a class="reference internal" href="#marhsall2022cybermarine">marhsall2022cybermarine</a></li>
<li class="toctree-l2"><a class="reference internal" href="#matyushin2020gas">matyushin2020gas</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mikolov2013linguistic">mikolov2013linguistic</a></li>
<li class="toctree-l2"><a class="reference internal" href="#miles1998state">miles1998state</a></li>
<li class="toctree-l2"><a class="reference internal" href="#miller1994exploiting">miller1994exploiting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#moraglio2012geometric">moraglio2012geometric</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nguyen2014filter">nguyen2014filter</a></li>
<li class="toctree-l2"><a class="reference internal" href="#olah2018building">olah2018building</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pearce2021empirical">pearce2021empirical</a></li>
<li class="toctree-l2"><a class="reference internal" href="#raine1997brain">raine1997brain</a></li>
<li class="toctree-l2"><a class="reference internal" href="#restek2018high">restek2018high</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robinson2020genetic">robinson2020genetic</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robnik2003theoretical">robnik2003theoretical</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scholkopf2000new">scholkopf2000new</a></li>
<li class="toctree-l2"><a class="reference internal" href="#shahriari2015taking">shahriari2015taking</a></li>
<li class="toctree-l2"><a class="reference internal" href="#szegedy2013intriguing">szegedy2013intriguing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tegmark2020ai">tegmark2020ai</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tegmark2020ai2">tegmark2020ai2</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tomasi2004correlation">tomasi2004correlation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tran2018variable">tran2018variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="#wolpert1997no">wolpert1997no</a></li>
<li class="toctree-l2"><a class="reference internal" href="#xue2014particle">xue2014particle</a></li>
<li class="toctree-l2"><a class="reference internal" href="#zhang2008two">zhang2008two</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="thoughts.html">Thoughts</a></li>
<li class="toctree-l1"><a class="reference internal" href="contact.html">Contact</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">fish</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Literature Review</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/papers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="literature-review">
<h1>Literature Review<a class="headerlink" href="#literature-review" title="Permalink to this headline">¶</a></h1>
<section id="aizerman1964theoretical">
<h2>aizerman1964theoretical<a class="headerlink" href="#aizerman1964theoretical" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>The original hyperplane algorithm used a linear kernel.</p></li>
</ul>
</div></blockquote>
</section>
<section id="akkaya2019solving">
<h2>akkaya2019solving<a class="headerlink" href="#akkaya2019solving" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Akkaya et al. propose a robotic hand that “single-handedly” solved a Rubiks cube cite{akkaya2019solving}.</p></li>
<li><p>They use Automatic Domain Randomization (ADR) and simulation to impute data for physics-based problems.</p></li>
<li><p>This technique was used to solve a Rubiks cube “single-handedly” by simulation.</p></li>
<li><p>It can be difficult to model an accurate physics engine.</p></li>
<li><p>Instead, ADR solves all possible sets of physics environments within given constraints for the Rubiks cube.</p></li>
<li><p>Through simulation, they create a model that generalizes well, with very little real-world experimentation needed.</p></li>
</ul>
</div></blockquote>
</section>
<section id="al2019survey">
<h2>al2019survey<a class="headerlink" href="#al2019survey" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Survey of evolutionary machine learning - Vuw staff.</p></li>
<li><p><strong>TODO</strong> read</p></li>
</ul>
</div></blockquote>
</section>
<section id="bi2020gc">
<h2>bi2020gc<a class="headerlink" href="#bi2020gc" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Bi et al. proposed a CNN model that incorporated GC-MS data fusion for food science.</p></li>
<li><p>The high-dimensional data was naturally suited towards the CNN.</p></li>
<li><p>Their work classified the flavour quality of peanut oil with 93% accuracy.</p></li>
<li><p>Similar to this project, the existing technique for analysis was intractable large scale.</p></li>
<li><p>The fusion of existing datasets improved the efficacy of their model.</p></li>
</ul>
</div></blockquote>
</section>
<section id="boser1992training">
<h2>boser1992training<a class="headerlink" href="#boser1992training" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Kernal trick for SVM.</p></li>
<li><p>These employ the kernel trick.</p></li>
</ul>
</div></blockquote>
</section>
<section id="brewer2006brown">
<h2>brewer2006brown<a class="headerlink" href="#brewer2006brown" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Flashbuld memories - recollections that seem vivid and clear, so we take them to be accurate.</p></li>
<li><p>Most likely occur for distinct stronly positive or negative emotional events.</p></li>
<li><p>Weddings, Funerals, Deaths, Tragedy, Violence.</p></li>
<li><p>We are more likely to be confident these are correct.</p></li>
<li><p>But our memory is shit, so we often re-write and incorrectly recall these events.</p></li>
<li><p>The distinictness of flashbulb memories, does help recall them longer, but does not guarantee correctness.</p></li>
</ul>
</div></blockquote>
</section>
<section id="brochu2010tutorial">
<h2>brochu2010tutorial<a class="headerlink" href="#brochu2010tutorial" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>A Tutorial on Bayesian Optimization of Expensive Cost Functions</p></li>
<li><dl class="simple">
<dt>Application:</dt><dd><ol class="arabic simple">
<li><p>Active User Modeling</p></li>
<li><p>Hierarchical Reinforcement Learning</p></li>
</ol>
</dd>
</dl>
</li>
<li><p>Covers the theory and intuition behind Bayesian optimizaiton with visual examples.</p></li>
<li><p>Discusses preference galleries, hierachichal control</p></li>
<li><p>Recommended reading from the FASLIP talk on Bayesian Optimizatio 2022-03-24.</p></li>
<li><p><strong>TODO</strong> read this!</p></li>
</ul>
</div></blockquote>
</section>
<section id="cai2020high">
<h2>cai2020high<a class="headerlink" href="#cai2020high" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>End-to-end driving via conditional imitation learning.</p></li>
<li><p>Model-free reinforcement learning - does not rely on human understanding of world and design controllers.</p></li>
<li><p>Human driver is the trajectory with is the goal, uses a professional driver playing the game with a steering wheel.</p></li>
<li><p>Model performs on different track difficulties.</p></li>
<li><p>Reward function is scaled by velocity, so faster lap times are rewarded.</p></li>
<li><p>Works for 4 different kinds of vehicles, although the truck struggles to achieve same performance as lighter ones.</p></li>
</ul>
</div></blockquote>
</section>
<section id="chase1973perception">
<h2>chase1973perception<a class="headerlink" href="#chase1973perception" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Domain expertise allows people to build meaningful schema to represent patterns.</p></li>
<li><p>Expert chess players recall 16 pieces, intermeidate 8, novice 4 when arranged in meaninful positions.</p></li>
<li><p>Recall was consistant for levels of expertise on nonsense chess boards.</p></li>
<li><p>Our mental schemas for encoding patterns break on noise (unseen data).</p></li>
</ul>
</div></blockquote>
</section>
<section id="chen2021evaluating">
<h2>chen2021evaluating<a class="headerlink" href="#chen2021evaluating" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>70% accuracy for basic DSA problems.</p></li>
<li><p>Can’t solve more difficult problems - doesn’t optimize solutions for performance.</p></li>
<li><p>CoPilot outperforms other state-of-the-art NLP code generation models.</p></li>
<li><p>Requires “fine-tuning”, supervised human intervention to hint towards correct answer.</p></li>
</ul>
</div></blockquote>
</section>
<section id="codevilla2018end">
<h2>codevilla2018end<a class="headerlink" href="#codevilla2018end" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>High-speed autonomous drifting with deep reinforcement learning.</p></li>
<li><p>Far easier to use real-world data on driving that has already been collected than generate simulation data.</p></li>
<li><p>Data augmentation used to help network generalize to new scenarios and edge cases not in the training data.</p></li>
</ul>
</div></blockquote>
</section>
<section id="cortes1995support">
<h2>cortes1995support<a class="headerlink" href="#cortes1995support" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Cortes and Vapnik proposed the Support Vector Machine (SVM).</p></li>
<li><p>This model creates a hyperplane that can draw distinct class boundaries between classes.</p></li>
<li><p>We call these class boundaries the support vectors.</p></li>
<li><p>We are performing multi-class classification, so it used a one-vs-all approach cite{sklearn2021feature}.</p></li>
<li><p>This creates a divide between one class and the rest, then repeats for the other classes.</p></li>
</ul>
</div></blockquote>
</section>
<section id="craik1972levels">
<h2>craik1972levels<a class="headerlink" href="#craik1972levels" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Levels of processing: A framework for memory research.</p></li>
<li><p>Elaborative rehearsal requires deeper processing than maintainence rehearsal.</p></li>
</ul>
</div></blockquote>
</section>
<section id="craik1975depth">
<h2>craik1975depth<a class="headerlink" href="#craik1975depth" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Deeper processing, semantic over structural or phonetic, better.</p></li>
<li><p>Depth processing increased later recognition of words in a list.</p></li>
<li><p>Annecodte, study: skim-read vs. thoughtful reading.</p></li>
</ul>
</div></blockquote>
</section>
<section id="da2018evolutionary">
<h2>da2018evolutionary<a class="headerlink" href="#da2018evolutionary" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Evolutionary Computation Approaches to Web Service Composition.</p></li>
<li><p>Service composition is an NP-hard combinatorial problem - local search via heuristic is needed.</p></li>
<li><p>Optimizes fitness as multi-objective function of correctness and exectution time.</p></li>
<li><p>Graph building algorithm that uses evolutionary techniques, mutation and crossover.</p></li>
<li><p>Don’t reinvet the wheel, encourage reuse of existing services.</p></li>
</ul>
</div></blockquote>
</section>
<section id="ding2005minimum">
<h2>ding2005minimum<a class="headerlink" href="#ding2005minimum" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Minimum Redundancy - Maximum Relevance (MRMR)</p></li>
</ul>
</div></blockquote>
</section>
<section id="eder1995gas">
<h2>eder1995gas<a class="headerlink" href="#eder1995gas" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Gas chromatography (GC) cite{eder1995gas} is a method that can identify chemicial structures in these fish oils.</p></li>
<li><p>This produces high-dimensional low sample size data from the fish oils.</p></li>
<li><p>Chemists compare a given sample to a reference sample to determine what chemicals are present.</p></li>
<li><p>The existing analytical techniques to perform these tasks are time-consuming and laborious.</p></li>
</ul>
</div></blockquote>
</section>
<section id="eiben2015evolutionary">
<h2>eiben2015evolutionary<a class="headerlink" href="#eiben2015evolutionary" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>From evolutionary computation to the evolution of things - Nature review article.</p></li>
<li><dl class="simple">
<dt>X-band antenneas for NASA Space Technology 5 (ST5) spacecraft</dt><dd><ul>
<li><p>Evolutionary-algorithm based aaporach discovered effective antennea esigns.</p></li>
<li><p>Also could adjust designs quckly when requirements changed .</p></li>
<li><p>One of these antennas was deployed, the first computer evolved hardware in space.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>EC has an advantage over manual design.</p></li>
<li><p>Similar to model-free in reinforcement learning (Cai 2020 - cai2020high, Codevilla 2018 - codevilla2018end)</p></li>
<li><dl class="simple">
<dt>State-of-the-art protein structure prediction</dt><dd><ul>
<li><p>Design an algorithm do develop complex energy functions with genetic programming.</p></li>
<li><p>EC great at exploring intractibly large combinatorial search spaces with high evaluation cost.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>EC have seperation of concerns, phenotype seperate from fitness, good modularity.</p></li>
<li><p>EC makes no implicit assumptions about the problem.</p></li>
<li><dl class="simple">
<dt>Trends</dt><dd><ul>
<li><p>Automated design and tuning of evolutionary algorithms.</p></li>
<li><p>Using surrogate models.</p></li>
<li><p>Handiling many objectives</p></li>
<li><p>Generative and developmental representations.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Crazy futurist ideas for this field, evolutionary factories, artificial bio-silica life, etc…</p></li>
</ul>
</div></blockquote>
</section>
<section id="eich1975state">
<h2>eich1975state<a class="headerlink" href="#eich1975state" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>State-dependent accessibility of retrieval cues in retneion of categorized list.</p></li>
<li><p>Subjects are asked to recall a list of words with and without the influence of marajuana.</p></li>
<li><p>Subjects who learn something high, are more likely to retrieve that information high.</p></li>
<li><p>People can not recall their drug-induced experience easily when they sober up.</p></li>
</ul>
</div></blockquote>
</section>
<section id="eyesenck1980effects">
<h2>eyesenck1980effects<a class="headerlink" href="#eyesenck1980effects" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Effects of processing depth, distinctiveness, and word frequency on retention.</p></li>
<li><p>In general distinct stimuli are better remembered than non-distinct ones.</p></li>
<li><p>We are more likely to remember things that are out of the blue, or that have a personal connection to us.</p></li>
</ul>
</div></blockquote>
</section>
<section id="fix1989discriminatory">
<h2>fix1989discriminatory<a class="headerlink" href="#fix1989discriminatory" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>K-nearest neighbours (KNN).</p></li>
</ul>
</div></blockquote>
</section>
<section id="fukushima1982neocognitron">
<h2>fukushima1982neocognitron<a class="headerlink" href="#fukushima1982neocognitron" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Rectified Linear Unit (ReLu) paper.</p></li>
<li><p>Activation function for neural networks.</p></li>
<li><p>Shares nice properties of linear function.</p></li>
<li><p>But allows for non-linearities to be captured.</p></li>
</ul>
</div></blockquote>
</section>
<section id="garnelo2018conditional">
<h2>garnelo2018conditional<a class="headerlink" href="#garnelo2018conditional" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Conditional Neural Processes.</p></li>
<li><p>Combine Bayesian optimizationa and Neural Networks.</p></li>
<li><p>Use Gaussian Processes (GP) to approximate functions within reasonable confidence.</p></li>
<li><p>Neural network, encoder-decoder GAN-like architecture to perform ML tasks.</p></li>
</ul>
</div></blockquote>
</section>
<section id="godden1975context">
<h2>godden1975context<a class="headerlink" href="#godden1975context" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Context-dependent memory in two natural environments: On land and underwater.</p></li>
<li><p>Scuba divers who learn lists of words underwater, best recalled them underwater.</p></li>
<li><p>Same true for words learnt on land.</p></li>
<li><p>Recall accuracy depends on similarity of context in sensory information.</p></li>
</ul>
</div></blockquote>
</section>
<section id="hand2001idiot">
<h2>hand2001idiot<a class="headerlink" href="#hand2001idiot" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Naive bayes.</p></li>
</ul>
</div></blockquote>
</section>
<section id="ho1995random">
<h2>ho1995random<a class="headerlink" href="#ho1995random" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Random forest.</p></li>
</ul>
</div></blockquote>
</section>
<section id="hofstadter1979godel">
<h2>Hofstadter1979godel<a class="headerlink" href="#hofstadter1979godel" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Godel Escher Bach</p></li>
<li><p>The hand that draws itself.</p></li>
</ul>
</div></blockquote>
</section>
<section id="karras2020analyzing">
<h2>karras2020analyzing<a class="headerlink" href="#karras2020analyzing" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>StyleGAN</p></li>
<li><p>Latent layer representation.</p></li>
<li><p>Manipulating latent layer gives a sense of semantically meaninful feature space.</p></li>
<li><p>We can see the change in style that sampling latent layer gives.</p></li>
</ul>
</div></blockquote>
</section>
<section id="kennedy1995particle">
<h2>kennedy1995particle<a class="headerlink" href="#kennedy1995particle" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Original PSO algorithm.</p></li>
</ul>
</div></blockquote>
</section>
<section id="kennedy1997discrete">
<h2>kennedy1997discrete<a class="headerlink" href="#kennedy1997discrete" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>PSO for feature selection.</p></li>
</ul>
</div></blockquote>
</section>
<section id="kingma2014adam">
<h2>kingma2014adam<a class="headerlink" href="#kingma2014adam" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Adam optimizer for neural networks.</p></li>
</ul>
</div></blockquote>
</section>
<section id="kishore2021fixed">
<h2>kishore2021fixed<a class="headerlink" href="#kishore2021fixed" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Hide messages in adversarial neural network.</p></li>
<li><p>Pre-trained stenograph, results in non-zero error, we need perfect reconstruction for encryption.</p></li>
<li><p>Face anonymization, post a persons face online, then regenerate the face, but encrypt the private face.</p></li>
<li><p>This lets friends anonmyously share images with their face online, without revealing their identity.</p></li>
</ul>
</div></blockquote>
</section>
<section id="koppen2000curse">
<h2>koppen2000curse<a class="headerlink" href="#koppen2000curse" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Curse of dimensionality.</p></li>
</ul>
</div></blockquote>
</section>
<section id="kullback1951information">
<h2>kullback1951information<a class="headerlink" href="#kullback1951information" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Kullback-Leibler (KL) divergence.</p></li>
<li><p>Measures distance between two probability distributions.</p></li>
<li><p>Most common loss function for deep learning with stochastic gradient descent.</p></li>
</ul>
</div></blockquote>
</section>
<section id="lecun1989generalization">
<h2>lecun1989generalization<a class="headerlink" href="#lecun1989generalization" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Original Convolutional Neural Network (CNN) paper.</p></li>
</ul>
</div></blockquote>
</section>
<section id="liu1995chi2">
<h2>liu1995chi2<a class="headerlink" href="#liu1995chi2" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>chi^2 classifier.</p></li>
</ul>
</div></blockquote>
</section>
<section id="loh2011classification">
<h2>loh2011classification<a class="headerlink" href="#loh2011classification" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Decision tree.</p></li>
</ul>
</div></blockquote>
</section>
<section id="mantyla1998cue">
<h2>mantyla1998cue<a class="headerlink" href="#mantyla1998cue" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Cue distinctiveness and forgetting: Effectiveness of self-generated retrieval cues in delayed recall.</p></li>
<li><p>Students were given a word list, and asked to make 1 or 3 retrieval cues.</p></li>
<li><p>Students with who used their own multiple retrieval cues had better recall.</p></li>
<li><p>Recall was terrible when using another students own personal retrieval cues.</p></li>
<li><p>Multiple self-generated retrieval cues is the most effective approach to maximising recall.</p></li>
</ul>
</div></blockquote>
</section>
<section id="marhsall2022cybermarine">
<h2>marhsall2022cybermarine<a class="headerlink" href="#marhsall2022cybermarine" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Cybermarine research magazine aims.</p></li>
<li><p>Focus on reducing by-product.</p></li>
<li><p>Non-destructure methods for analysis of chemical compounds in fish oil.</p></li>
<li><p>Factory of the future - uses AI to inform decisions in the assembly line.</p></li>
</ul>
</div></blockquote>
</section>
<section id="matyushin2020gas">
<h2>matyushin2020gas<a class="headerlink" href="#matyushin2020gas" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Matyshuin et al. proposed a stacking model for analysis of gas-chromatograph data.</p></li>
<li><p>It stacked the results of 1DConv, 2DConv, Deep Residual MLP and XGBoost.</p></li>
<li><p>Their model predicted the retention index for samples.</p></li>
<li><p>A retention index is a standardized value that only depends on the chemical structure of a compound.</p></li>
<li><p>Once identified the retention index can be used for further identification.</p></li>
<li><p>GC-MS data has underlying patterns that correspond to chemical compounds.</p></li>
</ul>
</div></blockquote>
</section>
<section id="mikolov2013linguistic">
<h2>mikolov2013linguistic<a class="headerlink" href="#mikolov2013linguistic" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Mikolov et al. found the word embeddings used in NLP were semantically meaningful cite{mikolov2013linguistic}.</p></li>
<li><p>They showed arithmetic could be applied to these word vectors that were interpretable.</p></li>
<li><p>For example “King” - “Man” + “Woman” = “Queen”.</p></li>
<li><p>The feature space was semantically meaningful, which serves as a powerful representation, that we intuitively reason with.</p></li>
<li><p>Similar thought has been applied to computer vision cite{olah2018building, karras2020analyzing}.</p></li>
<li><p>Semantically meaningful feature spaces allow for intuition about the behaviour of complex models, be it through visualisation or arithmetic.</p></li>
</ul>
</div></blockquote>
</section>
<section id="miles1998state">
<h2>miles1998state<a class="headerlink" href="#miles1998state" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>State-dependent memory produced by aeorobic exercise.</p></li>
<li><p>Students studies while exercising on a treadmil.</p></li>
<li><p>Material learnt on the treadmill was better recalled on the treadmill.</p></li>
<li><p>Greater information retrieval when the state (i.e. aerobic exercise) is similar.</p></li>
</ul>
</div></blockquote>
</section>
<section id="miller1994exploiting">
<h2>miller1994exploiting<a class="headerlink" href="#miller1994exploiting" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Complement natural selection with sexual selection.</p></li>
<li><p>Biological theory behind sexual selection.</p></li>
<li><p>Sexual selections influences culture around metrics for fitness/fertility.</p></li>
<li><p>Gendered candidate solutions.</p></li>
<li><p>Mate choice / mate preference.</p></li>
<li><p><strong>TODO</strong> read</p></li>
</ul>
</div></blockquote>
</section>
<section id="moraglio2012geometric">
<h2>moraglio2012geometric<a class="headerlink" href="#moraglio2012geometric" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Genetic semantic genetic programming.</p></li>
<li><p><strong>TODO</strong> read - related to Qi Chen talk on 2022-03-18 ECRG.</p></li>
<li><p>Unimodal fitness landscape, one global optima, but semantic search is intractable.</p></li>
<li><p>We approximate semantic search through geometric genetic programming methods.</p></li>
</ul>
</div></blockquote>
</section>
<section id="nguyen2014filter">
<h2>nguyen2014filter<a class="headerlink" href="#nguyen2014filter" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Nguyen et al. proposed a wrapper based PSO technique for feature selection in classification.</p></li>
<li><p>The algorithm uses a wrapper based fitness function of the classification error rate.</p></li>
<li><p>The local search only considers the global best using a filter based method.</p></li>
<li><p>It draws from the strengths of filter and wrapper based feature selection.</p></li>
<li><p>This proposed method outperformed three state-of-the-art and two traditional feature selection methods.</p></li>
</ul>
</div></blockquote>
</section>
<section id="olah2018building">
<h2>olah2018building<a class="headerlink" href="#olah2018building" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Semantically meaningful features in computer vision.</p></li>
<li><p>Distill <a class="reference external" href="https://distill.pub/2018/building-blocks/">https://distill.pub/2018/building-blocks/</a></p></li>
<li><p>Visualization techniques are powerful for understanding black-box systems.</p></li>
<li><p>Gain intution for semantically meaninful features in complex models.</p></li>
</ul>
</div></blockquote>
</section>
<section id="pearce2021empirical">
<h2>pearce2021empirical<a class="headerlink" href="#pearce2021empirical" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>70% accuracy for basic DSA problems.</p></li>
<li><p>Can’t solve more difficult problems - doesn’t optimize solutions for performance.</p></li>
<li><p>CoPilot outperforms other state-of-the-art NLP code generation models.</p></li>
<li><p>Requires “fine-tuning”, supervised human intervention to hint towards correct answer.</p></li>
</ul>
</div></blockquote>
</section>
<section id="raine1997brain">
<h2>raine1997brain<a class="headerlink" href="#raine1997brain" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Muderers pleading not guilty be reason of insanity (NGRI).</p></li>
<li><p>Pre-disposition to less activity in their pre-frontal cortex.</p></li>
<li><p>Pre-frontal cortex associated with goal-directed planning and delayed gratification.</p></li>
<li><p>Different brain chemistry meant more likely to perform violent impulsive behaviour.</p></li>
<li><p>Justification for lebotomy - electrocution of pre-frontal cortex - now replaced by anti-psychotics.</p></li>
</ul>
</div></blockquote>
</section>
<section id="restek2018high">
<h2>restek2018high<a class="headerlink" href="#restek2018high" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Explanation of gas-chromatraphy in food science for FAMEs.</p></li>
</ul>
</div></blockquote>
</section>
<section id="robinson2020genetic">
<h2>robinson2020genetic<a class="headerlink" href="#robinson2020genetic" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Demelza et al. proposed a feature and latent variable selection method for regression models in food science.</p></li>
<li><p>The vibrational spectroscopy dataset shared similarities in its high dimensionality and food science domain.</p></li>
<li><p>The purposes GA-PLSR generalized better and produced fewer complex models.</p></li>
<li><p>The study showed that Genetic Algorithms are powerful tools for feature selection in food science.</p></li>
</ul>
</div></blockquote>
</section>
<section id="robnik2003theoretical">
<h2>robnik2003theoretical<a class="headerlink" href="#robnik2003theoretical" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>releifF classifier.</p></li>
</ul>
</div></blockquote>
</section>
<section id="scholkopf2000new">
<h2>scholkopf2000new<a class="headerlink" href="#scholkopf2000new" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Nu-SVC classifier.</p></li>
<li><p>Setting the number of support vectors is a hyper-parameter.</p></li>
<li><p>Usually this is learned by the system.</p></li>
</ul>
</div></blockquote>
</section>
<section id="shahriari2015taking">
<h2>shahriari2015taking<a class="headerlink" href="#shahriari2015taking" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Taking the Human Out of theLoop: A Review of Bayesian Optimization.</p></li>
<li><p>Recommended reading from the FASLIP talk on Bayesian Optimizatio 2022-03-24.</p></li>
<li><p><strong>TODO</strong> read this.</p></li>
</ul>
</div></blockquote>
</section>
<section id="szegedy2013intriguing">
<h2>szegedy2013intriguing<a class="headerlink" href="#szegedy2013intriguing" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Intriguing properties of neural networks.</p></li>
<li><p>Adversarial attacks on neural networks.</p></li>
<li><p>Trick neural nets into making the wrong prediction on purpose.</p></li>
<li><p>Long tail problem of AI.</p></li>
</ul>
</div></blockquote>
</section>
<section id="tegmark2020ai">
<h2>tegmark2020ai<a class="headerlink" href="#tegmark2020ai" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Tegmark et al. developed they AI Feynman cite{udrescu2020ai}.</p></li>
<li><p>This algorithm can derive physics equations from data using symbolic regression.</p></li>
<li><p>Symbolic regression is a difficult task, but by simplifying properties exhibited by physics equations (i.e symmetry, composability, separability), the problem can be reduced.</p></li>
<li><p>Their work uses blackbox neural networks, to derive interpretable models that can easily be verified by humans.</p></li>
</ul>
</div></blockquote>
</section>
<section id="tegmark2020ai2">
<h2>tegmark2020ai2<a class="headerlink" href="#tegmark2020ai2" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>2nd iteration for the AI Feynman 2.0.</p></li>
<li><p>More robust towards noise and bad data.</p></li>
<li><p>Can discover more formulas that previous method.</p></li>
<li><p>Implements Normalizaing flows.</p></li>
<li><p>Method for generalized symmetries (abitrary modularity in the compuational graph formula)</p></li>
</ul>
</div></blockquote>
</section>
<section id="tomasi2004correlation">
<h2>tomasi2004correlation<a class="headerlink" href="#tomasi2004correlation" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Tomasi et al. investigated correlation optimisation warping (COW) and dynamic time warping (DT) for preprocessing chromatography data.</p></li>
<li><p>Unconstrained dynamic time warping was found to be too flexible.</p></li>
<li><p>The algorithm overcompensated when trying to fix the alignment in the data.</p></li>
</ul>
</div></blockquote>
</section>
<section id="tran2018variable">
<h2>tran2018variable<a class="headerlink" href="#tran2018variable" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Tran et al. propose a Variable-Length PSO.</p></li>
<li><p>Traditional PSO methods for feature selection are limited in the fixed length of their representation.</p></li>
<li><p>This leads to both high memory usage and computational cost.</p></li>
<li><p>The proposed algorithm allows particles to have shorter and different variable lengths.</p></li>
<li><p>Their length changing mechanism allows PSO to escape local optima.</p></li>
<li><p>Results across several high dimensional datasets showed improved performance in terms of computational time, fewer features selected and classification accuracy.</p></li>
</ul>
</div></blockquote>
</section>
<section id="wolpert1997no">
<h2>wolpert1997no<a class="headerlink" href="#wolpert1997no" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>No free lunch theorum.</p></li>
<li><p>No classification algorithm that beats the rest for every problem.</p></li>
<li><p>As training instances approaches infinity, classification accuracy on all distributions of noise, approaches predicting mean class.</p></li>
<li><p>All machine learning algorithms are task specific, don’t generalize to all problems, no artifical general intelligence (AGI), yet…</p></li>
</ul>
</div></blockquote>
</section>
<section id="xue2014particle">
<h2>xue2014particle<a class="headerlink" href="#xue2014particle" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Brown et al. proposed a PSO with novel initialising and updating mechanisms.</p></li>
<li><p>The initialization strategy utilized both forward and backwards selection.</p></li>
<li><p>The updating mechanism overcame the limitations of the traditional method by considering the number of features.</p></li>
<li><p>The proposed algorithm had better performance in terms of computing, fewer features selected and classification accuracy.</p></li>
</ul>
</div></blockquote>
</section>
<section id="zhang2008two">
<h2>zhang2008two<a class="headerlink" href="#zhang2008two" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Zhang et al. proposed a 2-D COW algorithm for aligning gas chromatography and mass spectrometry.</p></li>
<li><p>The algorithm warps local regions of the data to maximise the correlation with known reference samples.</p></li>
<li><p>This work uses data fusion with labelled reference samples, to improve the quality of new samples.</p></li>
</ul>
</div></blockquote>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="thoughts.html" class="btn btn-neutral float-right" title="Thoughts" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="minutes.html" class="btn btn-neutral float-left" title="Minutes" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Jesse Wood, Bing Xue, Mengjie Zhang, Bach Hoai Nguyen, Daniel Killeen

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>