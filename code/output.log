nohup: ignoring input
[I 2025-08-15 17:22:42,405] A new study created in memory with name: contrastive_simclr_transformer_optimization
A new study created in memory with name: contrastive_simclr_transformer_optimization
2025-08-15 17:22:42,663 - INFO - Using device: cuda
2025-08-15 17:22:53,709 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 17:22:53,771 - INFO - --- Starting Fold 1/2 ---
2025-08-15 17:22:53,772 - INFO - Starting training for fold 1/2
2025-08-15 17:22:57,800 - INFO - Fold 1, Epoch 10: Val Acc: 0.53%
2025-08-15 17:22:58,744 - INFO - Fold 1, Epoch 20: Val Acc: 0.38%
2025-08-15 17:22:59,166 - INFO - Fold 1, Epoch 30: Val Acc: 0.50%
2025-08-15 17:22:59,584 - INFO - Fold 1, Epoch 40: Val Acc: 0.50%
2025-08-15 17:23:00,760 - INFO - Fold 1, Epoch 50: Val Acc: 0.53%
2025-08-15 17:23:01,180 - INFO - Fold 1, Epoch 60: Val Acc: 0.44%
2025-08-15 17:23:01,599 - INFO - Fold 1, Epoch 70: Val Acc: 0.59%
2025-08-15 17:23:02,018 - INFO - Fold 1, Epoch 80: Val Acc: 0.47%
2025-08-15 17:23:02,439 - INFO - Fold 1, Epoch 90: Val Acc: 0.47%
2025-08-15 17:23:02,857 - INFO - Fold 1, Epoch 100: Val Acc: 0.41%
2025-08-15 17:23:03,275 - INFO - Fold 1, Epoch 110: Val Acc: 0.38%
2025-08-15 17:23:03,695 - INFO - Fold 1, Epoch 120: Val Acc: 0.53%
2025-08-15 17:23:04,114 - INFO - Fold 1, Epoch 130: Val Acc: 0.47%
2025-08-15 17:23:04,530 - INFO - Fold 1, Epoch 140: Val Acc: 0.59%
2025-08-15 17:23:04,866 - INFO - Early stopping at epoch 148
2025-08-15 17:24:28,682 - INFO - --- Starting Fold 2/2 ---
2025-08-15 17:24:28,685 - INFO - Starting training for fold 2/2
2025-08-15 17:24:29,948 - INFO - Fold 2, Epoch 10: Val Acc: 0.50%
2025-08-15 17:24:30,814 - INFO - Fold 2, Epoch 20: Val Acc: 0.59%
2025-08-15 17:24:31,673 - INFO - Fold 2, Epoch 30: Val Acc: 0.41%
2025-08-15 17:24:32,118 - INFO - Fold 2, Epoch 40: Val Acc: 0.53%
2025-08-15 17:24:32,565 - INFO - Fold 2, Epoch 50: Val Acc: 0.53%
2025-08-15 17:24:33,008 - INFO - Fold 2, Epoch 60: Val Acc: 0.19%
2025-08-15 17:24:33,452 - INFO - Fold 2, Epoch 70: Val Acc: 0.41%
2025-08-15 17:24:33,897 - INFO - Fold 2, Epoch 80: Val Acc: 0.59%
2025-08-15 17:24:34,342 - INFO - Fold 2, Epoch 90: Val Acc: 0.56%
2025-08-15 17:24:34,788 - INFO - Fold 2, Epoch 100: Val Acc: 0.50%
2025-08-15 17:24:35,234 - INFO - Fold 2, Epoch 110: Val Acc: 0.50%
2025-08-15 17:24:35,677 - INFO - Fold 2, Epoch 120: Val Acc: 0.53%
2025-08-15 17:24:35,765 - INFO - Early stopping at epoch 122
2025-08-15 17:26:00,663 - INFO - Final Test Set Evaluation - Loss: 4.3125, Accuracy: 0.4688
2025-08-15 17:26:00,664 - INFO - Cross-validation statistics for transformer: {'train_loss': {'mean': np.float64(4.206089019775391), 'std': np.float64(0.058829307556152344)}, 'train_accuracy': {'mean': np.float64(0.640625), 'std': np.float64(0.015625)}, 'val_loss': {'mean': np.float64(4.198734283447266), 'std': np.float64(0.05449390411376953)}, 'val_accuracy': {'mean': np.float64(0.671875), 'std': np.float64(0.015625)}, 'epoch': {'mean': np.float64(34.0), 'std': np.float64(13.0)}, 'test_loss': 4.31248664855957, 'test_accuracy': np.float64(0.46875)}
[I 2025-08-15 17:26:00,670] Trial 0 finished with value: -0.671875 and parameters: {'learning_rate': 0.0003722967961013729, 'batch_size': 32, 'num_epochs': 521, 'temperature': 0.2488065673452569, 'embedding_dim': 128, 'hidden_dim': 512, 'dropout': 0.3348415628017334, 'num_layers': 1, 'num_heads': 8, 'noise_enabled': False, 'shift_enabled': True, 'scale_enabled': False, 'crop_enabled': True, 'flip_enabled': False, 'permutation_enabled': False, 'crop_size': 0.7447672192151903}. Best is trial 0 with value: -0.671875.
Attempting to load data from: /vol/ecrg-solar/woodj4/fishy-business/data/REIMS.xlsx
Trial 0 finished with value: -0.671875 and parameters: {'learning_rate': 0.0003722967961013729, 'batch_size': 32, 'num_epochs': 521, 'temperature': 0.2488065673452569, 'embedding_dim': 128, 'hidden_dim': 512, 'dropout': 0.3348415628017334, 'num_layers': 1, 'num_heads': 8, 'noise_enabled': False, 'shift_enabled': True, 'scale_enabled': False, 'crop_enabled': True, 'flip_enabled': False, 'permutation_enabled': False, 'crop_size': 0.7447672192151903}. Best is trial 0 with value: -0.671875.
2025-08-15 17:26:00,671 - INFO - Using device: cuda
2025-08-15 17:26:08,329 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 17:26:08,387 - INFO - --- Starting Fold 1/2 ---
2025-08-15 17:26:08,388 - INFO - Starting training for fold 1/2
2025-08-15 17:26:10,015 - INFO - Fold 1, Epoch 10: Val Acc: 0.50%
2025-08-15 17:26:11,028 - INFO - Fold 1, Epoch 20: Val Acc: 0.44%
2025-08-15 17:26:11,579 - INFO - Fold 1, Epoch 30: Val Acc: 0.46%
2025-08-15 17:26:12,135 - INFO - Fold 1, Epoch 40: Val Acc: 0.50%
2025-08-15 17:26:12,696 - INFO - Fold 1, Epoch 50: Val Acc: 0.50%
2025-08-15 17:26:13,254 - INFO - Fold 1, Epoch 60: Val Acc: 0.50%
2025-08-15 17:26:14,352 - INFO - Fold 1, Epoch 70: Val Acc: 0.48%
2025-08-15 17:26:14,953 - INFO - Fold 1, Epoch 80: Val Acc: 0.58%
2025-08-15 17:26:15,560 - INFO - Fold 1, Epoch 90: Val Acc: 0.56%
2025-08-15 17:26:16,165 - INFO - Fold 1, Epoch 100: Val Acc: 0.50%
2025-08-15 17:26:16,769 - INFO - Fold 1, Epoch 110: Val Acc: 0.50%
2025-08-15 17:26:17,368 - INFO - Fold 1, Epoch 120: Val Acc: 0.48%
2025-08-15 17:26:17,964 - INFO - Fold 1, Epoch 130: Val Acc: 0.54%
2025-08-15 17:26:18,566 - INFO - Fold 1, Epoch 140: Val Acc: 0.46%
2025-08-15 17:26:19,655 - INFO - Fold 1, Epoch 150: Val Acc: 0.48%
2025-08-15 17:26:20,265 - INFO - Fold 1, Epoch 160: Val Acc: 0.58%
2025-08-15 17:26:20,859 - INFO - Fold 1, Epoch 170: Val Acc: 0.50%
2025-08-15 17:26:21,448 - INFO - Fold 1, Epoch 180: Val Acc: 0.50%
2025-08-15 17:26:22,041 - INFO - Fold 1, Epoch 190: Val Acc: 0.48%
2025-08-15 17:26:22,640 - INFO - Fold 1, Epoch 200: Val Acc: 0.48%
2025-08-15 17:26:23,238 - INFO - Fold 1, Epoch 210: Val Acc: 0.48%
2025-08-15 17:26:23,837 - INFO - Fold 1, Epoch 220: Val Acc: 0.52%
2025-08-15 17:26:24,430 - INFO - Fold 1, Epoch 230: Val Acc: 0.54%
2025-08-15 17:26:25,016 - INFO - Fold 1, Epoch 240: Val Acc: 0.65%
2025-08-15 17:26:25,193 - INFO - Early stopping at epoch 243
2025-08-15 17:27:55,540 - INFO - --- Starting Fold 2/2 ---
2025-08-15 17:27:55,542 - INFO - Starting training for fold 2/2
2025-08-15 17:27:56,889 - INFO - Fold 2, Epoch 10: Val Acc: 0.54%
2025-08-15 17:27:57,492 - INFO - Fold 2, Epoch 20: Val Acc: 0.56%
2025-08-15 17:27:58,096 - INFO - Fold 2, Epoch 30: Val Acc: 0.46%
2025-08-15 17:27:58,696 - INFO - Fold 2, Epoch 40: Val Acc: 0.50%
2025-08-15 17:27:59,299 - INFO - Fold 2, Epoch 50: Val Acc: 0.50%
2025-08-15 17:27:59,900 - INFO - Fold 2, Epoch 60: Val Acc: 0.42%
2025-08-15 17:28:00,494 - INFO - Fold 2, Epoch 70: Val Acc: 0.50%
2025-08-15 17:28:01,094 - INFO - Fold 2, Epoch 80: Val Acc: 0.46%
2025-08-15 17:28:01,688 - INFO - Fold 2, Epoch 90: Val Acc: 0.52%
2025-08-15 17:28:02,244 - INFO - Fold 2, Epoch 100: Val Acc: 0.42%
2025-08-15 17:28:02,629 - INFO - Early stopping at epoch 107
2025-08-15 17:29:35,892 - INFO - Final Test Set Evaluation - Loss: 3.4958, Accuracy: 0.5000
2025-08-15 17:29:35,892 - INFO - Cross-validation statistics for transformer: {'train_loss': {'mean': np.float64(3.389715870221456), 'std': np.float64(0.006299853324890137)}, 'train_accuracy': {'mean': np.float64(0.5833333333333334), 'std': np.float64(0.0)}, 'val_loss': {'mean': np.float64(3.4788166284561157), 'std': np.float64(0.03880147139231349)}, 'val_accuracy': {'mean': np.float64(0.6354166666666667), 'std': np.float64(0.031250000000000056)}, 'epoch': {'mean': np.float64(74.0), 'std': np.float64(68.0)}, 'test_loss': 3.495802720387777, 'test_accuracy': np.float64(0.5)}
[I 2025-08-15 17:29:35,898] Trial 1 finished with value: -0.6354166666666667 and parameters: {'learning_rate': 0.00011618573867341985, 'batch_size': 16, 'num_epochs': 403, 'temperature': 0.304778138195455, 'embedding_dim': 512, 'hidden_dim': 128, 'dropout': 0.1781476879263663, 'num_layers': 1, 'num_heads': 2, 'noise_enabled': False, 'shift_enabled': True, 'scale_enabled': False, 'crop_enabled': False, 'flip_enabled': False, 'permutation_enabled': False}. Best is trial 0 with value: -0.671875.
Attempting to load data from: /vol/ecrg-solar/woodj4/fishy-business/data/REIMS.xlsx
Trial 1 finished with value: -0.6354166666666667 and parameters: {'learning_rate': 0.00011618573867341985, 'batch_size': 16, 'num_epochs': 403, 'temperature': 0.304778138195455, 'embedding_dim': 512, 'hidden_dim': 128, 'dropout': 0.1781476879263663, 'num_layers': 1, 'num_heads': 2, 'noise_enabled': False, 'shift_enabled': True, 'scale_enabled': False, 'crop_enabled': False, 'flip_enabled': False, 'permutation_enabled': False}. Best is trial 0 with value: -0.671875.
2025-08-15 17:29:35,899 - INFO - Using device: cuda
2025-08-15 17:29:43,277 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 17:29:43,485 - INFO - --- Starting Fold 1/2 ---
2025-08-15 17:29:43,486 - INFO - Starting training for fold 1/2
2025-08-15 17:29:47,106 - INFO - Fold 1, Epoch 10: Val Acc: 0.53%
2025-08-15 17:29:47,684 - INFO - Fold 1, Epoch 20: Val Acc: 0.44%
2025-08-15 17:29:49,817 - INFO - Fold 1, Epoch 30: Val Acc: 0.47%
2025-08-15 17:29:50,429 - INFO - Fold 1, Epoch 40: Val Acc: 0.47%
2025-08-15 17:29:51,044 - INFO - Fold 1, Epoch 50: Val Acc: 0.28%
2025-08-15 17:29:51,658 - INFO - Fold 1, Epoch 60: Val Acc: 0.56%
2025-08-15 17:29:52,276 - INFO - Fold 1, Epoch 70: Val Acc: 0.53%
2025-08-15 17:29:52,886 - INFO - Fold 1, Epoch 80: Val Acc: 0.47%
2025-08-15 17:29:53,483 - INFO - Fold 1, Epoch 90: Val Acc: 0.44%
2025-08-15 17:29:54,065 - INFO - Fold 1, Epoch 100: Val Acc: 0.50%
2025-08-15 17:29:54,646 - INFO - Fold 1, Epoch 110: Val Acc: 0.50%
2025-08-15 17:29:55,228 - INFO - Fold 1, Epoch 120: Val Acc: 0.50%
2025-08-15 17:29:55,575 - INFO - Early stopping at epoch 126
2025-08-15 17:34:22,163 - INFO - --- Starting Fold 2/2 ---
2025-08-15 17:34:22,164 - INFO - Starting training for fold 2/2
2025-08-15 17:34:26,368 - INFO - Fold 2, Epoch 10: Val Acc: 0.47%
2025-08-15 17:34:26,977 - INFO - Fold 2, Epoch 20: Val Acc: 0.50%
2025-08-15 17:34:27,558 - INFO - Fold 2, Epoch 30: Val Acc: 0.44%
2025-08-15 17:34:29,067 - INFO - Fold 2, Epoch 40: Val Acc: 0.66%
2025-08-15 17:34:29,652 - INFO - Fold 2, Epoch 50: Val Acc: 0.53%
2025-08-15 17:34:30,231 - INFO - Fold 2, Epoch 60: Val Acc: 0.56%
2025-08-15 17:34:31,793 - INFO - Fold 2, Epoch 70: Val Acc: 0.50%
2025-08-15 17:34:32,372 - INFO - Fold 2, Epoch 80: Val Acc: 0.56%
2025-08-15 17:34:32,949 - INFO - Fold 2, Epoch 90: Val Acc: 0.56%
2025-08-15 17:34:33,528 - INFO - Fold 2, Epoch 100: Val Acc: 0.41%
2025-08-15 17:34:34,135 - INFO - Fold 2, Epoch 110: Val Acc: 0.56%
2025-08-15 17:34:34,747 - INFO - Fold 2, Epoch 120: Val Acc: 0.50%
2025-08-15 17:34:35,362 - INFO - Fold 2, Epoch 130: Val Acc: 0.47%
2025-08-15 17:34:35,981 - INFO - Fold 2, Epoch 140: Val Acc: 0.47%
2025-08-15 17:34:36,595 - INFO - Fold 2, Epoch 150: Val Acc: 0.44%
2025-08-15 17:34:37,211 - INFO - Fold 2, Epoch 160: Val Acc: 0.44%
2025-08-15 17:34:37,394 - INFO - Early stopping at epoch 163
2025-08-15 17:39:07,799 - INFO - Final Test Set Evaluation - Loss: 5.2371, Accuracy: 0.4688
2025-08-15 17:39:07,800 - INFO - Cross-validation statistics for transformer: {'train_loss': {'mean': np.float64(4.764222621917725), 'std': np.float64(0.2271265983581543)}, 'train_accuracy': {'mean': np.float64(0.609375), 'std': np.float64(0.078125)}, 'val_loss': {'mean': np.float64(4.972897052764893), 'std': np.float64(0.21812772750854492)}, 'val_accuracy': {'mean': np.float64(0.71875), 'std': np.float64(0.0)}, 'epoch': {'mean': np.float64(43.5), 'std': np.float64(18.5)}, 'test_loss': 5.237109661102295, 'test_accuracy': np.float64(0.46875)}
[I 2025-08-15 17:39:07,805] Trial 2 finished with value: -0.71875 and parameters: {'learning_rate': 0.0008985650089177639, 'batch_size': 32, 'num_epochs': 742, 'temperature': 0.05431603520363524, 'embedding_dim': 256, 'hidden_dim': 256, 'dropout': 0.3586381981563861, 'num_layers': 4, 'num_heads': 8, 'noise_enabled': True, 'shift_enabled': True, 'scale_enabled': True, 'crop_enabled': True, 'flip_enabled': True, 'permutation_enabled': True, 'noise_level': 0.17061806921896075, 'crop_size': 0.8737466344859814}. Best is trial 2 with value: -0.71875.
Attempting to load data from: /vol/ecrg-solar/woodj4/fishy-business/data/REIMS.xlsx
Trial 2 finished with value: -0.71875 and parameters: {'learning_rate': 0.0008985650089177639, 'batch_size': 32, 'num_epochs': 742, 'temperature': 0.05431603520363524, 'embedding_dim': 256, 'hidden_dim': 256, 'dropout': 0.3586381981563861, 'num_layers': 4, 'num_heads': 8, 'noise_enabled': True, 'shift_enabled': True, 'scale_enabled': True, 'crop_enabled': True, 'flip_enabled': True, 'permutation_enabled': True, 'noise_level': 0.17061806921896075, 'crop_size': 0.8737466344859814}. Best is trial 2 with value: -0.71875.
2025-08-15 17:39:07,806 - INFO - Using device: cuda
2025-08-15 17:39:15,234 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 17:39:15,339 - INFO - --- Starting Fold 1/2 ---
2025-08-15 17:39:15,340 - INFO - Starting training for fold 1/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 17:39:18,214 - INFO - Fold 1, Epoch 10: Val Acc: 0.53%
2025-08-15 17:39:19,365 - INFO - Fold 1, Epoch 20: Val Acc: 0.59%
2025-08-15 17:39:19,824 - INFO - Fold 1, Epoch 30: Val Acc: 0.50%
2025-08-15 17:39:21,109 - INFO - Fold 1, Epoch 40: Val Acc: 0.53%
2025-08-15 17:39:21,559 - INFO - Fold 1, Epoch 50: Val Acc: 0.50%
2025-08-15 17:39:22,769 - INFO - Fold 1, Epoch 60: Val Acc: 0.72%
2025-08-15 17:39:23,226 - INFO - Fold 1, Epoch 70: Val Acc: 0.47%
2025-08-15 17:39:23,677 - INFO - Fold 1, Epoch 80: Val Acc: 0.47%
2025-08-15 17:39:24,129 - INFO - Fold 1, Epoch 90: Val Acc: 0.44%
2025-08-15 17:39:24,576 - INFO - Fold 1, Epoch 100: Val Acc: 0.50%
2025-08-15 17:39:25,029 - INFO - Fold 1, Epoch 110: Val Acc: 0.59%
2025-08-15 17:39:25,466 - INFO - Fold 1, Epoch 120: Val Acc: 0.47%
2025-08-15 17:39:26,673 - INFO - Fold 1, Epoch 130: Val Acc: 0.53%
2025-08-15 17:39:27,122 - INFO - Fold 1, Epoch 140: Val Acc: 0.62%
2025-08-15 17:39:27,583 - INFO - Fold 1, Epoch 150: Val Acc: 0.56%
2025-08-15 17:39:28,055 - INFO - Fold 1, Epoch 160: Val Acc: 0.50%
2025-08-15 17:39:28,527 - INFO - Fold 1, Epoch 170: Val Acc: 0.53%
2025-08-15 17:39:28,987 - INFO - Fold 1, Epoch 180: Val Acc: 0.69%
2025-08-15 17:39:29,428 - INFO - Fold 1, Epoch 190: Val Acc: 0.62%
2025-08-15 17:39:29,869 - INFO - Fold 1, Epoch 200: Val Acc: 0.56%
2025-08-15 17:39:30,292 - INFO - Fold 1, Epoch 210: Val Acc: 0.56%
2025-08-15 17:39:30,704 - INFO - Fold 1, Epoch 220: Val Acc: 0.69%
2025-08-15 17:39:31,925 - INFO - Fold 1, Epoch 230: Val Acc: 0.47%
2025-08-15 17:39:32,339 - INFO - Fold 1, Epoch 240: Val Acc: 0.56%
2025-08-15 17:39:32,752 - INFO - Fold 1, Epoch 250: Val Acc: 0.59%
2025-08-15 17:39:33,166 - INFO - Fold 1, Epoch 260: Val Acc: 0.69%
2025-08-15 17:39:34,377 - INFO - Fold 1, Epoch 270: Val Acc: 0.78%
2025-08-15 17:39:34,818 - INFO - Fold 1, Epoch 280: Val Acc: 0.53%
2025-08-15 17:39:35,254 - INFO - Fold 1, Epoch 290: Val Acc: 0.62%
2025-08-15 17:39:35,672 - INFO - Fold 1, Epoch 300: Val Acc: 0.75%
2025-08-15 17:39:36,088 - INFO - Fold 1, Epoch 310: Val Acc: 0.66%
2025-08-15 17:39:36,505 - INFO - Fold 1, Epoch 320: Val Acc: 0.53%
2025-08-15 17:39:36,920 - INFO - Fold 1, Epoch 330: Val Acc: 0.53%
2025-08-15 17:39:37,335 - INFO - Fold 1, Epoch 340: Val Acc: 0.53%
2025-08-15 17:39:37,748 - INFO - Fold 1, Epoch 350: Val Acc: 0.59%
2025-08-15 17:39:38,161 - INFO - Fold 1, Epoch 360: Val Acc: 0.47%
2025-08-15 17:39:38,367 - INFO - Early stopping at epoch 365
2025-08-15 17:41:29,228 - INFO - --- Starting Fold 2/2 ---
2025-08-15 17:41:29,230 - INFO - Starting training for fold 2/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 17:41:31,784 - INFO - Fold 2, Epoch 10: Val Acc: 0.59%
2025-08-15 17:41:32,959 - INFO - Fold 2, Epoch 20: Val Acc: 0.50%
2025-08-15 17:41:33,426 - INFO - Fold 2, Epoch 30: Val Acc: 0.47%
2025-08-15 17:41:33,899 - INFO - Fold 2, Epoch 40: Val Acc: 0.53%
2025-08-15 17:41:34,372 - INFO - Fold 2, Epoch 50: Val Acc: 0.50%
2025-08-15 17:41:34,844 - INFO - Fold 2, Epoch 60: Val Acc: 0.66%
2025-08-15 17:41:35,317 - INFO - Fold 2, Epoch 70: Val Acc: 0.44%
2025-08-15 17:41:35,787 - INFO - Fold 2, Epoch 80: Val Acc: 0.38%
2025-08-15 17:41:36,261 - INFO - Fold 2, Epoch 90: Val Acc: 0.53%
2025-08-15 17:41:36,732 - INFO - Fold 2, Epoch 100: Val Acc: 0.59%
2025-08-15 17:41:37,208 - INFO - Fold 2, Epoch 110: Val Acc: 0.53%
2025-08-15 17:41:37,255 - INFO - Early stopping at epoch 111
2025-08-15 17:44:50,451 - INFO - Final Test Set Evaluation - Loss: 4.7649, Accuracy: 0.6562
2025-08-15 17:44:50,452 - INFO - Cross-validation statistics for transformer: {'train_loss': {'mean': np.float64(4.838595151901245), 'std': np.float64(0.3152759075164795)}, 'train_accuracy': {'mean': np.float64(0.59375), 'std': np.float64(0.03125)}, 'val_loss': {'mean': np.float64(4.893273830413818), 'std': np.float64(0.34841489791870117)}, 'val_accuracy': {'mean': np.float64(0.75), 'std': np.float64(0.0625)}, 'epoch': {'mean': np.float64(137.0), 'std': np.float64(127.0)}, 'test_loss': 4.764869689941406, 'test_accuracy': np.float64(0.65625)}
[I 2025-08-15 17:44:50,457] Trial 3 finished with value: -0.75 and parameters: {'learning_rate': 4.0244165295086866e-05, 'batch_size': 32, 'num_epochs': 513, 'temperature': 0.11997188151238775, 'embedding_dim': 256, 'hidden_dim': 256, 'dropout': 0.3680027862885493, 'num_layers': 2, 'num_heads': 8, 'noise_enabled': False, 'shift_enabled': False, 'scale_enabled': True, 'crop_enabled': True, 'flip_enabled': False, 'permutation_enabled': False, 'crop_size': 0.5241498346423141}. Best is trial 3 with value: -0.75.
Attempting to load data from: /vol/ecrg-solar/woodj4/fishy-business/data/REIMS.xlsx
Trial 3 finished with value: -0.75 and parameters: {'learning_rate': 4.0244165295086866e-05, 'batch_size': 32, 'num_epochs': 513, 'temperature': 0.11997188151238775, 'embedding_dim': 256, 'hidden_dim': 256, 'dropout': 0.3680027862885493, 'num_layers': 2, 'num_heads': 8, 'noise_enabled': False, 'shift_enabled': False, 'scale_enabled': True, 'crop_enabled': True, 'flip_enabled': False, 'permutation_enabled': False, 'crop_size': 0.5241498346423141}. Best is trial 3 with value: -0.75.
2025-08-15 17:44:50,458 - INFO - Using device: cuda
2025-08-15 17:44:57,883 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 17:44:58,096 - INFO - --- Starting Fold 1/2 ---
2025-08-15 17:44:58,096 - INFO - Starting training for fold 1/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 17:45:03,750 - INFO - Fold 1, Epoch 10: Val Acc: 0.48%
2025-08-15 17:45:04,803 - INFO - Fold 1, Epoch 20: Val Acc: 0.46%
2025-08-15 17:45:05,856 - INFO - Fold 1, Epoch 30: Val Acc: 0.54%
2025-08-15 17:45:08,462 - INFO - Fold 1, Epoch 40: Val Acc: 0.52%
2025-08-15 17:45:09,580 - INFO - Fold 1, Epoch 50: Val Acc: 0.48%
2025-08-15 17:45:10,700 - INFO - Fold 1, Epoch 60: Val Acc: 0.48%
2025-08-15 17:45:11,825 - INFO - Fold 1, Epoch 70: Val Acc: 0.54%
2025-08-15 17:45:12,944 - INFO - Fold 1, Epoch 80: Val Acc: 0.54%
2025-08-15 17:45:14,061 - INFO - Fold 1, Epoch 90: Val Acc: 0.46%
2025-08-15 17:45:18,272 - INFO - Fold 1, Epoch 100: Val Acc: 0.67%
2025-08-15 17:45:19,325 - INFO - Fold 1, Epoch 110: Val Acc: 0.50%
2025-08-15 17:45:20,376 - INFO - Fold 1, Epoch 120: Val Acc: 0.38%
2025-08-15 17:45:21,426 - INFO - Fold 1, Epoch 130: Val Acc: 0.40%
2025-08-15 17:45:22,476 - INFO - Fold 1, Epoch 140: Val Acc: 0.46%
2025-08-15 17:45:23,520 - INFO - Fold 1, Epoch 150: Val Acc: 0.52%
2025-08-15 17:50:13,205 - INFO - --- Starting Fold 2/2 ---
2025-08-15 17:50:13,207 - INFO - Starting training for fold 2/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 17:50:17,621 - INFO - Fold 2, Epoch 10: Val Acc: 0.50%
2025-08-15 17:50:18,670 - INFO - Fold 2, Epoch 20: Val Acc: 0.44%
2025-08-15 17:50:19,717 - INFO - Fold 2, Epoch 30: Val Acc: 0.52%
2025-08-15 17:50:20,763 - INFO - Fold 2, Epoch 40: Val Acc: 0.58%
2025-08-15 17:50:22,841 - INFO - Fold 2, Epoch 50: Val Acc: 0.52%
2025-08-15 17:50:23,894 - INFO - Fold 2, Epoch 60: Val Acc: 0.44%
2025-08-15 17:50:24,941 - INFO - Fold 2, Epoch 70: Val Acc: 0.48%
2025-08-15 17:50:25,988 - INFO - Fold 2, Epoch 80: Val Acc: 0.50%
2025-08-15 17:50:27,039 - INFO - Fold 2, Epoch 90: Val Acc: 0.35%
2025-08-15 17:50:28,090 - INFO - Fold 2, Epoch 100: Val Acc: 0.54%
2025-08-15 17:50:29,134 - INFO - Fold 2, Epoch 110: Val Acc: 0.50%
2025-08-15 17:50:30,173 - INFO - Fold 2, Epoch 120: Val Acc: 0.54%
2025-08-15 17:50:31,217 - INFO - Fold 2, Epoch 130: Val Acc: 0.48%
2025-08-15 17:50:33,200 - INFO - Fold 2, Epoch 140: Val Acc: 0.44%
2025-08-15 17:50:34,163 - INFO - Fold 2, Epoch 150: Val Acc: 0.58%
2025-08-15 17:55:53,464 - INFO - Final Test Set Evaluation - Loss: 4.3475, Accuracy: 0.5208
2025-08-15 17:55:53,464 - INFO - Cross-validation statistics for transformer: {'train_loss': {'mean': np.float64(4.046600302060445), 'std': np.float64(0.018291831016540527)}, 'train_accuracy': {'mean': np.float64(0.625), 'std': np.float64(0.020833333333333315)}, 'val_loss': {'mean': np.float64(4.038674434026083), 'std': np.float64(0.11634834607442235)}, 'val_accuracy': {'mean': np.float64(0.7083333333333333), 'std': np.float64(0.041666666666666685)}, 'epoch': {'mean': np.float64(114.5), 'std': np.float64(15.5)}, 'test_loss': 4.347523053487142, 'test_accuracy': np.float64(0.5208333333333334)}
[I 2025-08-15 17:55:53,472] Trial 4 finished with value: -0.7083333333333333 and parameters: {'learning_rate': 3.570571929507697e-05, 'batch_size': 16, 'num_epochs': 159, 'temperature': 0.07417964502418527, 'embedding_dim': 128, 'hidden_dim': 512, 'dropout': 0.3741463271898563, 'num_layers': 4, 'num_heads': 2, 'noise_enabled': False, 'shift_enabled': False, 'scale_enabled': False, 'crop_enabled': True, 'flip_enabled': True, 'permutation_enabled': True, 'crop_size': 0.8133651080719093}. Best is trial 3 with value: -0.75.
Attempting to load data from: /vol/ecrg-solar/woodj4/fishy-business/data/REIMS.xlsx
Trial 4 finished with value: -0.7083333333333333 and parameters: {'learning_rate': 3.570571929507697e-05, 'batch_size': 16, 'num_epochs': 159, 'temperature': 0.07417964502418527, 'embedding_dim': 128, 'hidden_dim': 512, 'dropout': 0.3741463271898563, 'num_layers': 4, 'num_heads': 2, 'noise_enabled': False, 'shift_enabled': False, 'scale_enabled': False, 'crop_enabled': True, 'flip_enabled': True, 'permutation_enabled': True, 'crop_size': 0.8133651080719093}. Best is trial 3 with value: -0.75.
2025-08-15 17:55:53,473 - INFO - Using device: cuda
2025-08-15 17:56:00,657 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 17:56:00,869 - INFO - --- Starting Fold 1/2 ---
2025-08-15 17:56:00,870 - INFO - Starting training for fold 1/2
2025-08-15 17:56:06,435 - INFO - Fold 1, Epoch 10: Val Acc: 0.50%
2025-08-15 17:56:07,404 - INFO - Fold 1, Epoch 20: Val Acc: 0.52%
2025-08-15 17:56:08,374 - INFO - Fold 1, Epoch 30: Val Acc: 0.50%
2025-08-15 17:56:09,339 - INFO - Fold 1, Epoch 40: Val Acc: 0.48%
2025-08-15 17:56:11,804 - INFO - Fold 1, Epoch 50: Val Acc: 0.58%
2025-08-15 17:56:12,865 - INFO - Fold 1, Epoch 60: Val Acc: 0.56%
2025-08-15 17:56:13,928 - INFO - Fold 1, Epoch 70: Val Acc: 0.52%
2025-08-15 17:56:16,642 - INFO - Fold 1, Epoch 80: Val Acc: 0.50%
2025-08-15 17:56:17,724 - INFO - Fold 1, Epoch 90: Val Acc: 0.52%
2025-08-15 17:56:18,860 - INFO - Fold 1, Epoch 100: Val Acc: 0.50%
2025-08-15 17:56:19,991 - INFO - Fold 1, Epoch 110: Val Acc: 0.50%
2025-08-15 17:56:21,122 - INFO - Fold 1, Epoch 120: Val Acc: 0.50%
2025-08-15 17:56:22,260 - INFO - Fold 1, Epoch 130: Val Acc: 0.50%
2025-08-15 17:56:23,398 - INFO - Fold 1, Epoch 140: Val Acc: 0.56%
2025-08-15 17:56:26,036 - INFO - Fold 1, Epoch 150: Val Acc: 0.50%
2025-08-15 17:56:28,671 - INFO - Fold 1, Epoch 160: Val Acc: 0.50%
2025-08-15 17:56:29,707 - INFO - Fold 1, Epoch 170: Val Acc: 0.50%
2025-08-15 17:56:30,687 - INFO - Fold 1, Epoch 180: Val Acc: 0.50%
2025-08-15 17:56:31,656 - INFO - Fold 1, Epoch 190: Val Acc: 0.54%
2025-08-15 17:56:32,627 - INFO - Fold 1, Epoch 200: Val Acc: 0.58%
2025-08-15 17:56:33,593 - INFO - Fold 1, Epoch 210: Val Acc: 0.60%
2025-08-15 17:56:34,563 - INFO - Fold 1, Epoch 220: Val Acc: 0.60%
2025-08-15 17:56:35,533 - INFO - Fold 1, Epoch 230: Val Acc: 0.67%
2025-08-15 17:56:36,506 - INFO - Fold 1, Epoch 240: Val Acc: 0.52%
2025-08-15 17:56:37,478 - INFO - Fold 1, Epoch 250: Val Acc: 0.52%
2025-08-15 17:56:37,963 - INFO - Early stopping at epoch 255
2025-08-15 18:01:07,689 - INFO - --- Starting Fold 2/2 ---
2025-08-15 18:01:07,692 - INFO - Starting training for fold 2/2
2025-08-15 18:01:10,317 - INFO - Fold 2, Epoch 10: Val Acc: 0.52%
2025-08-15 18:01:11,458 - INFO - Fold 2, Epoch 20: Val Acc: 0.48%
2025-08-15 18:01:12,599 - INFO - Fold 2, Epoch 30: Val Acc: 0.44%
2025-08-15 18:01:13,737 - INFO - Fold 2, Epoch 40: Val Acc: 0.50%
2025-08-15 18:01:15,915 - INFO - Fold 2, Epoch 50: Val Acc: 0.60%
2025-08-15 18:01:18,160 - INFO - Fold 2, Epoch 60: Val Acc: 0.50%
2025-08-15 18:01:19,293 - INFO - Fold 2, Epoch 70: Val Acc: 0.50%
2025-08-15 18:01:20,427 - INFO - Fold 2, Epoch 80: Val Acc: 0.58%
2025-08-15 18:01:21,571 - INFO - Fold 2, Epoch 90: Val Acc: 0.56%
2025-08-15 18:01:22,713 - INFO - Fold 2, Epoch 100: Val Acc: 0.58%
2025-08-15 18:01:23,859 - INFO - Fold 2, Epoch 110: Val Acc: 0.62%
2025-08-15 18:01:25,032 - INFO - Fold 2, Epoch 120: Val Acc: 0.65%
2025-08-15 18:01:27,238 - INFO - Fold 2, Epoch 130: Val Acc: 0.67%
2025-08-15 18:01:28,385 - INFO - Fold 2, Epoch 140: Val Acc: 0.50%
2025-08-15 18:01:30,601 - INFO - Fold 2, Epoch 150: Val Acc: 0.58%
2025-08-15 18:01:31,740 - INFO - Fold 2, Epoch 160: Val Acc: 0.56%
2025-08-15 18:01:32,880 - INFO - Fold 2, Epoch 170: Val Acc: 0.60%
2025-08-15 18:01:34,022 - INFO - Fold 2, Epoch 180: Val Acc: 0.67%
2025-08-15 18:01:35,160 - INFO - Fold 2, Epoch 190: Val Acc: 0.56%
2025-08-15 18:01:36,301 - INFO - Fold 2, Epoch 200: Val Acc: 0.54%
2025-08-15 18:01:37,442 - INFO - Fold 2, Epoch 210: Val Acc: 0.56%
2025-08-15 18:01:38,576 - INFO - Fold 2, Epoch 220: Val Acc: 0.54%
2025-08-15 18:01:39,711 - INFO - Fold 2, Epoch 230: Val Acc: 0.52%
2025-08-15 18:01:42,159 - INFO - Fold 2, Epoch 240: Val Acc: 0.71%
2025-08-15 18:01:43,303 - INFO - Fold 2, Epoch 250: Val Acc: 0.62%
2025-08-15 18:01:44,414 - INFO - Fold 2, Epoch 260: Val Acc: 0.54%
2025-08-15 18:01:45,384 - INFO - Fold 2, Epoch 270: Val Acc: 0.65%
2025-08-15 18:01:46,353 - INFO - Fold 2, Epoch 280: Val Acc: 0.58%
2025-08-15 18:01:47,324 - INFO - Fold 2, Epoch 290: Val Acc: 0.56%
2025-08-15 18:01:48,293 - INFO - Fold 2, Epoch 300: Val Acc: 0.50%
2025-08-15 18:01:50,256 - INFO - Fold 2, Epoch 310: Val Acc: 0.50%
2025-08-15 18:01:51,224 - INFO - Fold 2, Epoch 320: Val Acc: 0.69%
2025-08-15 18:01:52,252 - INFO - Fold 2, Epoch 330: Val Acc: 0.54%
2025-08-15 18:01:53,311 - INFO - Fold 2, Epoch 340: Val Acc: 0.58%
2025-08-15 18:01:54,373 - INFO - Fold 2, Epoch 350: Val Acc: 0.56%
2025-08-15 18:01:55,431 - INFO - Fold 2, Epoch 360: Val Acc: 0.56%
2025-08-15 18:01:56,491 - INFO - Fold 2, Epoch 370: Val Acc: 0.50%
2025-08-15 18:01:57,552 - INFO - Fold 2, Epoch 380: Val Acc: 0.56%
2025-08-15 18:01:58,609 - INFO - Fold 2, Epoch 390: Val Acc: 0.54%
2025-08-15 18:01:59,671 - INFO - Fold 2, Epoch 400: Val Acc: 0.65%
2025-08-15 18:02:00,097 - INFO - Early stopping at epoch 404
2025-08-15 18:07:05,903 - INFO - Final Test Set Evaluation - Loss: 3.8977, Accuracy: 0.5208
2025-08-15 18:07:05,904 - INFO - Cross-validation statistics for transformer: {'train_loss': {'mean': np.float64(3.167298952738444), 'std': np.float64(0.08497023582458496)}, 'train_accuracy': {'mean': np.float64(0.8020833333333334), 'std': np.float64(0.07291666666666663)}, 'val_loss': {'mean': np.float64(3.5119454463322954), 'std': np.float64(0.032090624173482185)}, 'val_accuracy': {'mean': np.float64(0.75), 'std': np.float64(0.02083333333333326)}, 'epoch': {'mean': np.float64(228.5), 'std': np.float64(74.5)}, 'test_loss': 3.8977365493774414, 'test_accuracy': np.float64(0.5208333333333333)}
[I 2025-08-15 18:07:05,911] Trial 5 finished with value: -0.75 and parameters: {'learning_rate': 0.00025161738635136555, 'batch_size': 16, 'num_epochs': 474, 'temperature': 0.1834774691995165, 'embedding_dim': 512, 'hidden_dim': 256, 'dropout': 0.3736004571040895, 'num_layers': 4, 'num_heads': 8, 'noise_enabled': True, 'shift_enabled': False, 'scale_enabled': True, 'crop_enabled': True, 'flip_enabled': True, 'permutation_enabled': False, 'noise_level': 0.06439662808036516, 'crop_size': 0.7074270451282055}. Best is trial 3 with value: -0.75.
Attempting to load data from: /vol/ecrg-solar/woodj4/fishy-business/data/REIMS.xlsx
Trial 5 finished with value: -0.75 and parameters: {'learning_rate': 0.00025161738635136555, 'batch_size': 16, 'num_epochs': 474, 'temperature': 0.1834774691995165, 'embedding_dim': 512, 'hidden_dim': 256, 'dropout': 0.3736004571040895, 'num_layers': 4, 'num_heads': 8, 'noise_enabled': True, 'shift_enabled': False, 'scale_enabled': True, 'crop_enabled': True, 'flip_enabled': True, 'permutation_enabled': False, 'noise_level': 0.06439662808036516, 'crop_size': 0.7074270451282055}. Best is trial 3 with value: -0.75.
2025-08-15 18:07:05,913 - INFO - Using device: cuda
2025-08-15 18:07:13,674 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 18:07:13,783 - INFO - --- Starting Fold 1/2 ---
2025-08-15 18:07:13,784 - INFO - Starting training for fold 1/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 18:07:16,647 - INFO - Fold 1, Epoch 10: Val Acc: 0.62%
2025-08-15 18:07:20,243 - INFO - Fold 1, Epoch 20: Val Acc: 0.84%
2025-08-15 18:07:20,677 - INFO - Fold 1, Epoch 30: Val Acc: 0.78%
2025-08-15 18:07:21,844 - INFO - Fold 1, Epoch 40: Val Acc: 0.78%
2025-08-15 18:07:22,269 - INFO - Fold 1, Epoch 50: Val Acc: 0.72%
2025-08-15 18:07:22,693 - INFO - Fold 1, Epoch 60: Val Acc: 0.66%
2025-08-15 18:07:23,119 - INFO - Fold 1, Epoch 70: Val Acc: 0.72%
2025-08-15 18:07:23,541 - INFO - Fold 1, Epoch 80: Val Acc: 0.66%
2025-08-15 18:07:23,966 - INFO - Fold 1, Epoch 90: Val Acc: 0.62%
2025-08-15 18:07:24,395 - INFO - Fold 1, Epoch 100: Val Acc: 0.62%
2025-08-15 18:07:24,821 - INFO - Fold 1, Epoch 110: Val Acc: 0.69%
2025-08-15 18:07:25,246 - INFO - Fold 1, Epoch 120: Val Acc: 0.66%
2025-08-15 18:07:25,669 - INFO - Fold 1, Epoch 130: Val Acc: 0.69%
2025-08-15 18:07:25,967 - INFO - Early stopping at epoch 137
2025-08-15 18:10:12,027 - INFO - --- Starting Fold 2/2 ---
2025-08-15 18:10:12,030 - INFO - Starting training for fold 2/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 18:10:14,630 - INFO - Fold 2, Epoch 10: Val Acc: 0.62%
2025-08-15 18:10:15,692 - INFO - Fold 2, Epoch 20: Val Acc: 0.75%
2025-08-15 18:10:17,447 - INFO - Fold 2, Epoch 30: Val Acc: 0.75%
2025-08-15 18:10:18,520 - INFO - Fold 2, Epoch 40: Val Acc: 0.81%
2025-08-15 18:10:18,973 - INFO - Fold 2, Epoch 50: Val Acc: 0.75%
2025-08-15 18:10:19,428 - INFO - Fold 2, Epoch 60: Val Acc: 0.66%
2025-08-15 18:10:19,878 - INFO - Fold 2, Epoch 70: Val Acc: 0.56%
2025-08-15 18:10:20,331 - INFO - Fold 2, Epoch 80: Val Acc: 0.62%
2025-08-15 18:10:20,783 - INFO - Fold 2, Epoch 90: Val Acc: 0.69%
2025-08-15 18:10:21,235 - INFO - Fold 2, Epoch 100: Val Acc: 0.50%
2025-08-15 18:10:21,689 - INFO - Fold 2, Epoch 110: Val Acc: 0.62%
2025-08-15 18:10:22,142 - INFO - Fold 2, Epoch 120: Val Acc: 0.56%
2025-08-15 18:10:22,595 - INFO - Fold 2, Epoch 130: Val Acc: 0.59%
2025-08-15 18:10:22,732 - INFO - Early stopping at epoch 133
2025-08-15 18:13:04,742 - INFO - Final Test Set Evaluation - Loss: 7.5497, Accuracy: 0.6250
2025-08-15 18:13:04,742 - INFO - Cross-validation statistics for transformer: {'train_loss': {'mean': np.float64(5.968161106109619), 'std': np.float64(0.07116365432739258)}, 'train_accuracy': {'mean': np.float64(0.9375), 'std': np.float64(0.03125)}, 'val_loss': {'mean': np.float64(7.640406847000122), 'std': np.float64(0.1971595287322998)}, 'val_accuracy': {'mean': np.float64(0.921875), 'std': np.float64(0.015625)}, 'epoch': {'mean': np.float64(34.0), 'std': np.float64(2.0)}, 'test_loss': 7.549686431884766, 'test_accuracy': np.float64(0.625)}
[I 2025-08-15 18:13:04,748] Trial 6 finished with value: -0.921875 and parameters: {'learning_rate': 0.00038179177087207297, 'batch_size': 32, 'num_epochs': 799, 'temperature': 0.06965394490874383, 'embedding_dim': 256, 'hidden_dim': 128, 'dropout': 0.17495858526947955, 'num_layers': 2, 'num_heads': 4, 'noise_enabled': False, 'shift_enabled': False, 'scale_enabled': True, 'crop_enabled': False, 'flip_enabled': False, 'permutation_enabled': False}. Best is trial 6 with value: -0.921875.
Attempting to load data from: /vol/ecrg-solar/woodj4/fishy-business/data/REIMS.xlsx
Trial 6 finished with value: -0.921875 and parameters: {'learning_rate': 0.00038179177087207297, 'batch_size': 32, 'num_epochs': 799, 'temperature': 0.06965394490874383, 'embedding_dim': 256, 'hidden_dim': 128, 'dropout': 0.17495858526947955, 'num_layers': 2, 'num_heads': 4, 'noise_enabled': False, 'shift_enabled': False, 'scale_enabled': True, 'crop_enabled': False, 'flip_enabled': False, 'permutation_enabled': False}. Best is trial 6 with value: -0.921875.
2025-08-15 18:13:04,749 - INFO - Using device: cuda
2025-08-15 18:13:11,768 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 18:13:11,876 - INFO - --- Starting Fold 1/2 ---
2025-08-15 18:13:11,877 - INFO - Starting training for fold 1/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 18:13:15,753 - INFO - Fold 1, Epoch 10: Val Acc: 0.50%
2025-08-15 18:13:16,573 - INFO - Fold 1, Epoch 20: Val Acc: 0.56%
2025-08-15 18:13:17,396 - INFO - Fold 1, Epoch 30: Val Acc: 0.46%
2025-08-15 18:13:18,218 - INFO - Fold 1, Epoch 40: Val Acc: 0.56%
2025-08-15 18:13:19,035 - INFO - Fold 1, Epoch 50: Val Acc: 0.46%
2025-08-15 18:13:19,855 - INFO - Fold 1, Epoch 60: Val Acc: 0.48%
2025-08-15 18:13:20,682 - INFO - Fold 1, Epoch 70: Val Acc: 0.56%
2025-08-15 18:13:21,496 - INFO - Fold 1, Epoch 80: Val Acc: 0.48%
2025-08-15 18:13:22,308 - INFO - Fold 1, Epoch 90: Val Acc: 0.44%
2025-08-15 18:13:23,125 - INFO - Fold 1, Epoch 100: Val Acc: 0.54%
2025-08-15 18:13:23,855 - INFO - Early stopping at epoch 109
2025-08-15 18:15:38,616 - INFO - --- Starting Fold 2/2 ---
2025-08-15 18:15:38,618 - INFO - Starting training for fold 2/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 18:15:40,891 - INFO - Fold 2, Epoch 10: Val Acc: 0.46%
2025-08-15 18:15:43,519 - INFO - Fold 2, Epoch 20: Val Acc: 0.50%
2025-08-15 18:15:44,876 - INFO - Fold 2, Epoch 30: Val Acc: 0.52%
2025-08-15 18:15:45,610 - INFO - Fold 2, Epoch 40: Val Acc: 0.50%
2025-08-15 18:15:46,341 - INFO - Fold 2, Epoch 50: Val Acc: 0.48%
2025-08-15 18:15:47,072 - INFO - Fold 2, Epoch 60: Val Acc: 0.46%
2025-08-15 18:15:47,802 - INFO - Fold 2, Epoch 70: Val Acc: 0.50%
2025-08-15 18:15:49,108 - INFO - Fold 2, Epoch 80: Val Acc: 0.54%
2025-08-15 18:15:49,845 - INFO - Fold 2, Epoch 90: Val Acc: 0.54%
2025-08-15 18:15:50,582 - INFO - Fold 2, Epoch 100: Val Acc: 0.48%
2025-08-15 18:15:51,318 - INFO - Fold 2, Epoch 110: Val Acc: 0.50%
2025-08-15 18:15:52,054 - INFO - Fold 2, Epoch 120: Val Acc: 0.48%
2025-08-15 18:15:52,788 - INFO - Fold 2, Epoch 130: Val Acc: 0.56%
2025-08-15 18:15:53,521 - INFO - Fold 2, Epoch 140: Val Acc: 0.48%
2025-08-15 18:15:54,256 - INFO - Fold 2, Epoch 150: Val Acc: 0.48%
2025-08-15 18:15:54,992 - INFO - Fold 2, Epoch 160: Val Acc: 0.46%
2025-08-15 18:15:55,723 - INFO - Fold 2, Epoch 170: Val Acc: 0.42%
2025-08-15 18:15:55,943 - INFO - Early stopping at epoch 173
2025-08-15 18:18:35,340 - INFO - Final Test Set Evaluation - Loss: 4.4290, Accuracy: 0.5000
2025-08-15 18:18:35,341 - INFO - Cross-validation statistics for transformer: {'train_loss': {'mean': np.float64(4.266239404678345), 'std': np.float64(0.2731330394744873)}, 'train_accuracy': {'mean': np.float64(0.5729166666666667), 'std': np.float64(0.010416666666666685)}, 'val_loss': {'mean': np.float64(4.381530602773031), 'std': np.float64(0.07304795583089208)}, 'val_accuracy': {'mean': np.float64(0.6666666666666667), 'std': np.float64(0.020833333333333315)}, 'epoch': {'mean': np.float64(40.0), 'std': np.float64(32.0)}, 'test_loss': 4.4289852778116865, 'test_accuracy': np.float64(0.5)}
[I 2025-08-15 18:18:35,346] Trial 7 finished with value: -0.6666666666666667 and parameters: {'learning_rate': 2.8307779601198263e-05, 'batch_size': 16, 'num_epochs': 219, 'temperature': 0.05739560733303127, 'embedding_dim': 128, 'hidden_dim': 256, 'dropout': 0.49360265546168647, 'num_layers': 2, 'num_heads': 2, 'noise_enabled': False, 'shift_enabled': True, 'scale_enabled': True, 'crop_enabled': True, 'flip_enabled': False, 'permutation_enabled': True, 'crop_size': 0.8111986847954722}. Best is trial 6 with value: -0.921875.
Attempting to load data from: /vol/ecrg-solar/woodj4/fishy-business/data/REIMS.xlsx
Trial 7 finished with value: -0.6666666666666667 and parameters: {'learning_rate': 2.8307779601198263e-05, 'batch_size': 16, 'num_epochs': 219, 'temperature': 0.05739560733303127, 'embedding_dim': 128, 'hidden_dim': 256, 'dropout': 0.49360265546168647, 'num_layers': 2, 'num_heads': 2, 'noise_enabled': False, 'shift_enabled': True, 'scale_enabled': True, 'crop_enabled': True, 'flip_enabled': False, 'permutation_enabled': True, 'crop_size': 0.8111986847954722}. Best is trial 6 with value: -0.921875.
2025-08-15 18:18:35,347 - INFO - Using device: cuda
2025-08-15 18:18:42,780 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 18:18:42,839 - INFO - --- Starting Fold 1/2 ---
2025-08-15 18:18:42,840 - INFO - Starting training for fold 1/2
2025-08-15 18:18:45,396 - INFO - Fold 1, Epoch 10: Val Acc: 0.50%
2025-08-15 18:18:46,545 - INFO - Fold 1, Epoch 20: Val Acc: 0.48%
2025-08-15 18:18:47,231 - INFO - Fold 1, Epoch 30: Val Acc: 0.56%
2025-08-15 18:18:47,909 - INFO - Fold 1, Epoch 40: Val Acc: 0.46%
2025-08-15 18:18:48,543 - INFO - Fold 1, Epoch 50: Val Acc: 0.50%
2025-08-15 18:18:49,174 - INFO - Fold 1, Epoch 60: Val Acc: 0.52%
2025-08-15 18:18:49,802 - INFO - Fold 1, Epoch 70: Val Acc: 0.50%
2025-08-15 18:18:50,433 - INFO - Fold 1, Epoch 80: Val Acc: 0.48%
2025-08-15 18:18:51,519 - INFO - Fold 1, Epoch 90: Val Acc: 0.52%
2025-08-15 18:18:52,206 - INFO - Fold 1, Epoch 100: Val Acc: 0.52%
2025-08-15 18:18:53,424 - INFO - Fold 1, Epoch 110: Val Acc: 0.48%
2025-08-15 18:18:54,099 - INFO - Fold 1, Epoch 120: Val Acc: 0.42%
2025-08-15 18:18:54,782 - INFO - Fold 1, Epoch 130: Val Acc: 0.52%
2025-08-15 18:18:55,467 - INFO - Fold 1, Epoch 140: Val Acc: 0.44%
2025-08-15 18:18:56,150 - INFO - Fold 1, Epoch 150: Val Acc: 0.56%
2025-08-15 18:18:56,851 - INFO - Fold 1, Epoch 160: Val Acc: 0.50%
2025-08-15 18:18:57,534 - INFO - Fold 1, Epoch 170: Val Acc: 0.46%
2025-08-15 18:18:58,212 - INFO - Fold 1, Epoch 180: Val Acc: 0.54%
2025-08-15 18:18:58,885 - INFO - Fold 1, Epoch 190: Val Acc: 0.48%
2025-08-15 18:18:59,554 - INFO - Fold 1, Epoch 200: Val Acc: 0.50%
2025-08-15 18:19:00,089 - INFO - Early stopping at epoch 208
2025-08-15 18:20:33,554 - INFO - --- Starting Fold 2/2 ---
2025-08-15 18:20:33,556 - INFO - Starting training for fold 2/2
2025-08-15 18:20:35,585 - INFO - Fold 2, Epoch 10: Val Acc: 0.44%
2025-08-15 18:20:36,727 - INFO - Fold 2, Epoch 20: Val Acc: 0.52%
2025-08-15 18:20:37,492 - INFO - Fold 2, Epoch 30: Val Acc: 0.48%
2025-08-15 18:20:38,244 - INFO - Fold 2, Epoch 40: Val Acc: 0.54%
2025-08-15 18:20:39,290 - INFO - Fold 2, Epoch 50: Val Acc: 0.52%
2025-08-15 18:20:39,974 - INFO - Fold 2, Epoch 60: Val Acc: 0.50%
2025-08-15 18:20:40,669 - INFO - Fold 2, Epoch 70: Val Acc: 0.56%
2025-08-15 18:20:41,368 - INFO - Fold 2, Epoch 80: Val Acc: 0.50%
2025-08-15 18:20:42,059 - INFO - Fold 2, Epoch 90: Val Acc: 0.48%
2025-08-15 18:20:42,740 - INFO - Fold 2, Epoch 100: Val Acc: 0.48%
2025-08-15 18:20:43,423 - INFO - Fold 2, Epoch 110: Val Acc: 0.50%
2025-08-15 18:20:44,109 - INFO - Fold 2, Epoch 120: Val Acc: 0.54%
2025-08-15 18:20:44,801 - INFO - Fold 2, Epoch 130: Val Acc: 0.46%
2025-08-15 18:20:45,489 - INFO - Fold 2, Epoch 140: Val Acc: 0.46%
2025-08-15 18:20:46,106 - INFO - Early stopping at epoch 149
2025-08-15 18:22:08,287 - INFO - Final Test Set Evaluation - Loss: 3.6346, Accuracy: 0.4792
2025-08-15 18:22:08,287 - INFO - Cross-validation statistics for transformer: {'train_loss': {'mean': np.float64(3.3709179560343423), 'std': np.float64(0.05984640121459961)}, 'train_accuracy': {'mean': np.float64(0.6354166666666666), 'std': np.float64(0.01041666666666663)}, 'val_loss': {'mean': np.float64(3.5217047532399492), 'std': np.float64(0.016532977422078377)}, 'val_accuracy': {'mean': np.float64(0.625), 'std': np.float64(0.020833333333333315)}, 'epoch': {'mean': np.float64(77.5), 'std': np.float64(29.5)}, 'test_loss': 3.6345505714416504, 'test_accuracy': np.float64(0.4791666666666667)}
[I 2025-08-15 18:22:08,293] Trial 8 finished with value: -0.625 and parameters: {'learning_rate': 6.348164248761896e-05, 'batch_size': 16, 'num_epochs': 677, 'temperature': 0.1319741451658171, 'embedding_dim': 256, 'hidden_dim': 512, 'dropout': 0.1807702092611037, 'num_layers': 1, 'num_heads': 8, 'noise_enabled': False, 'shift_enabled': True, 'scale_enabled': True, 'crop_enabled': True, 'flip_enabled': False, 'permutation_enabled': True, 'crop_size': 0.7033859682630722}. Best is trial 6 with value: -0.921875.
Attempting to load data from: /vol/ecrg-solar/woodj4/fishy-business/data/REIMS.xlsx
Trial 8 finished with value: -0.625 and parameters: {'learning_rate': 6.348164248761896e-05, 'batch_size': 16, 'num_epochs': 677, 'temperature': 0.1319741451658171, 'embedding_dim': 256, 'hidden_dim': 512, 'dropout': 0.1807702092611037, 'num_layers': 1, 'num_heads': 8, 'noise_enabled': False, 'shift_enabled': True, 'scale_enabled': True, 'crop_enabled': True, 'flip_enabled': False, 'permutation_enabled': True, 'crop_size': 0.7033859682630722}. Best is trial 6 with value: -0.921875.
2025-08-15 18:22:08,294 - INFO - Using device: cuda
2025-08-15 18:22:15,498 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 18:22:15,551 - INFO - --- Starting Fold 1/2 ---
2025-08-15 18:22:15,552 - INFO - Starting training for fold 1/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 18:22:17,484 - INFO - Fold 1, Epoch 10: Val Acc: 0.54%
2025-08-15 18:22:18,578 - INFO - Fold 1, Epoch 20: Val Acc: 0.42%
2025-08-15 18:22:19,191 - INFO - Fold 1, Epoch 30: Val Acc: 0.50%
2025-08-15 18:22:20,294 - INFO - Fold 1, Epoch 40: Val Acc: 0.50%
2025-08-15 18:22:20,915 - INFO - Fold 1, Epoch 50: Val Acc: 0.40%
2025-08-15 18:22:21,525 - INFO - Fold 1, Epoch 60: Val Acc: 0.40%
2025-08-15 18:22:22,130 - INFO - Fold 1, Epoch 70: Val Acc: 0.50%
2025-08-15 18:22:22,743 - INFO - Fold 1, Epoch 80: Val Acc: 0.46%
2025-08-15 18:22:23,364 - INFO - Fold 1, Epoch 90: Val Acc: 0.56%
2025-08-15 18:22:24,466 - INFO - Fold 1, Epoch 100: Val Acc: 0.46%
2025-08-15 18:22:25,088 - INFO - Fold 1, Epoch 110: Val Acc: 0.58%
2025-08-15 18:22:25,712 - INFO - Fold 1, Epoch 120: Val Acc: 0.50%
2025-08-15 18:22:26,330 - INFO - Fold 1, Epoch 130: Val Acc: 0.40%
2025-08-15 18:22:27,412 - INFO - Fold 1, Epoch 140: Val Acc: 0.42%
2025-08-15 18:22:28,031 - INFO - Fold 1, Epoch 150: Val Acc: 0.52%
2025-08-15 18:22:28,661 - INFO - Fold 1, Epoch 160: Val Acc: 0.48%
2025-08-15 18:22:29,280 - INFO - Fold 1, Epoch 170: Val Acc: 0.52%
2025-08-15 18:22:29,897 - INFO - Fold 1, Epoch 180: Val Acc: 0.52%
2025-08-15 18:22:30,512 - INFO - Fold 1, Epoch 190: Val Acc: 0.56%
2025-08-15 18:22:31,129 - INFO - Fold 1, Epoch 200: Val Acc: 0.46%
2025-08-15 18:22:31,758 - INFO - Fold 1, Epoch 210: Val Acc: 0.38%
2025-08-15 18:22:32,382 - INFO - Fold 1, Epoch 220: Val Acc: 0.50%
2025-08-15 18:22:32,997 - INFO - Fold 1, Epoch 230: Val Acc: 0.65%
2025-08-15 18:22:33,119 - INFO - Early stopping at epoch 232
2025-08-15 18:24:27,153 - INFO - --- Starting Fold 2/2 ---
2025-08-15 18:24:27,155 - INFO - Starting training for fold 2/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 18:24:28,991 - INFO - Fold 2, Epoch 10: Val Acc: 0.46%
2025-08-15 18:24:30,403 - INFO - Fold 2, Epoch 20: Val Acc: 0.46%
2025-08-15 18:24:31,101 - INFO - Fold 2, Epoch 30: Val Acc: 0.52%
2025-08-15 18:24:31,809 - INFO - Fold 2, Epoch 40: Val Acc: 0.50%
2025-08-15 18:24:32,514 - INFO - Fold 2, Epoch 50: Val Acc: 0.52%
2025-08-15 18:24:33,221 - INFO - Fold 2, Epoch 60: Val Acc: 0.48%
2025-08-15 18:24:33,935 - INFO - Fold 2, Epoch 70: Val Acc: 0.50%
2025-08-15 18:24:34,646 - INFO - Fold 2, Epoch 80: Val Acc: 0.52%
2025-08-15 18:24:35,725 - INFO - Fold 2, Epoch 90: Val Acc: 0.73%
2025-08-15 18:24:36,433 - INFO - Fold 2, Epoch 100: Val Acc: 0.50%
2025-08-15 18:24:37,142 - INFO - Fold 2, Epoch 110: Val Acc: 0.60%
2025-08-15 18:24:37,856 - INFO - Fold 2, Epoch 120: Val Acc: 0.48%
2025-08-15 18:24:38,561 - INFO - Fold 2, Epoch 130: Val Acc: 0.46%
2025-08-15 18:24:39,278 - INFO - Fold 2, Epoch 140: Val Acc: 0.54%
2025-08-15 18:24:39,983 - INFO - Fold 2, Epoch 150: Val Acc: 0.54%
2025-08-15 18:24:40,684 - INFO - Fold 2, Epoch 160: Val Acc: 0.40%
2025-08-15 18:24:41,398 - INFO - Fold 2, Epoch 170: Val Acc: 0.48%
2025-08-15 18:24:42,111 - INFO - Fold 2, Epoch 180: Val Acc: 0.50%
2025-08-15 18:24:42,824 - INFO - Early stopping at epoch 190
2025-08-15 18:26:16,692 - INFO - Final Test Set Evaluation - Loss: 4.5625, Accuracy: 0.5208
2025-08-15 18:26:16,692 - INFO - Cross-validation statistics for transformer: {'train_loss': {'mean': np.float64(4.334776083628336), 'std': np.float64(0.40597407023111964)}, 'train_accuracy': {'mean': np.float64(0.59375), 'std': np.float64(0.052083333333333315)}, 'val_loss': {'mean': np.float64(4.640149354934692), 'std': np.float64(0.06114983558654785)}, 'val_accuracy': {'mean': np.float64(0.6979166666666667), 'std': np.float64(0.03125)}, 'epoch': {'mean': np.float64(110.0), 'std': np.float64(21.0)}, 'test_loss': 4.56246550877889, 'test_accuracy': np.float64(0.5208333333333334)}
[I 2025-08-15 18:26:16,700] Trial 9 finished with value: -0.6979166666666667 and parameters: {'learning_rate': 0.00017544604754928794, 'batch_size': 16, 'num_epochs': 655, 'temperature': 0.05693779263221817, 'embedding_dim': 128, 'hidden_dim': 128, 'dropout': 0.4265634623558504, 'num_layers': 1, 'num_heads': 8, 'noise_enabled': False, 'shift_enabled': False, 'scale_enabled': False, 'crop_enabled': False, 'flip_enabled': True, 'permutation_enabled': True}. Best is trial 6 with value: -0.921875.
Attempting to load data from: /vol/ecrg-solar/woodj4/fishy-business/data/REIMS.xlsx
Trial 9 finished with value: -0.6979166666666667 and parameters: {'learning_rate': 0.00017544604754928794, 'batch_size': 16, 'num_epochs': 655, 'temperature': 0.05693779263221817, 'embedding_dim': 128, 'hidden_dim': 128, 'dropout': 0.4265634623558504, 'num_layers': 1, 'num_heads': 8, 'noise_enabled': False, 'shift_enabled': False, 'scale_enabled': False, 'crop_enabled': False, 'flip_enabled': True, 'permutation_enabled': True}. Best is trial 6 with value: -0.921875.
2025-08-15 18:26:16,724 - INFO - Using device: cuda
2025-08-15 18:26:24,420 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 18:26:24,733 - INFO - --- Starting Fold 1/2 ---
2025-08-15 18:26:24,734 - INFO - Starting training for fold 1/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 18:26:34,451 - INFO - Fold 1, Epoch 10: Val Acc: 0.59%
2025-08-15 18:26:41,678 - INFO - Fold 1, Epoch 20: Val Acc: 0.69%
2025-08-15 18:26:42,294 - INFO - Fold 1, Epoch 30: Val Acc: 0.75%
2025-08-15 18:26:42,914 - INFO - Fold 1, Epoch 40: Val Acc: 0.59%
2025-08-15 18:26:43,531 - INFO - Fold 1, Epoch 50: Val Acc: 0.59%
2025-08-15 18:26:44,152 - INFO - Fold 1, Epoch 60: Val Acc: 0.62%
2025-08-15 18:26:44,773 - INFO - Fold 1, Epoch 70: Val Acc: 0.72%
2025-08-15 18:26:45,393 - INFO - Fold 1, Epoch 80: Val Acc: 0.75%
2025-08-15 18:26:46,014 - INFO - Fold 1, Epoch 90: Val Acc: 0.66%
2025-08-15 18:26:46,636 - INFO - Fold 1, Epoch 100: Val Acc: 0.66%
2025-08-15 18:26:47,258 - INFO - Fold 1, Epoch 110: Val Acc: 0.66%
2025-08-15 18:26:47,445 - INFO - Early stopping at epoch 113
2025-08-15 18:36:06,206 - INFO - --- Starting Fold 2/2 ---
2025-08-15 18:36:06,207 - INFO - Starting training for fold 2/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 18:36:12,102 - INFO - Fold 2, Epoch 10: Val Acc: 0.56%
2025-08-15 18:36:15,660 - INFO - Fold 2, Epoch 20: Val Acc: 0.59%
2025-08-15 18:36:16,289 - INFO - Fold 2, Epoch 30: Val Acc: 0.75%
2025-08-15 18:36:16,916 - INFO - Fold 2, Epoch 40: Val Acc: 0.56%
2025-08-15 18:36:17,542 - INFO - Fold 2, Epoch 50: Val Acc: 0.62%
2025-08-15 18:36:18,169 - INFO - Fold 2, Epoch 60: Val Acc: 0.66%
2025-08-15 18:36:18,793 - INFO - Fold 2, Epoch 70: Val Acc: 0.56%
2025-08-15 18:36:19,417 - INFO - Fold 2, Epoch 80: Val Acc: 0.56%
2025-08-15 18:36:20,045 - INFO - Fold 2, Epoch 90: Val Acc: 0.56%
2025-08-15 18:36:20,672 - INFO - Fold 2, Epoch 100: Val Acc: 0.72%
2025-08-15 18:36:21,297 - INFO - Fold 2, Epoch 110: Val Acc: 0.59%
2025-08-15 18:36:21,483 - INFO - Early stopping at epoch 113
2025-08-15 18:45:32,647 - INFO - Final Test Set Evaluation - Loss: 5.7227, Accuracy: 0.6250
2025-08-15 18:45:32,647 - INFO - Cross-validation statistics for transformer: {'train_loss': {'mean': np.float64(5.141571521759033), 'std': np.float64(0.2173295021057129)}, 'train_accuracy': {'mean': np.float64(0.9375), 'std': np.float64(0.03125)}, 'val_loss': {'mean': np.float64(5.407015800476074), 'std': np.float64(0.04766559600830078)}, 'val_accuracy': {'mean': np.float64(0.859375), 'std': np.float64(0.015625)}, 'epoch': {'mean': np.float64(12.0), 'std': np.float64(0.0)}, 'test_loss': 5.722691535949707, 'test_accuracy': np.float64(0.625)}
[I 2025-08-15 18:45:32,655] Trial 10 finished with value: -0.859375 and parameters: {'learning_rate': 0.0009752288725176678, 'batch_size': 32, 'num_epochs': 993, 'temperature': 0.09027935951906466, 'embedding_dim': 256, 'hidden_dim': 128, 'dropout': 0.10813183982446002, 'num_layers': 6, 'num_heads': 4, 'noise_enabled': True, 'shift_enabled': False, 'scale_enabled': True, 'crop_enabled': False, 'flip_enabled': False, 'permutation_enabled': False, 'noise_level': 0.012546353364345023}. Best is trial 6 with value: -0.921875.
Attempting to load data from: /vol/ecrg-solar/woodj4/fishy-business/data/REIMS.xlsx
Trial 10 finished with value: -0.859375 and parameters: {'learning_rate': 0.0009752288725176678, 'batch_size': 32, 'num_epochs': 993, 'temperature': 0.09027935951906466, 'embedding_dim': 256, 'hidden_dim': 128, 'dropout': 0.10813183982446002, 'num_layers': 6, 'num_heads': 4, 'noise_enabled': True, 'shift_enabled': False, 'scale_enabled': True, 'crop_enabled': False, 'flip_enabled': False, 'permutation_enabled': False, 'noise_level': 0.012546353364345023}. Best is trial 6 with value: -0.921875.
2025-08-15 18:45:32,672 - INFO - Using device: cuda
2025-08-15 18:45:40,067 - INFO - Data split: 48 train/val samples, 24 test samples.
2025-08-15 18:45:40,383 - INFO - --- Starting Fold 1/2 ---
2025-08-15 18:45:40,383 - INFO - Starting training for fold 1/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 18:45:47,454 - INFO - Fold 1, Epoch 10: Val Acc: 0.72%
2025-08-15 18:45:50,145 - INFO - Fold 1, Epoch 20: Val Acc: 0.72%
2025-08-15 18:45:50,758 - INFO - Fold 1, Epoch 30: Val Acc: 0.66%
2025-08-15 18:45:51,371 - INFO - Fold 1, Epoch 40: Val Acc: 0.69%
2025-08-15 18:45:51,983 - INFO - Fold 1, Epoch 50: Val Acc: 0.66%
2025-08-15 18:45:52,598 - INFO - Fold 1, Epoch 60: Val Acc: 0.62%
2025-08-15 18:45:53,196 - INFO - Fold 1, Epoch 70: Val Acc: 0.72%
2025-08-15 18:45:53,786 - INFO - Fold 1, Epoch 80: Val Acc: 0.69%
2025-08-15 18:45:54,374 - INFO - Fold 1, Epoch 90: Val Acc: 0.59%
2025-08-15 18:45:54,962 - INFO - Fold 1, Epoch 100: Val Acc: 0.69%
2025-08-15 18:45:55,550 - INFO - Fold 1, Epoch 110: Val Acc: 0.59%
2025-08-15 18:45:55,667 - INFO - Early stopping at epoch 112
2025-08-15 18:55:20,002 - INFO - --- Starting Fold 2/2 ---
2025-08-15 18:55:20,005 - INFO - Starting training for fold 2/2
/usr/pkg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
2025-08-15 18:55:25,647 - INFO - Fold 2, Epoch 10: Val Acc: 0.47%
2025-08-15 18:55:28,981 - INFO - Fold 2, Epoch 20: Val Acc: 0.72%
2025-08-15 18:55:29,572 - INFO - Fold 2, Epoch 30: Val Acc: 0.62%
2025-08-15 18:55:30,160 - INFO - Fold 2, Epoch 40: Val Acc: 0.53%
2025-08-15 18:55:30,750 - INFO - Fold 2, Epoch 50: Val Acc: 0.53%
2025-08-15 18:55:31,344 - INFO - Fold 2, Epoch 60: Val Acc: 0.53%
2025-08-15 18:55:31,933 - INFO - Fold 2, Epoch 70: Val Acc: 0.66%
2025-08-15 18:55:32,526 - INFO - Fold 2, Epoch 80: Val Acc: 0.53%
2025-08-15 18:55:33,118 - INFO - Fold 2, Epoch 90: Val Acc: 0.75%
2025-08-15 18:55:33,707 - INFO - Fold 2, Epoch 100: Val Acc: 0.59%
2025-08-15 18:55:34,297 - INFO - Fold 2, Epoch 110: Val Acc: 0.56%
2025-08-15 18:55:34,592 - INFO - Early stopping at epoch 115
