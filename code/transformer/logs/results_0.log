INFO:__main__:Reading the dataset: fish species
INFO:__main__:model: Transformer(
  (encoder): Encoder(
    (layers): ModuleList(
      (0-2): 3 x EncoderLayer(
        (self_attention): MultiHeadAttention(
          (query): Linear(in_features=1023, out_features=1023, bias=True)
          (key): Linear(in_features=1023, out_features=1023, bias=True)
          (value): Linear(in_features=1023, out_features=1023, bias=True)
          (fc_out): Linear(in_features=1023, out_features=1023, bias=True)
        )
        (feed_forward): FeedForward(
          (fc1): Linear(in_features=1023, out_features=128, bias=True)
          (fc2): Linear(in_features=128, out_features=1023, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
        (norm1): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0-2): 3 x DecoderLayer(
        (self_attention): MultiHeadAttention(
          (query): Linear(in_features=1023, out_features=1023, bias=True)
          (key): Linear(in_features=1023, out_features=1023, bias=True)
          (value): Linear(in_features=1023, out_features=1023, bias=True)
          (fc_out): Linear(in_features=1023, out_features=1023, bias=True)
        )
        (cross_attention): MultiHeadAttention(
          (query): Linear(in_features=1023, out_features=1023, bias=True)
          (key): Linear(in_features=1023, out_features=1023, bias=True)
          (value): Linear(in_features=1023, out_features=1023, bias=True)
          (fc_out): Linear(in_features=1023, out_features=1023, bias=True)
        )
        (feed_forward): FeedForward(
          (fc1): Linear(in_features=1023, out_features=128, bias=True)
          (fc2): Linear(in_features=128, out_features=1023, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
        (norm1): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
        (dropout3): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (fc): Linear(in_features=1023, out_features=1023, bias=True)
)
INFO:__main__:Pre-training the network: Masked Spectra Modelling
INFO:pre_training:Epoch [1/10], Loss: 0.0186, Val: 0.0031
INFO:pre_training:Epoch [2/10], Loss: 0.0171, Val: 0.0029
INFO:pre_training:Epoch [3/10], Loss: 0.0162, Val: 0.0029
INFO:pre_training:Epoch [4/10], Loss: 0.0155, Val: 0.0026
INFO:pre_training:Epoch [5/10], Loss: 0.0152, Val: 0.0025
INFO:pre_training:Epoch [6/10], Loss: 0.0147, Val: 0.0025
INFO:pre_training:Epoch [7/10], Loss: 0.0143, Val: 0.0023
INFO:pre_training:Epoch [8/10], Loss: 0.0139, Val: 0.0023
INFO:pre_training:Epoch [9/10], Loss: 0.0135, Val: 0.0022
INFO:pre_training:Epoch [10/10], Loss: 0.0132, Val: 0.0021
INFO:__main__:Total time taken to pre-train the model: 8.48s
INFO:__main__:model: Transformer(
  (encoder): Encoder(
    (layers): ModuleList(
      (0-2): 3 x EncoderLayer(
        (self_attention): MultiHeadAttention(
          (query): Linear(in_features=1023, out_features=1023, bias=True)
          (key): Linear(in_features=1023, out_features=1023, bias=True)
          (value): Linear(in_features=1023, out_features=1023, bias=True)
          (fc_out): Linear(in_features=1023, out_features=1023, bias=True)
        )
        (feed_forward): FeedForward(
          (fc1): Linear(in_features=1023, out_features=128, bias=True)
          (fc2): Linear(in_features=128, out_features=1023, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
        (norm1): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0-2): 3 x DecoderLayer(
        (self_attention): MultiHeadAttention(
          (query): Linear(in_features=1023, out_features=1023, bias=True)
          (key): Linear(in_features=1023, out_features=1023, bias=True)
          (value): Linear(in_features=1023, out_features=1023, bias=True)
          (fc_out): Linear(in_features=1023, out_features=1023, bias=True)
        )
        (cross_attention): MultiHeadAttention(
          (query): Linear(in_features=1023, out_features=1023, bias=True)
          (key): Linear(in_features=1023, out_features=1023, bias=True)
          (value): Linear(in_features=1023, out_features=1023, bias=True)
          (fc_out): Linear(in_features=1023, out_features=1023, bias=True)
        )
        (feed_forward): FeedForward(
          (fc1): Linear(in_features=1023, out_features=128, bias=True)
          (fc2): Linear(in_features=128, out_features=1023, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
        (norm1): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
        (dropout3): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (fc): Linear(in_features=1023, out_features=2, bias=True)
)
INFO:__main__:Pre-training the network: Next Spectra Prediction
INFO:pre_training:Epoch 1, Average Loss: 0.5944 Validation: 0.2809
INFO:pre_training:Epoch 2, Average Loss: 0.5904 Validation: 0.3380
INFO:pre_training:Epoch 3, Average Loss: 0.5711 Validation: 0.2906
INFO:pre_training:Epoch 4, Average Loss: 0.5687 Validation: 0.2845
INFO:pre_training:Epoch 5, Average Loss: 0.5634 Validation: 0.2897
INFO:pre_training:Epoch 6, Average Loss: 0.5515 Validation: 0.2798
INFO:pre_training:Epoch 7, Average Loss: 0.5454 Validation: 0.2727
INFO:pre_training:Epoch 8, Average Loss: 0.5546 Validation: 0.2839
INFO:pre_training:Epoch 9, Average Loss: 0.5440 Validation: 0.3511
INFO:pre_training:Epoch 10, Average Loss: 0.5830 Validation: 0.2654
INFO:__main__:Total time taken to pre-train the model: 73.89s
INFO:__main__:model: Transformer(
  (encoder): Encoder(
    (layers): ModuleList(
      (0-2): 3 x EncoderLayer(
        (self_attention): MultiHeadAttention(
          (query): Linear(in_features=1023, out_features=1023, bias=True)
          (key): Linear(in_features=1023, out_features=1023, bias=True)
          (value): Linear(in_features=1023, out_features=1023, bias=True)
          (fc_out): Linear(in_features=1023, out_features=1023, bias=True)
        )
        (feed_forward): FeedForward(
          (fc1): Linear(in_features=1023, out_features=128, bias=True)
          (fc2): Linear(in_features=128, out_features=1023, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
        (norm1): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0-2): 3 x DecoderLayer(
        (self_attention): MultiHeadAttention(
          (query): Linear(in_features=1023, out_features=1023, bias=True)
          (key): Linear(in_features=1023, out_features=1023, bias=True)
          (value): Linear(in_features=1023, out_features=1023, bias=True)
          (fc_out): Linear(in_features=1023, out_features=1023, bias=True)
        )
        (cross_attention): MultiHeadAttention(
          (query): Linear(in_features=1023, out_features=1023, bias=True)
          (key): Linear(in_features=1023, out_features=1023, bias=True)
          (value): Linear(in_features=1023, out_features=1023, bias=True)
          (fc_out): Linear(in_features=1023, out_features=1023, bias=True)
        )
        (feed_forward): FeedForward(
          (fc1): Linear(in_features=1023, out_features=128, bias=True)
          (fc2): Linear(in_features=128, out_features=1023, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
        (norm1): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((1023,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
        (dropout3): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (fc): Linear(in_features=1023, out_features=2, bias=True)
)
INFO:__main__:Training the network
INFO:train:Epoch 1/100 	Train Loss: 0.9113, Train Acc: 0.4186	 Val Loss: 0.6328, Val Acc: 0.5909
INFO:train:Epoch 2/100 	Train Loss: 0.6182, Train Acc: 0.6860	 Val Loss: 0.5708, Val Acc: 0.5909
INFO:train:Epoch 3/100 	Train Loss: 0.5686, Train Acc: 0.6279	 Val Loss: 0.4440, Val Acc: 0.8636
INFO:train:Epoch 4/100 	Train Loss: 0.4520, Train Acc: 0.8488	 Val Loss: 0.3935, Val Acc: 1.0000
INFO:train:Epoch 5/100 	Train Loss: 0.3993, Train Acc: 0.8837	 Val Loss: 0.3453, Val Acc: 0.9545
INFO:train:Epoch 6/100 	Train Loss: 0.3623, Train Acc: 0.9419	 Val Loss: 0.3251, Val Acc: 0.9545
INFO:train:Epoch 7/100 	Train Loss: 0.3252, Train Acc: 0.9651	 Val Loss: 0.2859, Val Acc: 1.0000
INFO:train:Epoch 8/100 	Train Loss: 0.3210, Train Acc: 0.9535	 Val Loss: 0.2592, Val Acc: 1.0000
INFO:train:Epoch 9/100 	Train Loss: 0.2834, Train Acc: 0.9651	 Val Loss: 0.2428, Val Acc: 1.0000
INFO:train:Epoch 10/100 	Train Loss: 0.2598, Train Acc: 0.9884	 Val Loss: 0.2372, Val Acc: 1.0000
INFO:train:Epoch 11/100 	Train Loss: 0.2505, Train Acc: 0.9884	 Val Loss: 0.2365, Val Acc: 1.0000
INFO:train:Epoch 12/100 	Train Loss: 0.2481, Train Acc: 0.9884	 Val Loss: 0.2310, Val Acc: 1.0000
INFO:train:Epoch 13/100 	Train Loss: 0.2404, Train Acc: 1.0000	 Val Loss: 0.2296, Val Acc: 1.0000
INFO:train:Epoch 14/100 	Train Loss: 0.2332, Train Acc: 1.0000	 Val Loss: 0.2245, Val Acc: 1.0000
INFO:train:Epoch 15/100 	Train Loss: 0.2296, Train Acc: 1.0000	 Val Loss: 0.2225, Val Acc: 1.0000
INFO:train:Epoch 16/100 	Train Loss: 0.2225, Train Acc: 1.0000	 Val Loss: 0.2226, Val Acc: 1.0000
INFO:train:Epoch 17/100 	Train Loss: 0.2219, Train Acc: 1.0000	 Val Loss: 0.2222, Val Acc: 1.0000
INFO:train:Epoch 18/100 	Train Loss: 0.2202, Train Acc: 1.0000	 Val Loss: 0.2241, Val Acc: 1.0000
INFO:train:Epoch 19/100 	Train Loss: 0.2194, Train Acc: 1.0000	 Val Loss: 0.2213, Val Acc: 1.0000
INFO:train:Epoch 20/100 	Train Loss: 0.2163, Train Acc: 1.0000	 Val Loss: 0.2249, Val Acc: 1.0000
INFO:train:Epoch 21/100 	Train Loss: 0.2189, Train Acc: 1.0000	 Val Loss: 0.2252, Val Acc: 1.0000
INFO:train:Epoch 22/100 	Train Loss: 0.2173, Train Acc: 1.0000	 Val Loss: 0.2257, Val Acc: 1.0000
INFO:train:Epoch 23/100 	Train Loss: 0.2168, Train Acc: 1.0000	 Val Loss: 0.2219, Val Acc: 1.0000
INFO:train:Early stopping triggered after 23 epochs
INFO:plot:Saving attention map to: figures/model_accuracy.png
INFO:__main__:Total time taken to train the model: 5.79s
INFO:train:train got 64 / 64 correct, accuracy: 1.0
INFO:plot:Saving cofusion matrix map to: figures/train_confusion_matrix.png
INFO:train:train got 22 / 22 correct, accuracy: 1.0
INFO:plot:Saving cofusion matrix map to: figures/train_confusion_matrix.png
INFO:train:Total time taken evaluate on train set the model: 0.23s
INFO:train:validation got 22 / 22 correct, accuracy: 1.0
INFO:plot:Saving cofusion matrix map to: figures/validation_confusion_matrix.png
INFO:train:Total time taken evaluate on validation set the model: 0.08s
INFO:plot:Saving attention map to: figures/encoder_attention_map.png
INFO:plot:Saving attention map to: figures/decoder_attention_map.png
