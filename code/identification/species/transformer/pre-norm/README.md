# Transformer runs - Pre-norm

This transformer uses the Pre-norm formulation (Xiong 2020, Ba 2016).

The layer normalization happens after the attention and feedforward blocks.

## References

1. Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., ... & Liu, T. (2020, November). On layer normalization in the transformer architecture. In International Conference on Machine Learning (pp. 10524-10533). PMLR.
2. Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.