{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaF0AMYUWG5pUNBScP7qTB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/fishy-business/blob/main/code/identification/species/RO1_SO1_Identification_Species_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Research Objective 1 - Identification, Subobjetive 1 - Fish species\n",
        "\n",
        "To identify a fish species, either Hoki or Mackeral, is a binary classification task.\n",
        "\n",
        "## Convolutional Neural Network (CNN)\n",
        "\n",
        "Convolutional Neural Network (CNN) is used to perform binary classification for the fish species identification task.\n"
      ],
      "metadata": {
        "id": "u5suJbKQTNzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the google drive where the closed-source dataset is safely and privately stored."
      ],
      "metadata": {
        "id": "vt6vhFfGizQt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s9dsHzmSwH7",
        "outputId": "7881b2ba-ed2b-474f-ae37-0a5d993997d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Colab Notebooks',\n",
              " 'University',\n",
              " 'AI',\n",
              " 'profile.jpg',\n",
              " 'personal',\n",
              " 'ass3_example_solutions_marking_guides_2023_V2.docx',\n",
              " 'JAPA101: Handwriting Assignment - Part 2 - Creative Writing and Reflection.gdoc']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.listdir('/content/drive/My Drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w6Eox3wNTkpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data\n",
        "\n",
        "To load an xlsx file into memory using Python, you can use the pandas library. The pandas library provides a read_excel() function that can be used to read an xlsx file into a DataFrame object.\n",
        "\n",
        "This code will join the strings in the path_array variable and print the resulting joined path. The output will be /usr/local/bin.\n",
        "\n",
        "Note that the * operator is used to unpack the path_array variable so that each element in the array is passed as a separate argument to the os.path.join() function. This allows the function to join the strings in the array together to form a valid path.\n",
        "\n",
        "To convert the feature names (column names) of a Pandas DataFrame to strings, you can use the DataFrame.columns.astype(str) method. This will convert the column names to strings and return a new index object."
      ],
      "metadata": {
        "id": "nEPV60W7YmqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "path = ['drive', 'MyDrive', 'AI', 'fish', 'REIMS_data.xlsx']\n",
        "path = os.path.join(*path)\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel(path)"
      ],
      "metadata": {
        "id": "gPaRCdSHsLoB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Methods\n"
      ],
      "metadata": {
        "id": "BdqnVEWOsd1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification - Traditional Machine Learning Techniques"
      ],
      "metadata": {
        "id": "NOKQ21nBsSvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "mGLWfzXKpQBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "WtA51eTzpRcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1E-4 #@param {type:\"integer\"}\n",
        "batch_size = 64 # @param {type:\"integer\"}\n",
        "epochs = 2000 # @param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "68Y7uBE9pUbK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "NqLlBsg_ozc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, samples, labels):\n",
        "        self.samples = torch.tensor(samples.to_numpy(), dtype=torch.float32)\n",
        "        self.labels = torch.tensor([np.array(ys) for ys in labels], dtype=torch.float32)\n",
        "\n",
        "        # Normalize the features to be between in [0,1]\n",
        "        self.samples = F.normalize(self.samples, dim = 0)\n",
        "\n",
        "        # full_batch_size = self.samples.shape[0]  # Get the batch size\n",
        "        # in_channels = 1  # The number of channels in each item\n",
        "        # sequence_length = self.samples[0].shape[0]  # Get the sequence length\n",
        "        # # Reshape the input data\n",
        "        # self.samples = self.samples.view(full_batch_size, in_channels, sequence_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "print(\"[INFO] Reading the dataset.\")\n",
        "raw = pd.read_excel(path)\n",
        "\n",
        "data = raw[~raw['m/z'].str.contains('HM')]\n",
        "data = data[~data['m/z'].str.contains('QC')]\n",
        "data = data[~data['m/z'].str.contains('HM')]\n",
        "X = data.drop('m/z', axis=1) # X contains only the features.\n",
        "# Onehot encoding for the class labels, e.g. [0,1] for Hoki, [1,0] for Mackeral.\n",
        "y = data['m/z'].apply(lambda x: [0,1] if 'H' in x else [1,0])\n",
        "y = np.array(y)\n",
        "\n",
        "# Evaluation parameters.\n",
        "train_split = 0.8\n",
        "val_split = 0.5 # 1/2 of 20%, validation and test, 10% and 10%, respectively.\n",
        "# print(f\"[DEBUG] train_split, val_split, test_split: ({train_split},{val_split},{test_split})\")\n",
        "\n",
        "# Step 2: Split your dataset into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, stratify=y, test_size=(1-train_split))#, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_split)#, random_state=42)\n",
        "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1-train_split), random_state=42)\n",
        "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_split, random_state=42)\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "val_dataset = CustomDataset(X_val, y_val)\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "\n",
        "assert train_dataset.samples.shape[0] == train_dataset.labels.shape[0] , \"train_dataset samples and labels should have same length.\"\n",
        "assert val_dataset.samples.shape[0] == val_dataset.labels.shape[0] , \"train_dataset samples and labels should have same length.\"\n",
        "assert test_dataset.samples.shape[0] == test_dataset.labels.shape[0] , \"train_dataset samples and labels should have same length.\"\n",
        "\n",
        "# Step 4: Create PyTorch DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# calculate steps per epoch for training and validation set\n",
        "train_steps = len(train_loader.dataset) // batch_size\n",
        "val_steps = len(val_loader.dataset) // batch_size\n",
        "# when batch_size greater than dataset size, avoid division by zero.\n",
        "train_steps = max(1, train_steps)\n",
        "val_steps = max(1, val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZBX4pTprxQN",
        "outputId": "c1a37cc6-ccda-41c7-be9e-a3e03d2bdc33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Reading the dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-f7cdbc4509a5>:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  self.labels = torch.tensor([np.array(ys) for ys in labels], dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "E6D_L3XasM5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn import Sequential, Module, Conv1d, Linear, MaxPool1d, Dropout, Flatten, GELU, Sigmoid, LogSoftmax, CrossEntropyLoss, BatchNorm1d\n",
        "from torch import flatten\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "class LeNet(Module):\n",
        "\n",
        "\n",
        "    def __init__(self, output=2):\n",
        "        \"\"\"\n",
        "        A simple convolutional neural network, based on LeNet from (LeCun 1989)\n",
        "\n",
        "        Args:\n",
        "            shape (np-like): the architecture for the CNN.\n",
        "            for\n",
        "                (input, hidden, output) = shape\n",
        "            where\n",
        "                input - number of features.\n",
        "                hidden - dimensions for hidden layer(s).\n",
        "                output - number of classes.\n",
        "\n",
        "        References:\n",
        "        1. LeCun, Y. (1989). Generalization and network design strategies.\n",
        "            Connectionism in perspective, 19(143-155), 18.\n",
        "        2. LeCun, Y., Boser, B., Denker, J., Henderson, D., Howard,\n",
        "            R., Hubbard, W., & Jackel, L. (1989).\n",
        "            Handwritten digit recognition with a back-propagation network.\n",
        "            Advances in neural information processing systems, 2.\n",
        "        3. LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E.,\n",
        "            Hubbard, W., & Jackel, L. D. (1989).\n",
        "            Backpropagation applied to handwritten zip code recognition.\n",
        "            Neural computation, 1(4), 541-551.\n",
        "        4. Hendrycks, D., & Gimpel, K. (2016).\n",
        "            Gaussian error linear units (gelus).\n",
        "            arXiv preprint arXiv:1606.08415.\n",
        "        5. Ioffe, S., & Szegedy, C. (2015, June).\n",
        "            Batch normalization: Accelerating deep network training by reducing internal covariate shift.\n",
        "            In International conference on machine learning (pp. 448-456). pmlr.\n",
        "        6. Srivastava, N., Hinton, G., Krizhevsky, A.,\n",
        "            Sutskever, I., & Salakhutdinov, R. (2014).\n",
        "            Dropout: a simple way to prevent neural networks from overfitting.\n",
        "            The journal of machine learning research, 15(1), 1929-1958.\n",
        "        7. Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever,\n",
        "            I., & Salakhutdinov, R. R. (2012).\n",
        "            Improving neural networks by preventing co-adaptation of feature detectors.\n",
        "            arXiv preprint arXiv:1207.0580.\n",
        "        8. Glorot, X., & Bengio, Y. (2010, March).\n",
        "            Understanding the difficulty of training deep feedforward neural networks.\n",
        "            In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256).\n",
        "            JMLR Workshop and Conference Proceedings.\n",
        "        9. Loshchilov, I., & Hutter, F. (2017).\n",
        "            Decoupled weight decay regularization.\n",
        "            arXiv preprint arXiv:1711.05101.\n",
        "\n",
        "        \"\"\"\n",
        "        # CNN [1,2,3]\n",
        "        super(LeNet, self).__init__()\n",
        "        activation = GELU() # GELU [4]\n",
        "\n",
        "        self.layers = Sequential(OrderedDict([\n",
        "            # First convolutional layer\n",
        "            ('conv1', Conv1d(in_channels=1, out_channels=64, kernel_size=5)),\n",
        "            ('batchnorm1', BatchNorm1d(64)), # BatchNorm [5]\n",
        "            ('activation1', activation),\n",
        "            ('maxpool1', MaxPool1d(kernel_size=2, stride=2)),\n",
        "            ('dropout1', Dropout(p=0.5)), # Dropout [6,7]\n",
        "            # Second convolutional layer\n",
        "            ('conv2', Conv1d(in_channels=64, out_channels=128, kernel_size=5)),\n",
        "            ('batchnorm2', BatchNorm1d(128)),\n",
        "            ('activation2', activation),\n",
        "            ('maxpool2', MaxPool1d(kernel_size=2, stride=2)),\n",
        "            ('dropout2', Dropout(p=0.5)),\n",
        "            # Fully connected layer 1\n",
        "            ('flatten', Flatten()),\n",
        "            ('fc1', Linear(in_features=32256, out_features=800)),  # With 4 convolutional layers\n",
        "            ('batchnorm3', BatchNorm1d(800)),\n",
        "            ('activation3', activation),\n",
        "            ('dropout3', Dropout(p=0.5)),\n",
        "            # Fully connected layer 2\n",
        "            ('fc2', Linear(in_features=800, out_features=2)),\n",
        "            # Output layer.\n",
        "            ('batchnorm4', BatchNorm1d(output)),\n",
        "            ('activation4', LogSoftmax(dim=1)),\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass of the convolutional neural network.\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "print(\"[INFO] Initialize cuda environment\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPUs if available.\n",
        "\n",
        "print(\"[INFO] Initializing the LeNet model\")\n",
        "\n",
        "model = LeNet()\n",
        "\n",
        " # Xavier initialization [8]\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
        "        xavier_uniform_(m.weight)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "# AdamW [9]\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "loss_function = CrossEntropyLoss()\n",
        "\n",
        "H = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_acc\": []\n",
        "}\n",
        "\n",
        "print(\"[INFO] Training the network\")\n",
        "startTime = time.time()\n",
        "\n",
        "# Training the CNN model.\n",
        "for e in range(0, epochs):\n",
        "    # Puts the model in train mode.\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    train_correct = 0\n",
        "    val_correct = 0\n",
        "\n",
        "    # Training routine\n",
        "    # Note: the benefit of using torch's DataLoader,\n",
        "    # is that is automatically yields batches of training data.\n",
        "    # This makes it ammeneable to training a deep neural network.\n",
        "    for (x,y) in train_loader:\n",
        "        (x,y) = (x.to(device), y.to(device))\n",
        "        prediction = model(x.unsqueeze(1))\n",
        "        loss = loss_function(prediction, y)\n",
        "        optimizer.zero_grad()  # 1. Zero out the gradients\n",
        "        loss.backward() # 2. Perform a backwards pass.\n",
        "        optimizer.step() # 3. Update the weights.\n",
        "        total_train_loss += loss\n",
        "        train_correct += (prediction.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "\n",
        "    # Evalulation routine\n",
        "    # no_grad() turns off gradient tracking and computation.\n",
        "    with torch.no_grad():\n",
        "        # Puts the model in evaluation mode.\n",
        "        model.eval()\n",
        "        for (x,y) in val_loader:\n",
        "            (x,y) = (x.to(device), y.to(device))\n",
        "            prediction = model(x.unsqueeze(1))\n",
        "            total_val_loss = loss_function(prediction,y)\n",
        "            val_correct += (prediction.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "\n",
        "    # calculate the average training and validation loss\n",
        "    avgTrainLoss = total_train_loss / train_steps\n",
        "    avgValLoss = total_val_loss / val_steps\n",
        "\n",
        "    # calculate the training and validation accuracy\n",
        "    trainCorrect = train_correct / len(train_loader.dataset)\n",
        "    valCorrect = val_correct / len(val_loader.dataset)\n",
        "\n",
        "    # update our training history\n",
        "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "    H[\"train_acc\"].append(trainCorrect)\n",
        "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
        "    H[\"val_acc\"].append(valCorrect)\n",
        "\n",
        "    # Print telemetry every 50 epochs, to avoid spamming standard out.\n",
        "    if e % 50 == 0:\n",
        "        # print the model training and validation information\n",
        "        print(\"[INFO] EPOCH: {}/{}\".format(e + 1, epochs))\n",
        "        print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, trainCorrect))\n",
        "        print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(avgValLoss, valCorrect))\n",
        "\n",
        "    # Early stopping\n",
        "    # if valCorrect >= 1.00 and trainCorrect >= 1.00:\n",
        "    #     print(\"[INFO] Finished on => EPOCH: {}/{}\".format(e + 1, epochs))\n",
        "    #     print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, trainCorrect))\n",
        "    #     print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(avgValLoss, valCorrect))\n",
        "    #     break;\n",
        "\n",
        "# finish measuring how long training took\n",
        "endTime = time.time()\n",
        "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
        "\tendTime - startTime))\n",
        "# we can now evaluate the network on the test set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
        "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
        "plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.legend(bbox_to_anchor=(1.0, 1.0))\n",
        "plt.savefig(\"model_accuracy.png\")\n",
        "# serialize the model to disk\n",
        "# torch.save(model, \"model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "yFhnqGlot9ki",
        "outputId": "f9d4dc28-53ae-4646-e4d0-bee7bfdb0383"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Initialize cuda environment\n",
            "[INFO] Initializing the LeNet model\n",
            "[INFO] Training the network\n",
            "[INFO] EPOCH: 1/2000\n",
            "Train loss: 0.957570, Train accuracy: 0.6578\n",
            "Val loss: 0.685985, Val accuracy: 0.5217\n",
            "\n",
            "[INFO] EPOCH: 51/2000\n",
            "Train loss: 0.220299, Train accuracy: 1.0000\n",
            "Val loss: 0.413532, Val accuracy: 0.9130\n",
            "\n",
            "[INFO] EPOCH: 101/2000\n",
            "Train loss: 0.199341, Train accuracy: 1.0000\n",
            "Val loss: 0.353103, Val accuracy: 0.9565\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e1e3728e6e1d>\u001b[0m in \u001b[0;36m<cell line: 133>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 3. Update the weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mtrain_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# Evalulation routine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# switch off autograd\n",
        "with torch.no_grad():\n",
        "    # loop over the test set\n",
        "    datasets = [(\"train\", train_loader), (\"validation\", val_loader), (\"test\", test_loader)]\n",
        "    for name, dataset in datasets:\n",
        "        for (x,y) in dataset:\n",
        "            (x,y) = (x.to(device), y.to(device))\n",
        "            y_true = y\n",
        "            pred = model(x.unsqueeze(1))\n",
        "            y_pred = pred.argmax(axis=1).cpu().numpy()\n",
        "            test_correct = (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "            print(f\"[INFO] {name} accuracy {test_correct} / {len(x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdP9FpGvjnzW",
        "outputId": "fd4bc9fa-0225-43fc-96ad-f86594f93c51"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] train accuracy 64.0 / 64\n",
            "[INFO] train accuracy 64.0 / 64\n",
            "[INFO] train accuracy 59.0 / 59\n",
            "[INFO] validation accuracy 22.0 / 23\n",
            "[INFO] test accuracy 24.0 / 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the CNN 30 times for statistical significance testing.\n",
        "# Also to allow comparison to other classification methods.\n",
        "\n",
        "run = {\n",
        "    \"1\" : (1,1),\n",
        "    \"2\" : (1,0.875),\n",
        "    \"3\" : (1,1),\n",
        "    \"4\" : (1,1),\n",
        "    \"5\" : (1,0.875),\n",
        "    \"6\" : (1,1),\n",
        "    \"7\" : (1,0.95833),\n",
        "    \"8\" : (1,1),\n",
        "    \"9\" : (1,1),\n",
        "    \"10\": (1,1),\n",
        "    \"11\": (1,1),\n",
        "    \"12\": (1,1),\n",
        "    \"13\": (1,0.95833),\n",
        "    \"14\": (1,1),\n",
        "    \"15\": (1,0.91666),\n",
        "    \"16\": (1,0.95833),\n",
        "    \"17\": (1,1),\n",
        "    \"18\": (1,0.95833),\n",
        "    \"19\": (1,0.95833),\n",
        "    \"20\": (1,1),\n",
        "    \"21\": (1,1),\n",
        "    \"22\": (1,1),\n",
        "    \"23\": (1,1),\n",
        "    \"24\": (1,1),\n",
        "    \"25\": (1,0.91666),\n",
        "    \"26\": (1,1),\n",
        "    \"27\": (1,1),\n",
        "    \"28\": (1,1),\n",
        "    \"29\": (1,1),\n",
        "    \"30\": (1,1)\n",
        "}"
      ],
      "metadata": {
        "id": "7-Gg3ImsyeJ2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V2 runs, updated architecture.\n",
        "\n",
        "# Run the CNN 30 times for statistical significance testing.\n",
        "# Also to allow comparison to other classification methods.\n",
        "\n",
        "run = {\n",
        "    \"1\" : (1,1),\n",
        "    \"2\" : (1,0.9583),\n",
        "    \"3\" : (1,1),\n",
        "    \"4\" : (1,1),\n",
        "    \"5\" : (1,1),\n",
        "    \"6\" : (1,1),\n",
        "    \"7\" : (1,1),\n",
        "    \"8\" : (1,1),\n",
        "    \"9\" : (1,0.9583),\n",
        "    \"10\": (1,1),\n",
        "    \"11\": (1,1),\n",
        "    \"12\": (1,1),\n",
        "    \"13\": (1,1),\n",
        "    \"14\": (1,0.95833),\n",
        "    \"15\": (1,916666666667),\n",
        "    \"16\": (1,0.95833),\n",
        "    \"17\": (1,1),\n",
        "    \"18\": (1,1),\n",
        "    \"19\": (1,1),\n",
        "    \"20\": (1,1),\n",
        "    \"21\": (1,1),\n",
        "    \"22\": (1,0.95833),\n",
        "    \"23\": (1,1),\n",
        "    \"24\": (1,0.95833),\n",
        "    \"25\": (1,0.916666666667),\n",
        "    \"26\": (1,0.95833),\n",
        "    \"27\": (1,1),\n",
        "    \"28\": (1,1),\n",
        "    \"29\": (1,0.95833),\n",
        "    \"30\": (1,1),\n",
        "}"
      ],
      "metadata": {
        "id": "Y8LnWKxuxhoc"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}