{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOuGuMQ4VUYBw+nz9tzI2IA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/fishy-business/blob/main/code/identification/species/RO1_SO1_Identification_Species_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Research Objective 1 - Identification, Subobjetive 1 - Fish species\n",
        "\n",
        "To identify a fish species, either Hoki or Mackeral, is a binary classification task.\n",
        "\n",
        "## Convolutional Neural Network (CNN)\n",
        "\n",
        "Convolutional Neural Network (CNN) is used to perform binary classification for the fish species identification task.\n"
      ],
      "metadata": {
        "id": "u5suJbKQTNzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the google drive where the closed-source dataset is safely and privately stored."
      ],
      "metadata": {
        "id": "vt6vhFfGizQt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s9dsHzmSwH7",
        "outputId": "6d812b2f-e936-4de8-a5f6-61892aa219d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Colab Notebooks',\n",
              " 'University',\n",
              " 'AI',\n",
              " 'profile.jpg',\n",
              " 'personal',\n",
              " 'ass3_example_solutions_marking_guides_2023_V2.docx',\n",
              " 'JAPA101: Handwriting Assignment - Part 2 - Creative Writing and Reflection.gdoc']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.listdir('/content/drive/My Drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w6Eox3wNTkpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data\n",
        "\n",
        "To load an xlsx file into memory using Python, you can use the pandas library. The pandas library provides a read_excel() function that can be used to read an xlsx file into a DataFrame object.\n",
        "\n",
        "This code will join the strings in the path_array variable and print the resulting joined path. The output will be /usr/local/bin.\n",
        "\n",
        "Note that the * operator is used to unpack the path_array variable so that each element in the array is passed as a separate argument to the os.path.join() function. This allows the function to join the strings in the array together to form a valid path.\n",
        "\n",
        "To convert the feature names (column names) of a Pandas DataFrame to strings, you can use the DataFrame.columns.astype(str) method. This will convert the column names to strings and return a new index object."
      ],
      "metadata": {
        "id": "nEPV60W7YmqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "path = ['drive', 'MyDrive', 'AI', 'fish', 'REIMS_data.xlsx']\n",
        "path = os.path.join(*path)\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel(path)"
      ],
      "metadata": {
        "id": "gPaRCdSHsLoB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Methods\n"
      ],
      "metadata": {
        "id": "BdqnVEWOsd1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification - Traditional Machine Learning Techniques"
      ],
      "metadata": {
        "id": "NOKQ21nBsSvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "mGLWfzXKpQBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "WtA51eTzpRcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1E-4 #@param {type:\"integer\"}\n",
        "batch_size = 64 # @param {type:\"integer\"}\n",
        "epochs = 2000 # @param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "68Y7uBE9pUbK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "NqLlBsg_ozc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, samples, labels):\n",
        "        self.samples = torch.tensor(samples.to_numpy(), dtype=torch.float32)\n",
        "        self.labels = torch.tensor([np.array(ys) for ys in labels], dtype=torch.float32)\n",
        "\n",
        "        # Normalize the features to be between in [0,1]\n",
        "        self.samples = F.normalize(self.samples, dim = 0)\n",
        "\n",
        "        # full_batch_size = self.samples.shape[0]  # Get the batch size\n",
        "        # in_channels = 1  # The number of channels in each item\n",
        "        # sequence_length = self.samples[0].shape[0]  # Get the sequence length\n",
        "        # # Reshape the input data\n",
        "        # self.samples = self.samples.view(full_batch_size, in_channels, sequence_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "print(\"[INFO] Reading the dataset.\")\n",
        "raw = pd.read_excel(path)\n",
        "\n",
        "data = raw[~raw['m/z'].str.contains('HM')]\n",
        "data = data[~data['m/z'].str.contains('QC')]\n",
        "data = data[~data['m/z'].str.contains('HM')]\n",
        "X = data.drop('m/z', axis=1) # X contains only the features.\n",
        "# Onehot encoding for the class labels, e.g. [0,1] for Hoki, [1,0] for Mackeral.\n",
        "y = data['m/z'].apply(lambda x: [0,1] if 'H' in x else [1,0])\n",
        "y = np.array(y)\n",
        "\n",
        "# Evaluation parameters.\n",
        "train_split = 0.8\n",
        "val_split = 0.5 # 1/2 of 20%, validation and test, 10% and 10%, respectively.\n",
        "# print(f\"[DEBUG] train_split, val_split, test_split: ({train_split},{val_split},{test_split})\")\n",
        "\n",
        "# Step 2: Split your dataset into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, stratify=y, test_size=(1-train_split))#, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_split)#, random_state=42)\n",
        "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1-train_split), random_state=42)\n",
        "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_split, random_state=42)\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "val_dataset = CustomDataset(X_val, y_val)\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "\n",
        "assert train_dataset.samples.shape[0] == train_dataset.labels.shape[0] , \"train_dataset samples and labels should have same length.\"\n",
        "assert val_dataset.samples.shape[0] == val_dataset.labels.shape[0] , \"train_dataset samples and labels should have same length.\"\n",
        "assert test_dataset.samples.shape[0] == test_dataset.labels.shape[0] , \"train_dataset samples and labels should have same length.\"\n",
        "\n",
        "# Step 4: Create PyTorch DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# calculate steps per epoch for training and validation set\n",
        "train_steps = len(train_loader.dataset) // batch_size\n",
        "val_steps = len(val_loader.dataset) // batch_size\n",
        "# when batch_size greater than dataset size, avoid division by zero.\n",
        "train_steps = max(1, train_steps)\n",
        "val_steps = max(1, val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZBX4pTprxQN",
        "outputId": "49a39649-0bf2-4496-8fcd-18effe976513"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Reading the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "E6D_L3XasM5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Module, Conv1d, Linear, MaxPool1d, AdaptiveAvgPool1d, Dropout, ReLU, LeakyReLU, GELU, Sigmoid, LogSoftmax, Softmax, CrossEntropyLoss, BatchNorm1d, LayerNorm\n",
        "from torch import flatten\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "class LeNet(Module):\n",
        "\n",
        "\n",
        "    def __init__(self, shape):\n",
        "        \"\"\"\n",
        "        A simple convolutional neural network, based on LeNet from (LeCun 1989)\n",
        "\n",
        "        Args:\n",
        "            shape (np-like): the architecture for the CNN.\n",
        "            for\n",
        "                (input, hidden, output) = shape\n",
        "            where\n",
        "                input - number of features.\n",
        "                hidden - dimensions for hidden layer(s).\n",
        "                output - number of classes.\n",
        "\n",
        "        References:\n",
        "        1. LeCun, Y. (1989). Generalization and network design strategies.\n",
        "            Connectionism in perspective, 19(143-155), 18.\n",
        "        2. LeCun, Y., Boser, B., Denker, J., Henderson, D., Howard,\n",
        "            R., Hubbard, W., & Jackel, L. (1989).\n",
        "            Handwritten digit recognition with a back-propagation network.\n",
        "            Advances in neural information processing systems, 2.\n",
        "        3. LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E.,\n",
        "            Hubbard, W., & Jackel, L. D. (1989).\n",
        "            Backpropagation applied to handwritten zip code recognition.\n",
        "            Neural computation, 1(4), 541-551.\n",
        "        \"\"\"\n",
        "        super(LeNet, self).__init__()\n",
        "        self.shape = shape\n",
        "        (input, hidden, output) = shape\n",
        "        activation = GELU()\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = Conv1d(in_channels=1, out_channels=64, kernel_size=5)\n",
        "        self.batchnorm1 = BatchNorm1d(64)\n",
        "        self.activation1 = activation\n",
        "        self.maxpool1 = MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = Dropout(p=0.5)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = Conv1d(in_channels=64, out_channels=128, kernel_size=5)\n",
        "        self.batchnorm2 = BatchNorm1d(128)\n",
        "        self.activation2 = activation\n",
        "        self.maxpool2 = MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout2 = Dropout(p=0.5)\n",
        "\n",
        "        # # # Third convolutional layer\n",
        "        # self.conv3 = Conv1d(in_channels=16, out_channels=32, kernel_size=5)\n",
        "        # self.activation3 = LeakyReLU()\n",
        "        # self.maxpool3 = MaxPool1d(kernel_size=2, stride=2)\n",
        "        # self.dropout3 = Dropout(p=0.5)\n",
        "\n",
        "        # # Fourth convolutional layer\n",
        "        # self.conv4 = Conv1d(in_channels=32, out_channels=64, kernel_size=5)\n",
        "        # self.activation4 = LeakyReLU()\n",
        "        # self.maxpool4 = MaxPool1d(kernel_size=2, stride=2)\n",
        "        # self.dropout4 = Dropout(p=0.5)\n",
        "\n",
        "        # Fully connected layer 1\n",
        "        self.flatten = lambda x: flatten(x, 1)\n",
        "        self.fc1 = Linear(in_features=32256, out_features=800)  # With 4 convolutional layers\n",
        "        self.dropout5 = Dropout(p=0.5)\n",
        "\n",
        "        # self.fc1 = Linear(in_features=8064, out_features=800) # With 2 convolutional layers\n",
        "\n",
        "        self.batchnorm3 = BatchNorm1d(800)\n",
        "        self.activation5 = activation\n",
        "\n",
        "        # # Fully connetcted layer 2\n",
        "        # self.fc2 = Linear(in_features=800, out_features=500)\n",
        "        # self.activation6 = activation\n",
        "\n",
        "        # Fully connected layer 3\n",
        "        self.fc3 = Linear(in_features=800, out_features=output)\n",
        "        # Output layer.\n",
        "        self.batchnorm4 = BatchNorm1d(output)\n",
        "        self.activation7 = LogSoftmax(dim=1)\n",
        "        # self.softmax = Softmax()\n",
        "\n",
        "        self.layers = layers = [\n",
        "            self.conv1, self.batchnorm1, self.activation1, self.maxpool1, # First layer\n",
        "            self.dropout1, # Regularization\n",
        "            self.conv2, self.batchnorm2, self.activation2, self.maxpool2, # Second layer\n",
        "            self.dropout2, # Regularization\n",
        "            # self.conv3, self.activation3, self.maxpool3, # Third layer\n",
        "            # self.dropout3, # Regularization\n",
        "            # self.conv4, self.activation4, self.maxpool4, # Fourth layer\n",
        "            # self.dropout4, # Regularization\n",
        "            self.flatten,\n",
        "            self.fc1, self.batchnorm3, self.activation5, # Fifth layer\n",
        "            self.dropout5,\n",
        "            # self.fc2, self.activation6, # Sixth layer\n",
        "            self.fc3, self.batchnorm4, self.activation7 # Seventh layer\n",
        "        ]\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass of the convolutional neural network.\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "print(\"[INFO] Initialize cuda environment\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPUs if available.\n",
        "\n",
        "print(\"[INFO] Initializing the LeNet model\")\n",
        "\n",
        "input = batch_size\n",
        "num_channels = 1\n",
        "output = 2\n",
        "model = LeNet(\n",
        "    shape = (\n",
        "        input, # Input\n",
        "         [num_channels, batch_size, batch_size, 124, 50], #Hidden\n",
        "        output) # Output\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "loss_function = CrossEntropyLoss()\n",
        "\n",
        "H = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_acc\": []\n",
        "}\n",
        "\n",
        "print(\"[INFO] Training the network\")\n",
        "startTime = time.time()\n",
        "\n",
        "# Training the CNN model.\n",
        "for e in range(0, epochs):\n",
        "    # Puts the model in train mode.\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    train_correct = 0\n",
        "    val_correct = 0\n",
        "\n",
        "    # Training routine\n",
        "    # Note: the benefit of using torch's DataLoader,\n",
        "    # is that is automatically yields batches of training data.\n",
        "    # This makes it ammeneable to training a deep neural network.\n",
        "    for (x,y) in train_loader:\n",
        "        (x,y) = (x.to(device), y.to(device))\n",
        "        prediction = model(x.unsqueeze(1))\n",
        "        loss = loss_function(prediction, y)\n",
        "        optimizer.zero_grad()  # 1. Zero out the gradients\n",
        "        loss.backward() # 2. Perform a backwards pass.\n",
        "        optimizer.step() # 3. Update the weights.\n",
        "        total_train_loss += loss\n",
        "        train_correct += (prediction.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "\n",
        "    # Evalulation routine\n",
        "    # no_grad() turns off gradient tracking and computation.\n",
        "    with torch.no_grad():\n",
        "        # Puts the model in evaluation mode.\n",
        "        model.eval()\n",
        "        for (x,y) in val_loader:\n",
        "            (x,y) = (x.to(device), y.to(device))\n",
        "            prediction = model(x.unsqueeze(1))\n",
        "            total_val_loss = loss_function(prediction,y)\n",
        "            val_correct += (prediction.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "\n",
        "    # calculate the average training and validation loss\n",
        "    avgTrainLoss = total_train_loss / train_steps\n",
        "    avgValLoss = total_val_loss / val_steps\n",
        "\n",
        "    # calculate the training and validation accuracy\n",
        "    trainCorrect = train_correct / len(train_loader.dataset)\n",
        "    valCorrect = val_correct / len(val_loader.dataset)\n",
        "\n",
        "    # update our training history\n",
        "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "    H[\"train_acc\"].append(trainCorrect)\n",
        "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
        "    H[\"val_acc\"].append(valCorrect)\n",
        "\n",
        "    # Print telemetry every 50 epochs, to avoid spamming standard out.\n",
        "    if e % 50 == 0:\n",
        "        # print the model training and validation information\n",
        "        print(\"[INFO] EPOCH: {}/{}\".format(e + 1, epochs))\n",
        "        print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, trainCorrect))\n",
        "        print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(avgValLoss, valCorrect))\n",
        "\n",
        "    # Early stopping\n",
        "    if valCorrect >= 1.00 and trainCorrect >= 1.00:\n",
        "        print(\"[INFO] Finished on => EPOCH: {}/{}\".format(e + 1, epochs))\n",
        "        print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, trainCorrect))\n",
        "        print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(avgValLoss, valCorrect))\n",
        "        break;\n",
        "\n",
        "# finish measuring how long training took\n",
        "endTime = time.time()\n",
        "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
        "\tendTime - startTime))\n",
        "# we can now evaluate the network on the test set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
        "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
        "plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.legend(bbox_to_anchor=(1.0, 1.0))\n",
        "plt.savefig(\"model_accuracy.png\")\n",
        "# serialize the model to disk\n",
        "# torch.save(model, \"model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yFhnqGlot9ki",
        "outputId": "def79145-f34b-47b9-9de1-cdcc190ed38d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Initialize cuda environment\n",
            "[INFO] Initializing the LeNet model\n",
            "[INFO] Training the network\n",
            "[INFO] EPOCH: 1/2000\n",
            "Train loss: 0.910916, Train accuracy: 0.7059\n",
            "Val loss: 0.685646, Val accuracy: 0.4783\n",
            "\n",
            "[INFO] EPOCH: 51/2000\n",
            "Train loss: 0.210378, Train accuracy: 1.0000\n",
            "Val loss: 0.321720, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 101/2000\n",
            "Train loss: 0.195702, Train accuracy: 1.0000\n",
            "Val loss: 0.387193, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 151/2000\n",
            "Train loss: 0.198310, Train accuracy: 1.0000\n",
            "Val loss: 0.382929, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 201/2000\n",
            "Train loss: 0.178228, Train accuracy: 1.0000\n",
            "Val loss: 0.417814, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 251/2000\n",
            "Train loss: 0.174305, Train accuracy: 1.0000\n",
            "Val loss: 0.389420, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 301/2000\n",
            "Train loss: 0.170047, Train accuracy: 1.0000\n",
            "Val loss: 0.403455, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 351/2000\n",
            "Train loss: 0.170258, Train accuracy: 1.0000\n",
            "Val loss: 0.400290, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 401/2000\n",
            "Train loss: 0.166728, Train accuracy: 1.0000\n",
            "Val loss: 0.365163, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 451/2000\n",
            "Train loss: 0.161255, Train accuracy: 1.0000\n",
            "Val loss: 0.369102, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 501/2000\n",
            "Train loss: 0.154303, Train accuracy: 1.0000\n",
            "Val loss: 0.364374, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 551/2000\n",
            "Train loss: 0.149245, Train accuracy: 1.0000\n",
            "Val loss: 0.347759, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 601/2000\n",
            "Train loss: 0.144659, Train accuracy: 1.0000\n",
            "Val loss: 0.357362, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 651/2000\n",
            "Train loss: 0.149541, Train accuracy: 1.0000\n",
            "Val loss: 0.334457, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 701/2000\n",
            "Train loss: 0.136974, Train accuracy: 1.0000\n",
            "Val loss: 0.324895, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 751/2000\n",
            "Train loss: 0.137807, Train accuracy: 1.0000\n",
            "Val loss: 0.317220, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 801/2000\n",
            "Train loss: 0.137276, Train accuracy: 1.0000\n",
            "Val loss: 0.310901, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 851/2000\n",
            "Train loss: 0.133089, Train accuracy: 1.0000\n",
            "Val loss: 0.295571, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 901/2000\n",
            "Train loss: 0.126437, Train accuracy: 1.0000\n",
            "Val loss: 0.302703, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 951/2000\n",
            "Train loss: 0.121048, Train accuracy: 1.0000\n",
            "Val loss: 0.290395, Val accuracy: 0.9565\n",
            "\n",
            "[INFO] EPOCH: 1001/2000\n",
            "Train loss: 0.118810, Train accuracy: 1.0000\n",
            "Val loss: 0.280764, Val accuracy: 0.9565\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-f0135bf17c0d>\u001b[0m in \u001b[0;36m<cell line: 148>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1. Zero out the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2. Perform a backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 3. Update the weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# switch off autograd\n",
        "with torch.no_grad():\n",
        "    # loop over the test set\n",
        "    datasets = [(\"train\", train_loader), (\"validation\", val_loader), (\"test\", test_loader)]\n",
        "    for name, dataset in datasets:\n",
        "        for (x,y) in dataset:\n",
        "            (x,y) = (x.to(device), y.to(device))\n",
        "            y_true = y\n",
        "            pred = model(x.unsqueeze(1))\n",
        "            y_pred = pred.argmax(axis=1).cpu().numpy()\n",
        "            test_correct = (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "            print(f\"[INFO] {name} accuracy {test_correct} / {len(x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdP9FpGvjnzW",
        "outputId": "85009532-f232-4a94-c9ca-008e47da27f1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] train accuracy 64.0 / 64\n",
            "[INFO] train accuracy 64.0 / 64\n",
            "[INFO] train accuracy 59.0 / 59\n",
            "[INFO] validation accuracy 22.0 / 23\n",
            "[INFO] test accuracy 24.0 / 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the CNN 30 times for statistical significance testing.\n",
        "# Also to allow comparison to other classification methods.\n",
        "\n",
        "run = {\n",
        "    \"1\" : (1,1),\n",
        "    \"2\" : (1,0.875),\n",
        "    \"3\" : (1,1),\n",
        "    \"4\" : (1,1),\n",
        "    \"5\" : (1,0.875),\n",
        "    \"6\" : (1,1),\n",
        "    \"7\" : (1,0.95833),\n",
        "    \"8\" : (1,1),\n",
        "    \"9\" : (1,1),\n",
        "    \"10\": (1,1),\n",
        "    \"11\": (1,1),\n",
        "    \"12\": (1,1),\n",
        "    \"13\": (1,0.95833),\n",
        "    \"14\": (1,1),\n",
        "    \"15\": (1,0.91666),\n",
        "    \"16\": (1,0.95833),\n",
        "    \"17\": (1,1),\n",
        "    \"18\": (1,0.95833),\n",
        "    \"19\": (1,0.95833),\n",
        "    \"20\": (1,1),\n",
        "    \"21\": (1,1),\n",
        "    \"22\": (1,1),\n",
        "    \"23\": (1,1),\n",
        "    \"24\": (1,1),\n",
        "    \"25\": (1,0.91666),\n",
        "    \"26\": (1,1),\n",
        "    \"27\": (1,1),\n",
        "    \"28\": (1,1),\n",
        "    \"29\": (1,1),\n",
        "    \"30\": (1,1)\n",
        "}"
      ],
      "metadata": {
        "id": "7-Gg3ImsyeJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V2 runs, updated architecture.\n",
        "\n",
        "# Run the CNN 30 times for statistical significance testing.\n",
        "# Also to allow comparison to other classification methods.\n",
        "\n",
        "run = {\n",
        "    \"1\" : (1,1),\n",
        "    \"2\" : (1,0.9583),\n",
        "    \"3\" : (1,1),\n",
        "    \"4\" : (1,1),\n",
        "    \"5\" : (1,1),\n",
        "    \"6\" : (1,1),\n",
        "    \"7\" : (1,1),\n",
        "    \"8\" : (1,1),\n",
        "    \"9\" : (1,0.9583),\n",
        "    \"10\": (1,1),\n",
        "}"
      ],
      "metadata": {
        "id": "Y8LnWKxuxhoc"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}