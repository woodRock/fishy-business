{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/fishy-business/blob/main/code/identification/part/mtgp_runs/run_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAlW5BfrgjQk"
      },
      "source": [
        "# Multi-tree Genetic Program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TESFcfJMSeT5"
      },
      "source": [
        "## Kicked for inactivity?\n",
        "\n",
        "To stop a colab notebook from disconnecting, open up the console with CTRL + SHIFT + I, and copy and execute the following code.\n",
        "\n",
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```\n",
        "\n",
        "This tricks the browser into thinking the user is active."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLL18yL7WWRt",
        "outputId": "57dcd493-d5d4-40bf-eab8-5096af6644d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deap in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.25.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libgraphviz-dev is already the newest version (2.42.2-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: pygraphviz in /usr/local/lib/python3.10/dist-packages (1.12)\n",
            "Requirement already satisfied: skfeature-chappers in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from skfeature-chappers) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from skfeature-chappers) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from skfeature-chappers) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->skfeature-chappers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->skfeature-chappers) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skfeature-chappers) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skfeature-chappers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skfeature-chappers) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->skfeature-chappers) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install deap\n",
        "!apt install libgraphviz-dev\n",
        "!pip install pygraphviz\n",
        "!pip install skfeature-chappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCcCiUIvL40G",
        "outputId": "9f69f3d5-f67a-4f34-b9a8-c11a7d42fb38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path_gdrive = '/content/drive/MyDrive/AI/Data'\n",
        "dataset = 'Part' #@param [\"Fish\", \"Part\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD4BzYVvq9_j"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-0dl8EoqK0T",
        "outputId": "c2c1de9a-11ec-4af9-bf0a-c51302685df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[INFO] Reading the dataset.\n",
            "Class Counts: [6 6 3 6 6 3], Class Ratios: [0.2 0.2 0.1 0.2 0.2 0.1]\n",
            "Number of features: 1023\n",
            "Number of instances: 30\n",
            "Number of classes 6.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Data - data.py\n",
        "==============\n",
        "\n",
        "This is the data module. It contains the functions for loading, preparing, normalizing and encoding the data.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "import scipy.io\n",
        "import os\n",
        "\n",
        "def encode_labels(y, y_test=None):\n",
        "    \"\"\"\n",
        "    Convert text labels to numbers.\n",
        "\n",
        "    Args:\n",
        "        y: The labels.\n",
        "        y_test: The test labels. Defaults to None.\n",
        "    \"\"\"\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    y = le.fit_transform(y)\n",
        "    if y_test is not None:\n",
        "        y_test = le.transform(y_test)\n",
        "    return y, y_test, le\n",
        "\n",
        "\n",
        "def load(filename, folder=''):\n",
        "    \"\"\"\n",
        "    Load the data from the mat file.\n",
        "\n",
        "    Args:\n",
        "        filename: The name of the mat file.\n",
        "        folder: The folder where the mat file is located.\n",
        "    \"\"\"\n",
        "    path = os.path.join(folder, filename)\n",
        "    mat = scipy.io.loadmat(path)\n",
        "    return mat\n",
        "\n",
        "\n",
        "def prepare(mat):\n",
        "    \"\"\"\n",
        "    Load the data from matlab format into memory.\n",
        "\n",
        "    Args:\n",
        "        mat: The data in matlab format.\n",
        "    \"\"\"\n",
        "    X = mat['X']\n",
        "    X = X.astype(float)\n",
        "    y = mat['Y']\n",
        "    y = y[:, 0]\n",
        "    return X,y\n",
        "\n",
        "\n",
        "def normalize(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Normalize the input features within range [0,1].\n",
        "\n",
        "    Args:\n",
        "        X_train: The training data.\n",
        "        X_test: The test data.\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaler = scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    return X_train, X_test\n",
        "\n",
        "# Code snippet from elsewhere.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.listdir('/content/drive/My Drive')\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "path = ['drive', 'MyDrive', 'AI', 'fish', 'REIMS_data.xlsx']\n",
        "path = os.path.join(*path)\n",
        "\n",
        "# Load the dataset\n",
        "# data = pd.read_excel(path)\n",
        "\n",
        "print(\"[INFO] Reading the dataset.\")\n",
        "raw = pd.read_excel(path)\n",
        "\n",
        "data = raw[~raw['m/z'].str.contains('HM')]\n",
        "data = data[~data['m/z'].str.contains('QC')]\n",
        "data = data[~data['m/z'].str.contains('HM')]\n",
        "X = data.drop('m/z', axis=1) # X contains only the features.\n",
        "# y = data['m/z'].apply(lambda x:\n",
        "#                           [1,0,0,0,0,0] if 'Fillet' in x\n",
        "#                     else ([0,1,0,0,0,0] if 'Heads' in x\n",
        "#                     else ([0,0,1,0,0,0] if 'Livers' in x\n",
        "#                     else ([0,0,0,1,0,0] if 'Skins' in x\n",
        "#                     else ([0,0,0,0,1,0] if 'Guts' in x\n",
        "#                     else ([0,0,0,0,0,1] if 'Frames' in x\n",
        "#                     else None ))))))  # Labels for fish parts\n",
        "y = data['m/z'].apply(lambda x:\n",
        "                          0 if 'Fillet' in x\n",
        "                    else  1 if 'Heads' in x\n",
        "                    else (2 if 'Livers' in x\n",
        "                    else (3 if 'Skins' in x\n",
        "                    else (4 if 'Guts' in x\n",
        "                    else (5 if 'Frames' in x\n",
        "                    else None )))))  # For fish parts\n",
        "xs = []\n",
        "ys = []\n",
        "for (x,y) in zip(X.to_numpy(),y):\n",
        "    if y is not None and not np.isnan(y):\n",
        "       xs.append(x)\n",
        "       ys.append(y)\n",
        "X = np.array(xs)\n",
        "y = np.array(ys)\n",
        "\n",
        "# file = load(f'{dataset}.mat', folder=data_path_gdrive)\n",
        "# X,y = prepare(file)\n",
        "# X,_ = normalize(X,X)\n",
        "# y, _, le = encode_labels(y)\n",
        "# labels = le.inverse_transform(np.unique(y))\n",
        "classes, class_counts = np.unique(y, axis=0, return_counts=True)\n",
        "n_features = X.shape[1]\n",
        "n_instances = X.shape[0]\n",
        "n_classes = len(np.unique(y, axis=0))\n",
        "class_ratios = np.array(class_counts) / n_instances\n",
        "\n",
        "print(f\"Class Counts: {class_counts}, Class Ratios: {class_ratios}\")\n",
        "print(f\"Number of features: {n_features}\\nNumber of instances: {n_instances}\\nNumber of classes {n_classes}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eanPjO3KB121"
      },
      "source": [
        "## Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP6upHxeyFX_"
      },
      "source": [
        "## Operators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8AQBTITsyEzE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import copy\n",
        "import random\n",
        "import operator\n",
        "from re import I\n",
        "from operator import attrgetter\n",
        "from functools import wraps, partial\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from deap import algorithms\n",
        "from deap.algorithms import varAnd\n",
        "from deap import base, creator, tools, gp\n",
        "from deap.gp import PrimitiveTree, Primitive, Terminal\n",
        "\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "pset = gp.PrimitiveSet(\"MAIN\", n_features)\n",
        "pset.addPrimitive(operator.add, 2)\n",
        "pset.addPrimitive(operator.sub, 2)\n",
        "pset.addPrimitive(operator.mul, 2)\n",
        "pset.addPrimitive(operator.neg, 1)\n",
        "# pset.addEphemeralConstant(\"rand101\", lambda: random.randint(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kejku7hWyMbO"
      },
      "source": [
        "## Fitness Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "rQE3P3qLyMDr"
      },
      "outputs": [],
      "source": [
        "toolbox = base.Toolbox()\n",
        "\n",
        "minimized = False\n",
        "if minimized:\n",
        "    weight = -1.0\n",
        "else:\n",
        "    weight = 1.0\n",
        "\n",
        "weights = (weight,)\n",
        "\n",
        "if minimized:\n",
        "    creator.create(\"FitnessMin\", base.Fitness, weights=weights)\n",
        "    creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "else:\n",
        "    creator.create(\"FitnessMax\", base.Fitness, weights=weights)\n",
        "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "def quick_evaluate(expr: PrimitiveTree, pset, data, prefix='ARG'):\n",
        "    \"\"\" Quick evaluate offers a 500% speedup for the evluation of GP trees.\n",
        "\n",
        "    The default implementation of gp.compile provided by the DEAP library is\n",
        "    horrendously inefficient. (Zhang 2022) has shared his code which leads to a\n",
        "    5x speedup in the compilation and evaluation of GP trees when compared to the\n",
        "    standard library approach.\n",
        "\n",
        "    For multi-tree GP, this speedup factor is invaluable! As each individual conists\n",
        "    of m trees. For the fish dataset we have 4 classes, each with 3 constructed features,\n",
        "    which corresponds to 4 classes x 3 features = 12 trees for each individual.\n",
        "    12 trees x 500% speedup = 6,000% overall speedup, or 60 times faster.\n",
        "    The 500% speedup is fundamental, for efficient evaluation of multi-tree GP.\n",
        "\n",
        "    Args:\n",
        "        expr (PrimitiveTree): The uncompiled (gp.PrimitiveTree) GP tree.\n",
        "        pset: The primitive set.\n",
        "        data: The dataset to evaluate the GP tree for.\n",
        "        prefix: Prefix for variable arguments. Defaults to ARG.\n",
        "\n",
        "    Returns:\n",
        "        The (array-like) result of the GP tree evaluate on the dataset .\n",
        "    \"\"\"\n",
        "    result = None\n",
        "    stack = []\n",
        "    for node in expr:\n",
        "        stack.append((node, []))\n",
        "        while len(stack[-1][1]) == stack[-1][0].arity:\n",
        "            prim, args = stack.pop()\n",
        "            if isinstance(prim, Primitive):\n",
        "                result = pset.context[prim.name](*args)\n",
        "            elif isinstance(prim, Terminal):\n",
        "                if prefix in prim.name:\n",
        "                    result = data[:, int(prim.name.replace(prefix, ''))]\n",
        "                else:\n",
        "                    result = prim.value\n",
        "            else:\n",
        "                raise Exception\n",
        "            if len(stack) == 0:\n",
        "                break # If stack is empty, all nodes should have been seen\n",
        "            stack[-1][1].append(result)\n",
        "    return result\n",
        "\n",
        "def compileMultiTree(expr, pset):\n",
        "    \"\"\"Compile the expression represented by a list of trees.\n",
        "\n",
        "    A variation of the gp.compileADF method, that handles Multi-tree GP.\n",
        "\n",
        "    Args:\n",
        "        expr: Expression to compile. It can either be a PrimitiveTree,\n",
        "                 a string of Python code or any object that when\n",
        "                 converted into string produced a valid Python code\n",
        "                 expression.\n",
        "        pset: Primitive Set\n",
        "\n",
        "    Returns:\n",
        "        A set of functions that correspond for each tree in the Multi-tree.\n",
        "    \"\"\"\n",
        "    funcs = []\n",
        "    gp_tree = None\n",
        "    func = None\n",
        "\n",
        "    for subexpr in expr:\n",
        "        gp_tree = gp.PrimitiveTree(subexpr)\n",
        "        # 5x speedup by manually parsing GP tree (Zhang 2022) https://mail.google.com/mail/u/0/#inbox/FMfcgzGqQmQthcqPCCNmstgLZlKGXvbc\n",
        "        func = quick_evaluate(gp_tree, pset, X, prefix='ARG')\n",
        "        funcs.append(func)\n",
        "\n",
        "    # Hengzhe's method returns the features in the wrong rotation for multi-tree\n",
        "    features = np.array(funcs).T\n",
        "    return features\n",
        "\n",
        "# MCIFC constructs 8 feautres for a (c=4) multi-class classification problem (Tran 2019).\n",
        "# c - number of classes, r - construction ratio, m - total number of constructed features.\n",
        "# m = r * c = 2 ratio * 4 classes = 8 features\n",
        "\n",
        "r = 3\n",
        "c = n_classes\n",
        "m = r * c\n",
        "\n",
        "toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=2)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.expr, n=m)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"compile\", compileMultiTree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFj8hToTJ09i"
      },
      "source": [
        "## Infinite speedup!?\n",
        "\n",
        "For multi-tree GP, this speedup factor is invaluable! As each individual conists of m trees. For the fish dataset we have 4 classes, each with 3 constructed features, which corresponds to 4 classes x 3 features = 12 trees for each individual. 12 trees x 500% speedup = 6,000% overall speedup, or 60 times faster. The 500% speedup is fundamental, for efficient evaluation of multi-tree GP.\n",
        "\n",
        "The evaluation below shows my calculations may still be wrong. And perhaps, it is even faster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcGYavg1D-77",
        "outputId": "e657a929-fc03-400f-d577-81e758136d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.85 ms ± 759 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "8.4 µs ± 3.67 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
          ]
        }
      ],
      "source": [
        "first = toolbox.population(n=1)[0]\n",
        "\n",
        "subtree = first[0]\n",
        "gp_tree = gp.PrimitiveTree(subtree)\n",
        "\n",
        "%timeit gp.compile(gp_tree, pset)\n",
        "%timeit quick_evaluate(gp_tree, pset, X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "\n",
        "\n",
        "X,_ = normalize(X,X)\n",
        "train_split = 0.8\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, stratify=y, test_size=(1-train_split), random_state=1998)\n",
        "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, stratify=y_temp, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1998)\n",
        "\n",
        "# classes, class_counts = np.unique(y_train, axis=0, return_counts=True)\n",
        "# n_features = X.shape[1]\n",
        "# n_instances = X.shape[0]\n",
        "# n_classes = len(np.unique(y_train, axis=0))\n",
        "# class_ratios = np.array(class_counts) / n_instances\n",
        "# print(f\"Class Counts: {class_counts}, Class Ratios: {class_ratios}\")\n",
        "# print(f\"Number of features: {n_features}\\nNumber of instances: {n_instances}\\nNumber of classes {n_classes}.\")\n",
        "\n",
        "def is_same_class(a,b):\n",
        "    # print(f\"[DEBUG] {a[1], b[1]}\")\n",
        "    return a[1] == b[1]\n",
        "\n",
        "# Euclidean distance\n",
        "def euclidian_distance(a,b):\n",
        "    a,b = a[0], b[0]\n",
        "    return np.linalg.norm(a-b)\n",
        "\n",
        "def intraclass_distance(_X,_y):\n",
        "    data = list(zip(_X, _y))\n",
        "    length = len([1 if is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]])\n",
        "    print(f\"[DEBUG] length: {length}\")\n",
        "    pair_length = sum([1 if is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]])\n",
        "    print(f\"[DEBUG] pair length: {pair_length}\")\n",
        "    return sum([euclidian_distance(a,b) if is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]]) / (pair_length * X.shape[1])\n",
        "\n",
        "def interclass_distance(_X,_y):\n",
        "    data = list(zip(_X, _y))\n",
        "    length = len([1 if not is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]])\n",
        "    print(f\"[DEBUG] length: {length}\")\n",
        "    pair_length = sum([1 if not is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]])\n",
        "    print(f\"[DEBUG] pair length: {pair_length}\")\n",
        "    return sum([euclidian_distance(a,b) if not is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]]) / (pair_length * X.shape[1])\n",
        "\n",
        "for i in range(1):\n",
        "    b = intraclass_distance(X_train, y_train)\n",
        "    print(f\"[DEBUG] intraclass distance: {b}\")\n",
        "    c = interclass_distance(X_train, y_train)\n",
        "    print(f\"[DEBUG] interclass distance: {c}\")"
      ],
      "metadata": {
        "id": "FkymGiDcJmky",
        "outputId": "ae0b032b-2e92-45f7-a2c7-e4c16251931b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] length: 276\n",
            "[DEBUG] pair length: 42\n",
            "[DEBUG] intraclass distance: 0.008907155425521254\n",
            "[DEBUG] length: 276\n",
            "[DEBUG] pair length: 234\n",
            "[DEBUG] interclass distance: 0.011729963180235807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryKFCvmusAlS",
        "outputId": "bbd13cc9-948b-4a54-a313-902c5471fc38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 1.0, Validation accuracy: 0.625, Test accuracy: 0.5\n",
            "Train intra-class: 0.009351435900701907, Val intra-class: 0.008435299933447275\n",
            "Train inter-class: 0.011964009297661593, Val inter-class: 0.010493423399204789\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7502335348431358"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC as svm\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "\n",
        "def is_same_class(a,b):\n",
        "    return a[1] == b[1]\n",
        "\n",
        "# Euclidean distance\n",
        "def euclidian_distance(a,b):\n",
        "    a,b = a[0], b[0]\n",
        "    return np.linalg.norm(a-b)\n",
        "\n",
        "def intraclass_distance(_X,_y):\n",
        "    data = list(zip(_X, _y))\n",
        "    pair_length = sum([1 if is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]])\n",
        "    d = sum([euclidian_distance(a,b) if is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]]) / (pair_length * X.shape[1])\n",
        "    return d\n",
        "\n",
        "def interclass_distance(_X,_y):\n",
        "    data = list(zip(_X, _y))\n",
        "    pair_length = sum([1 if not is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]])\n",
        "    d = sum([euclidian_distance(a,b) if not is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]]) / (pair_length * X.shape[1])\n",
        "    return d\n",
        "\n",
        "def wrapper_classification_accuracy(X, k=10, verbose=False):\n",
        "    \"\"\" Evaluate balanced classification accuracy over stratified k-fold cross validation.\n",
        "\n",
        "    This method is our fitness measure for an individual. We measure each individual\n",
        "    based on its balanced classification accuracy using 10-fold cross-validation on\n",
        "    the training set.\n",
        "\n",
        "    If verbose, we evaluate performance on the test set as well, and print the results\n",
        "    to the standard output. By default, only the train set is evaluated, which\n",
        "    corresponds to a 2x speedup for training, when compared to the verbose method.\n",
        "\n",
        "    Args:\n",
        "        X: entire dataset, train and test.\n",
        "        k: Number of folds, for cross validation. Defaults to 10.\n",
        "        verbose: If true, prints stuff. Defaults to false.\n",
        "\n",
        "    Returns:\n",
        "        Average balanced classification accuracy with 10-fold CV on training set.\n",
        "    \"\"\"\n",
        "\n",
        "    # X_train, X_temp, y_train, y_temp = train_test_split(X, y, stratify=y, test_size=0.7, random_state=42)\n",
        "    # X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, stratify=y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    train_split = 0.66\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, stratify=y, test_size=(1-train_split), random_state=1998)\n",
        "    # X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, stratify=y_temp, test_size=0.5, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1998)\n",
        "\n",
        "    # train_accs = []\n",
        "    # test_accs = []\n",
        "    # skf = StratifiedKFold(n_splits=3)\n",
        "    # for train_idx, test_idx in skf.split(X,y):\n",
        "        # X_train, X_test = X[train_idx], X[test_idx]\n",
        "        # y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # Convergence errors for the fish part dataset.\n",
        "        # Need to use a different SVM hyperparameters for this dataset.\n",
        "        # model = svm(penalty='l2', max_iter=10_000)\n",
        "    model = svm()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_predict = model.predict(X_train)\n",
        "    train_acc = balanced_accuracy_score(y_train, y_predict)\n",
        "    # train_accs.append(train_acc)\n",
        "    # test_accs.append(test_acc)\n",
        "\n",
        "    y_predict = model.predict(X_val)\n",
        "    val_acc = balanced_accuracy_score(y_val, y_predict)\n",
        "\n",
        "    if verbose:\n",
        "        # Only evaluate test set if verbose!\n",
        "        # Results in 2x speedup for training.\n",
        "        y_predict = model.predict(X_test)\n",
        "        test_acc = balanced_accuracy_score(y_test, y_predict)\n",
        "        print(f\"Train accuracy: {train_acc}, Validation accuracy: {val_acc}, Test accuracy: {test_acc}\")\n",
        "\n",
        "    # Distance-based regularization method, intra/inter class distance.\n",
        "    train_intraclass_distance = intraclass_distance(X_train, y_train)\n",
        "    train_interclass_distance = interclass_distance(X_train, y_train)\n",
        "    val_intraclass_distance = intraclass_distance(X_val, y_val)\n",
        "    val_interclass_distance = interclass_distance(X_val, y_val)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Train intra-class: {train_intraclass_distance}, Val intra-class: {val_intraclass_distance}\")\n",
        "        print(f\"Train inter-class: {train_interclass_distance}, Val inter-class: {val_interclass_distance}\")\n",
        "\n",
        "    alpha = 0.5\n",
        "    beta = 0.25\n",
        "    gamma = 0.8\n",
        "    accuracy = alpha * train_acc + (1 - alpha) * val_acc\n",
        "    train_distance = beta * (1 - train_intraclass_distance) + beta * train_interclass_distance\n",
        "    val_distance = beta * (1 - val_intraclass_distance) + beta * val_interclass_distance\n",
        "    total_distance = train_distance + val_distance\n",
        "    fitness = gamma * accuracy + (1 - gamma) * total_distance\n",
        "    # Fitness value must be a tuple.\n",
        "    assert fitness <= 1, f\"fitness {fitness} should be normalized, and cannot exceed 1\"\n",
        "    if fitness > 1:\n",
        "        print(f\"Train intra-class: {train_intraclass_distance}, Val intra-class: {val_intraclass_distance}\")\n",
        "        print(f\"Train inter-class: {train_interclass_distance}, Val inter-class: {val_interclass_distance}\")\n",
        "    return fitness\n",
        "\n",
        "wrapper_classification_accuracy(X, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vOz-1Oislgd",
        "outputId": "c4e4eca4-e182-4efd-8fbd-bb4192bad823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   \t      \t                                 fitness                                 \t                      size                      \n",
            "   \t      \t-------------------------------------------------------------------------\t------------------------------------------------\n",
            "gen\tnevals\tavg     \tgen\tmax     \tmin     \tnevals\tstd      \tavg    \tgen\tmax\tmin\tnevals\tstd     \n",
            "0  \t1023  \t0.606989\t0  \t0.900033\t0.433351\t1023  \t0.0827613\t6.79081\t0  \t7  \t3  \t1023  \t0.592585\n",
            "1  \t758   \t0.712961\t1  \t0.900034\t0.483353\t758   \t0.0754621\t6.86804\t1  \t13 \t3  \t758   \t0.811203\n",
            "2  \t758   \t0.787039\t2  \t0.900072\t0.516689\t758   \t0.0718059\t6.91007\t2  \t13 \t4  \t758   \t0.903846\n",
            "3  \t786   \t0.837735\t3  \t0.900072\t0.6167  \t786   \t0.0562504\t6.90127\t3  \t13 \t4  \t786   \t0.721834\n",
            "4  \t773   \t0.851711\t4  \t0.900077\t0.483364\t773   \t0.0608073\t6.9306 \t4  \t13 \t6  \t773   \t0.762047\n",
            "5  \t752   \t0.870983\t5  \t0.900087\t0.616749\t752   \t0.0514452\t7.02444\t5  \t12 \t5  \t752   \t0.702174\n",
            "6  \t742   \t0.882327\t6  \t0.900091\t0.600061\t742   \t0.0402569\t7.1828 \t6  \t13 \t6  \t742   \t0.731111\n",
            "7  \t776   \t0.887567\t7  \t0.900101\t0.650047\t776   \t0.0363087\t7.4956 \t7  \t13 \t7  \t776   \t1.10173 \n",
            "8  \t786   \t0.887135\t8  \t0.900108\t0.650087\t786   \t0.0367013\t7.55132\t8  \t13 \t7  \t786   \t0.967232\n",
            "9  \t772   \t0.889043\t9  \t0.900118\t0.600087\t772   \t0.0332131\t7.79472\t9  \t13 \t7  \t772   \t1.15508 \n",
            "10 \t763   \t0.891852\t10 \t0.900127\t0.716759\t763   \t0.0279145\t8.10459\t10 \t15 \t7  \t763   \t1.55631 \n",
            "11 \t783   \t0.890848\t11 \t0.900127\t0.650087\t783   \t0.0296177\t9.23069\t11 \t17 \t7  \t783   \t2.38838 \n",
            "12 \t797   \t0.889182\t12 \t0.900142\t0.650105\t797   \t0.0325107\t10.435 \t12 \t21 \t7  \t797   \t2.61386 \n",
            "13 \t779   \t0.892366\t13 \t0.900142\t0.700122\t779   \t0.0263039\t11.6481\t13 \t19 \t7  \t779   \t2.45971 \n",
            "14 \t793   \t0.893137\t14 \t0.900153\t0.75011 \t793   \t0.0237957\t12.5611\t14 \t21 \t7  \t793   \t1.69985 \n",
            "15 \t811   \t0.893415\t15 \t0.900159\t0.700113\t811   \t0.0256495\t12.8993\t15 \t23 \t7  \t811   \t1.87086 \n",
            "16 \t792   \t0.8935  \t16 \t0.900169\t0.750126\t792   \t0.0234335\t13.563 \t16 \t23 \t7  \t792   \t2.19964 \n",
            "17 \t759   \t0.894724\t17 \t0.900175\t0.716807\t759   \t0.0224269\t13.7859\t17 \t29 \t7  \t759   \t2.34471 \n",
            "18 \t786   \t0.893955\t18 \t0.900188\t0.683482\t786   \t0.0232698\t14.2737\t18 \t29 \t7  \t786   \t2.89493 \n",
            "19 \t776   \t0.895022\t19 \t0.900224\t0.700137\t776   \t0.0215821\t14.9169\t19 \t33 \t7  \t776   \t3.53642 \n",
            "20 \t779   \t0.895771\t20 \t0.900238\t0.750162\t779   \t0.0197436\t14.8416\t20 \t33 \t7  \t779   \t3.62412 \n",
            "21 \t788   \t0.893818\t21 \t0.900238\t0.733493\t788   \t0.0235935\t15.7986\t21 \t33 \t7  \t788   \t4.10856 \n",
            "22 \t756   \t0.893292\t22 \t0.900277\t0.700193\t756   \t0.0252867\t15.5885\t22 \t37 \t7  \t756   \t4.0236  \n",
            "23 \t802   \t0.893482\t23 \t0.900287\t0.700228\t802   \t0.0258704\t15.9951\t23 \t37 \t9  \t802   \t3.96921 \n",
            "24 \t752   \t0.893064\t24 \t0.900307\t0.700226\t752   \t0.0256875\t15.7292\t24 \t29 \t11 \t752   \t4.57393 \n",
            "25 \t761   \t0.891798\t25 \t0.900319\t0.716908\t761   \t0.0281516\t19.4536\t25 \t35 \t11 \t761   \t6.60027 \n",
            "26 \t778   \t0.893469\t26 \t0.900351\t0.650264\t778   \t0.0262273\t17.3783\t26 \t35 \t11 \t778   \t6.09766 \n",
            "27 \t773   \t0.889851\t27 \t0.900437\t0.683609\t773   \t0.0311154\t15.563 \t27 \t35 \t11 \t773   \t4.68329 \n",
            "28 \t769   \t0.890305\t28 \t0.900441\t0.683623\t769   \t0.0314482\t15.3216\t28 \t35 \t13 \t769   \t3.14293 \n",
            "29 \t783   \t0.88785 \t29 \t0.900441\t0.733648\t783   \t0.0345574\t16.9335\t29 \t29 \t13 \t783   \t2.84419 \n",
            "30 \t772   \t0.890647\t30 \t0.900511\t0.700365\t772   \t0.0313467\t18.5865\t30 \t27 \t13 \t772   \t2.2723  \n",
            "31 \t776   \t0.889628\t31 \t0.900636\t0.70044 \t776   \t0.0320688\t20.4233\t31 \t33 \t13 \t776   \t2.23229 \n",
            "32 \t784   \t0.889247\t32 \t0.900636\t0.700432\t784   \t0.0323801\t21.827 \t32 \t33 \t13 \t784   \t2.46746 \n",
            "33 \t782   \t0.890527\t33 \t0.900695\t0.683843\t782   \t0.031828 \t24.871 \t33 \t35 \t13 \t782   \t3.83391 \n",
            "34 \t748   \t0.888256\t34 \t0.900862\t0.683844\t748   \t0.0348224\t29.7478\t34 \t45 \t15 \t748   \t2.79661 \n",
            "35 \t766   \t0.889171\t35 \t0.9009  \t0.683946\t766   \t0.0332101\t31.7986\t35 \t45 \t13 \t766   \t2.71279 \n",
            "36 \t787   \t0.88767 \t36 \t0.9009  \t0.650787\t787   \t0.0364694\t34.0166\t36 \t47 \t13 \t787   \t4.50293 \n",
            "37 \t795   \t0.887094\t37 \t0.900992\t0.650689\t795   \t0.0362884\t40.3421\t37 \t55 \t19 \t795   \t4.61036 \n",
            "38 \t780   \t0.888422\t38 \t0.900992\t0.700817\t780   \t0.0346906\t44.5601\t38 \t60 \t27 \t780   \t1.45578 \n",
            "39 \t788   \t0.88702 \t39 \t0.900995\t0.617551\t788   \t0.0372935\t45.2141\t39 \t55 \t13 \t788   \t2.30307 \n",
            "40 \t748   \t0.88887 \t40 \t0.901021\t0.700901\t748   \t0.033173 \t46.4506\t40 \t58 \t25 \t748   \t3.41282 \n",
            "41 \t779   \t0.885811\t41 \t0.901068\t0.667641\t779   \t0.0379957\t50.61  \t41 \t69 \t23 \t779   \t4.6795  \n",
            "42 \t801   \t0.882413\t42 \t0.901069\t0.667599\t801   \t0.0414583\t54.7674\t42 \t63 \t23 \t801   \t1.96813 \n",
            "43 \t762   \t0.883911\t43 \t0.901149\t0.634387\t762   \t0.0397781\t55.1818\t43 \t63 \t25 \t762   \t2.28476 \n",
            "44 \t779   \t0.883995\t44 \t0.901149\t0.651012\t779   \t0.0399519\t56.5288\t44 \t63 \t39 \t779   \t2.45724 \n",
            "45 \t758   \t0.883532\t45 \t0.901214\t0.701019\t758   \t0.0403902\t58.8739\t45 \t68 \t24 \t758   \t2.29128 \n",
            "46 \t793   \t0.882962\t46 \t0.901398\t0.651116\t793   \t0.0428476\t60.9296\t46 \t79 \t13 \t793   \t2.89621 \n",
            "47 \t797   \t0.883182\t47 \t0.901399\t0.662234\t797   \t0.043291 \t62.9814\t47 \t79 \t43 \t797   \t2.29212 \n",
            "48 \t770   \t0.880372\t48 \t0.901406\t0.651151\t770   \t0.0470133\t64.4594\t48 \t79 \t43 \t770   \t4.39609 \n",
            "49 \t782   \t0.879768\t49 \t0.901718\t0.634727\t782   \t0.045731 \t69.2659\t49 \t97 \t23 \t782   \t6.92169 \n",
            "50 \t790   \t0.883699\t50 \t0.901718\t0.634723\t790   \t0.0425988\t77.6452\t50 \t97 \t53 \t790   \t4.47347 \n",
            "51 \t753   \t0.886741\t51 \t0.901719\t0.651398\t753   \t0.0393801\t79.9785\t51 \t97 \t15 \t753   \t4.99946 \n",
            "52 \t751   \t0.885931\t52 \t0.901868\t0.618381\t751   \t0.0393549\t83.4946\t52 \t103\t15 \t751   \t7.81417 \n",
            "53 \t771   \t0.883626\t53 \t0.901869\t0.651713\t771   \t0.0441913\t93.4428\t53 \t105\t53 \t771   \t6.82245 \n",
            "54 \t766   \t0.88571 \t54 \t0.901874\t0.668384\t766   \t0.0420703\t97.176 \t54 \t105\t41 \t766   \t2.85086 \n",
            "55 \t763   \t0.881734\t55 \t0.901876\t0.535093\t763   \t0.0470446\t98.3284\t55 \t108\t49 \t763   \t3.52725 \n",
            "56 \t749   \t0.879775\t56 \t0.902025\t0.66843 \t749   \t0.0484897\t101.305\t56 \t111\t67 \t749   \t2.76955 \n",
            "57 \t774   \t0.879775\t57 \t0.902025\t0.668533\t774   \t0.047767 \t103.05 \t57 \t111\t71 \t774   \t1.75433 \n",
            "58 \t753   \t0.878634\t58 \t0.902026\t0.635195\t753   \t0.0504155\t103.651\t58 \t111\t61 \t753   \t2.65339 \n",
            "59 \t761   \t0.877971\t59 \t0.902034\t0.635279\t761   \t0.0493591\t105.386\t59 \t115\t77 \t761   \t3.22904 \n",
            "60 \t770   \t0.88211 \t60 \t0.902068\t0.635275\t770   \t0.0451026\t108.951\t60 \t116\t47 \t770   \t3.95917 \n",
            "61 \t790   \t0.884982\t61 \t0.902068\t0.585353\t790   \t0.0416162\t110.852\t61 \t113\t89 \t790   \t1.35011 \n",
            "62 \t771   \t0.884419\t62 \t0.902112\t0.668691\t771   \t0.0425587\t110.97 \t62 \t117\t57 \t771   \t2.12007 \n",
            "63 \t772   \t0.881625\t63 \t0.902151\t0.668703\t772   \t0.0437227\t111.268\t63 \t125\t79 \t772   \t1.96962 \n",
            "64 \t787   \t0.881873\t64 \t0.902153\t0.652075\t787   \t0.0438551\t112.402\t64 \t119\t95 \t787   \t1.54308 \n",
            "65 \t769   \t0.878704\t65 \t0.902153\t0.668735\t769   \t0.0460818\t113.57 \t65 \t120\t59 \t769   \t2.46317 \n",
            "66 \t763   \t0.885012\t66 \t0.902191\t0.668778\t763   \t0.0394249\t115.251\t66 \t119\t19 \t763   \t4.06983 \n",
            "67 \t790   \t0.881251\t67 \t0.902231\t0.668818\t790   \t0.0439242\t116.818\t67 \t122\t65 \t790   \t1.99956 \n",
            "68 \t757   \t0.881628\t68 \t0.902231\t0.535486\t757   \t0.0455351\t117.044\t68 \t125\t103\t757   \t1.08802 \n",
            "69 \t795   \t0.882617\t69 \t0.902232\t0.652228\t795   \t0.0433022\t117.323\t69 \t126\t19 \t795   \t3.9237  \n",
            "70 \t771   \t0.880294\t70 \t0.902234\t0.635503\t771   \t0.0464142\t118.633\t70 \t126\t59 \t771   \t2.77919 \n",
            "71 \t769   \t0.884513\t71 \t0.902263\t0.652195\t769   \t0.0436164\t120.063\t71 \t126\t69 \t769   \t2.20459 \n",
            "72 \t760   \t0.880066\t72 \t0.902311\t0.568899\t760   \t0.0509396\t120.843\t72 \t125\t93 \t760   \t1.50518 \n",
            "73 \t755   \t0.873832\t73 \t0.902475\t0.568896\t755   \t0.0583748\t120.956\t73 \t135\t95 \t755   \t1.86783 \n",
            "74 \t759   \t0.871656\t74 \t0.902475\t0.568906\t759   \t0.0611976\t121.496\t74 \t135\t91 \t759   \t2.68002 \n",
            "75 \t770   \t0.876813\t75 \t0.902483\t0.568935\t770   \t0.0567698\t123.29 \t75 \t135\t83 \t770   \t3.19255 \n",
            "76 \t744   \t0.880115\t76 \t0.902484\t0.518968\t744   \t0.0543416\t126.599\t76 \t135\t69 \t744   \t5.18435 \n",
            "77 \t762   \t0.87965 \t77 \t0.902546\t0.602474\t762   \t0.0510573\t131.868\t77 \t145\t69 \t762   \t5.11617 \n",
            "78 \t772   \t0.874112\t78 \t0.90256 \t0.530253\t772   \t0.0618603\t134.714\t78 \t141\t69 \t772   \t3.38721 \n",
            "79 \t770   \t0.873329\t79 \t0.902595\t0.530253\t770   \t0.0635243\t135.046\t79 \t141\t69 \t770   \t3.19929 \n",
            "80 \t750   \t0.876826\t80 \t0.902595\t0.569156\t750   \t0.0537596\t136.295\t80 \t143\t71 \t750   \t3.26611 \n",
            "81 \t778   \t0.879833\t81 \t0.902616\t0.535885\t778   \t0.0549249\t137.984\t81 \t147\t20 \t778   \t4.1194  \n",
            "82 \t759   \t0.882448\t82 \t0.902666\t0.569223\t759   \t0.0479478\t139.399\t82 \t143\t109\t759   \t2.31896 \n",
            "83 \t768   \t0.887259\t83 \t0.902681\t0.602588\t768   \t0.0400681\t140.551\t83 \t149\t71 \t768   \t2.98531 \n",
            "84 \t759   \t0.886956\t84 \t0.90269 \t0.669266\t759   \t0.0392756\t140.677\t84 \t147\t105\t759   \t2.22655 \n",
            "85 \t788   \t0.884487\t85 \t0.902783\t0.602662\t788   \t0.0451666\t140.589\t85 \t151\t111\t788   \t2.64881 \n",
            "86 \t792   \t0.88875 \t86 \t0.902792\t0.602681\t792   \t0.0381771\t143.382\t86 \t151\t107\t792   \t2.8242  \n",
            "87 \t784   \t0.886004\t87 \t0.902792\t0.552681\t784   \t0.0439675\t145.878\t87 \t151\t76 \t784   \t3.58089 \n",
            "88 \t768   \t0.886386\t88 \t0.902841\t0.619345\t768   \t0.0401918\t147.259\t88 \t154\t73 \t768   \t4.0614  \n",
            "89 \t791   \t0.888603\t89 \t0.902841\t0.61945 \t791   \t0.0381684\t150.325\t89 \t155\t79 \t791   \t3.57496 \n",
            "90 \t760   \t0.885509\t90 \t0.902843\t0.497218\t760   \t0.0415756\t150.781\t90 \t155\t109\t760   \t2.42562 \n",
            "91 \t739   \t0.888213\t91 \t0.90287 \t0.619457\t739   \t0.0396301\t150.717\t91 \t156\t113\t739   \t2.8494  \n",
            "92 \t765   \t0.889393\t92 \t0.90287 \t0.65282 \t765   \t0.0348475\t150.007\t92 \t155\t113\t765   \t2.55859 \n",
            "93 \t786   \t0.887865\t93 \t0.90292 \t0.619506\t786   \t0.038537 \t149.007\t93 \t155\t21 \t786   \t4.56226 \n",
            "94 \t771   \t0.885295\t94 \t0.903127\t0.569508\t771   \t0.0436855\t149.807\t94 \t163\t75 \t771   \t3.30423 \n",
            "95 \t770   \t0.885206\t95 \t0.903127\t0.619518\t770   \t0.0429493\t149.933\t95 \t163\t115\t770   \t2.69831 \n",
            "96 \t728   \t0.889163\t96 \t0.903166\t0.586303\t728   \t0.0358324\t151.696\t96 \t165\t127\t728   \t3.4729  \n",
            "97 \t767   \t0.887085\t97 \t0.903243\t0.530736\t767   \t0.0402313\t155.756\t97 \t169\t93 \t767   \t5.10198 \n",
            "98 \t776   \t0.885272\t98 \t0.903243\t0.669677\t776   \t0.0424788\t161.357\t98 \t169\t85 \t776   \t4.33932 \n",
            "99 \t765   \t0.887462\t99 \t0.903243\t0.669677\t765   \t0.0404786\t163.575\t99 \t169\t83 \t765   \t3.08308 \n",
            "100\t766   \t0.885098\t100\t0.903245\t0.603163\t766   \t0.0454761\t165.262\t100\t169\t91 \t766   \t3.76637 \n",
            "101\t780   \t0.885665\t101\t0.90325 \t0.619899\t780   \t0.0426317\t167.413\t101\t175\t141\t780   \t2.46618 \n",
            "102\t773   \t0.885078\t102\t0.903282\t0.519908\t773   \t0.0456156\t168.767\t102\t171\t133\t773   \t2.24689 \n",
            "103\t775   \t0.880812\t103\t0.903288\t0.603246\t775   \t0.0513002\t168.844\t103\t171\t115\t775   \t2.77099 \n",
            "104\t767   \t0.882673\t104\t0.903288\t0.603246\t767   \t0.0459822\t168.934\t104\t174\t89 \t767   \t2.94815 \n",
            "105\t769   \t0.879383\t105\t0.903333\t0.603247\t769   \t0.0515869\t169.489\t105\t173\t137\t769   \t1.76299 \n",
            "106\t773   \t0.883541\t106\t0.903354\t0.581061\t773   \t0.0449893\t170.627\t106\t178\t149\t773   \t1.34817 \n",
            "107\t780   \t0.87858 \t107\t0.903358\t0.569955\t780   \t0.0520768\t171.159\t107\t177\t117\t780   \t2.38745 \n",
            "108\t767   \t0.881545\t108\t0.903521\t0.519954\t767   \t0.0495585\t172.058\t108\t183\t105\t767   \t3.03444 \n",
            "109\t783   \t0.877511\t109\t0.903521\t0.603351\t783   \t0.052675 \t173.429\t109\t183\t121\t783   \t2.47541 \n",
            "110\t779   \t0.876768\t110\t0.903522\t0.569996\t779   \t0.0543461\t174.665\t110\t185\t95 \t779   \t4.09849 \n",
            "111\t767   \t0.876536\t111\t0.903524\t0.620019\t767   \t0.0533455\t176.007\t111\t183\t89 \t767   \t5.17806 \n",
            "112\t782   \t0.881323\t112\t0.903599\t0.570027\t782   \t0.0500696\t179.381\t112\t187\t87 \t782   \t4.56915 \n",
            "113\t785   \t0.878876\t113\t0.903638\t0.520186\t785   \t0.0519204\t182.782\t113\t189\t155\t785   \t1.65924 \n",
            "114\t787   \t0.879517\t114\t0.903638\t0.603522\t787   \t0.0507628\t183.139\t114\t189\t135\t787   \t2.33179 \n",
            "115\t788   \t0.873021\t115\t0.903716\t0.520265\t788   \t0.0589946\t183.935\t115\t193\t91 \t788   \t3.62146 \n",
            "116\t786   \t0.871693\t116\t0.903716\t0.520227\t786   \t0.0619149\t185.481\t116\t193\t133\t786   \t3.09153 \n",
            "117\t787   \t0.862865\t117\t0.903716\t0.520263\t787   \t0.0667241\t187.087\t117\t193\t101\t787   \t4.0143  \n",
            "118\t783   \t0.868827\t118\t0.90373 \t0.570265\t783   \t0.0610684\t188.903\t118\t197\t175\t783   \t2.11623 \n",
            "119\t796   \t0.874743\t119\t0.903794\t0.570212\t796   \t0.0555128\t190.864\t119\t199\t142\t796   \t2.72746 \n",
            "120\t771   \t0.8821  \t120\t0.903794\t0.537047\t771   \t0.0475381\t192.204\t120\t197\t155\t771   \t1.92993 \n",
            "121\t753   \t0.874952\t121\t0.903794\t0.620393\t753   \t0.0505449\t192.345\t121\t199\t45 \t753   \t5.88512 \n",
            "122\t759   \t0.873171\t122\t0.903911\t0.620408\t759   \t0.0522242\t192.938\t122\t201\t167\t759   \t2.6317  \n",
            "123\t766   \t0.875908\t123\t0.90395 \t0.620459\t766   \t0.0520715\t195.378\t123\t201\t165\t766   \t2.71245 \n",
            "124\t775   \t0.877194\t124\t0.903951\t0.603795\t775   \t0.0515678\t197.079\t124\t205\t131\t775   \t2.89277 \n",
            "125\t780   \t0.872269\t125\t0.903951\t0.570462\t780   \t0.0542245\t198.071\t125\t205\t105\t780   \t4.12438 \n",
            "126\t779   \t0.867181\t126\t0.903951\t0.53722 \t779   \t0.0556527\t199.87 \t126\t207\t115\t779   \t4.38425 \n",
            "127\t784   \t0.861833\t127\t0.903989\t0.637281\t784   \t0.0559756\t201.161\t127\t205\t101\t784   \t3.99931 \n",
            "128\t768   \t0.867824\t128\t0.903989\t0.620625\t768   \t0.0549099\t203.194\t128\t205\t99 \t768   \t4.03863 \n",
            "129\t780   \t0.871451\t129\t0.904029\t0.62063 \t780   \t0.0530738\t204.427\t129\t209\t139\t780   \t3.01225 \n",
            "130\t774   \t0.86908 \t130\t0.90404 \t0.62064 \t774   \t0.0549414\t204.465\t130\t213\t151\t774   \t2.35395 \n",
            "131\t724   \t0.870683\t131\t0.90404 \t0.570619\t724   \t0.0549764\t204.188\t131\t209\t109\t724   \t3.83965 \n",
            "132\t767   \t0.863551\t132\t0.904062\t0.570657\t767   \t0.0565969\t203.845\t132\t211\t95 \t767   \t5.28154 \n",
            "133\t782   \t0.8608  \t133\t0.904062\t0.520645\t782   \t0.0591396\t205.698\t133\t212\t197\t782   \t2.81729 \n",
            "134\t763   \t0.869539\t134\t0.904077\t0.620706\t763   \t0.0555156\t208.242\t134\t217\t193\t763   \t1.95198 \n",
            "135\t780   \t0.874577\t135\t0.904101\t0.604052\t780   \t0.0527198\t208.871\t135\t211\t191\t780   \t1.0871  \n",
            "136\t799   \t0.873346\t136\t0.904139\t0.6707  \t799   \t0.0503454\t208.935\t136\t213\t182\t799   \t1.07791 \n",
            "137\t761   \t0.876981\t137\t0.90414 \t0.520719\t761   \t0.0501982\t208.962\t137\t213\t157\t761   \t2.07661 \n",
            "138\t769   \t0.874514\t138\t0.904177\t0.604072\t769   \t0.0517109\t209.267\t138\t215\t99 \t769   \t6.08189 \n",
            "139\t765   \t0.872563\t139\t0.904334\t0.565186\t765   \t0.0524156\t211.054\t139\t223\t187\t765   \t2.37493 \n",
            "140\t780   \t0.874927\t140\t0.904334\t0.620803\t780   \t0.0506637\t212.74 \t140\t223\t183\t780   \t1.8886  \n",
            "141\t771   \t0.876672\t141\t0.904334\t0.587471\t771   \t0.0497192\t213.307\t141\t223\t103\t771   \t3.93441 \n",
            "142\t775   \t0.871806\t142\t0.904336\t0.620805\t775   \t0.0531501\t214.925\t142\t223\t177\t775   \t3.18403 \n",
            "143\t788   \t0.873347\t143\t0.90435 \t0.570849\t788   \t0.0530991\t218.021\t143\t225\t207\t788   \t3.93999 \n",
            "144\t787   \t0.876018\t144\t0.904352\t0.620926\t787   \t0.0502027\t221.957\t144\t225\t189\t787   \t2.94593 \n",
            "145\t751   \t0.882951\t145\t0.904412\t0.621001\t751   \t0.044102 \t222.901\t145\t227\t197\t751   \t1.33639 \n",
            "146\t813   \t0.88116 \t146\t0.904508\t0.64878 \t813   \t0.0448199\t222.893\t146\t233\t149\t813   \t3.53165 \n",
            "147\t789   \t0.87479 \t147\t0.904508\t0.621015\t789   \t0.0513543\t223.598\t147\t233\t191\t789   \t2.0701  \n",
            "148\t762   \t0.876938\t148\t0.904529\t0.49879 \t762   \t0.0497568\t225.077\t148\t233\t147\t762   \t3.91389 \n",
            "149\t756   \t0.877654\t149\t0.904547\t0.620822\t756   \t0.0478661\t228.298\t149\t235\t193\t756   \t3.56809 \n",
            "150\t773   \t0.881715\t150\t0.904568\t0.587838\t773   \t0.0434473\t231.798\t150\t239\t135\t773   \t3.89263 \n",
            "151\t753   \t0.883826\t151\t0.904573\t0.554505\t753   \t0.0432473\t232.984\t151\t235\t215\t753   \t1.05274 \n",
            "152\t766   \t0.881849\t152\t0.904586\t0.582345\t766   \t0.0449619\t233.147\t152\t237\t115\t766   \t4.24172 \n",
            "153\t762   \t0.881649\t153\t0.904597\t0.565655\t762   \t0.0432418\t234.09 \t153\t239\t121\t762   \t4.03403 \n",
            "154\t758   \t0.877007\t154\t0.904607\t0.532324\t758   \t0.0501517\t234.969\t154\t239\t207\t758   \t1.82279 \n",
            "155\t761   \t0.878803\t155\t0.904768\t0.654566\t761   \t0.045886 \t235.499\t155\t245\t219\t761   \t1.38902 \n",
            "156\t772   \t0.878038\t156\t0.904769\t0.621251\t772   \t0.0477799\t236.507\t156\t245\t173\t772   \t2.80338 \n",
            "157\t755   \t0.880703\t157\t0.904769\t0.654584\t755   \t0.0441937\t236.761\t157\t245\t205\t755   \t2.15398 \n",
            "158\t765   \t0.873212\t158\t0.904775\t0.632546\t765   \t0.050115 \t237.418\t158\t247\t121\t765   \t5.36789 \n",
            "159\t781   \t0.869945\t159\t0.904777\t0.482384\t781   \t0.053705 \t240.084\t159\t247\t187\t781   \t4.49337 \n",
            "160\t758   \t0.869691\t160\t0.90478 \t0.588106\t758   \t0.052529 \t243.551\t160\t247\t187\t758   \t3.80372 \n",
            "161\t802   \t0.872285\t161\t0.904822\t0.604775\t802   \t0.0510183\t245.298\t161\t249\t217\t802   \t1.95353 \n",
            "162\t752   \t0.875796\t162\t0.904822\t0.571443\t752   \t0.051013 \t246.145\t162\t249\t181\t752   \t2.94634 \n",
            "163\t752   \t0.876961\t163\t0.904822\t0.554773\t752   \t0.0497058\t246.418\t163\t250\t133\t752   \t4.47377 \n",
            "164\t774   \t0.873494\t164\t0.904842\t0.582558\t774   \t0.0517582\t246.359\t164\t255\t133\t774   \t6.5394  \n",
            "165\t785   \t0.869719\t165\t0.904861\t0.565899\t785   \t0.0533556\t247.295\t165\t251\t195\t785   \t2.60589 \n",
            "166\t773   \t0.869638\t166\t0.904861\t0.549266\t773   \t0.0541847\t248.244\t166\t251\t213\t773   \t1.80607 \n",
            "167\t759   \t0.869035\t167\t0.904881\t0.532599\t759   \t0.0557914\t248.63 \t167\t253\t121\t759   \t7.73993 \n",
            "168\t760   \t0.871103\t168\t0.904939\t0.532619\t760   \t0.0558551\t250.099\t168\t255\t213\t760   \t1.94022 \n",
            "169\t771   \t0.876247\t169\t0.904939\t0.538175\t771   \t0.0490434\t250.788\t169\t261\t189\t771   \t2.52475 \n",
            "170\t781   \t0.874441\t170\t0.904939\t0.549302\t781   \t0.0538126\t251.096\t170\t255\t183\t781   \t3.23269 \n",
            "171\t773   \t0.871499\t171\t0.90494 \t0.565976\t773   \t0.0563055\t252.087\t171\t258\t173\t773   \t3.22158 \n",
            "172\t781   \t0.868722\t172\t0.904959\t0.599324\t781   \t0.0568638\t253.578\t172\t257\t241\t781   \t1.39107 \n",
            "173\t776   \t0.868011\t173\t0.904978\t0.616051\t776   \t0.0542007\t254.39 \t173\t259\t173\t776   \t3.99772 \n",
            "174\t762   \t0.872151\t174\t0.90498 \t0.532716\t762   \t0.0537129\t254.911\t174\t258\t201\t762   \t2.57116 \n",
            "175\t783   \t0.87283 \t175\t0.904998\t0.51607 \t783   \t0.0558556\t255.202\t175\t259\t143\t783   \t4.60429 \n",
            "176\t808   \t0.871159\t176\t0.904998\t0.566021\t808   \t0.0556815\t256.148\t176\t259\t191\t808   \t2.94371 \n",
            "177\t791   \t0.868225\t177\t0.905038\t0.504959\t791   \t0.0555642\t256.543\t177\t263\t121\t791   \t4.40694 \n",
            "178\t766   \t0.870847\t178\t0.905038\t0.604978\t766   \t0.0525006\t256.88 \t178\t261\t226\t766   \t1.80534 \n",
            "179\t772   \t0.869975\t179\t0.905038\t0.482776\t772   \t0.0582884\t257.491\t179\t261\t179\t772   \t3.56803 \n",
            "180\t773   \t0.87108 \t180\t0.905054\t0.621683\t773   \t0.0535365\t258.581\t180\t267\t121\t773   \t4.55992 \n",
            "181\t780   \t0.868114\t181\t0.905055\t0.621663\t780   \t0.0535531\t259.061\t181\t263\t129\t780   \t4.34804 \n",
            "182\t766   \t0.86718 \t182\t0.905058\t0.549454\t766   \t0.0549925\t259.784\t182\t263\t223\t766   \t2.44215 \n",
            "183\t790   \t0.866098\t183\t0.905058\t0.466149\t790   \t0.0557338\t260.74 \t183\t263\t238\t790   \t1.44391 \n",
            "184\t788   \t0.864436\t184\t0.905061\t0.549482\t788   \t0.0564601\t260.939\t184\t265\t247\t788   \t0.692257\n",
            "185\t772   \t0.86251 \t185\t0.905066\t0.532823\t772   \t0.0571822\t260.694\t185\t267\t145\t772   \t4.0855  \n",
            "186\t778   \t0.861225\t186\t0.905097\t0.588389\t778   \t0.0578466\t260.802\t186\t263\t181\t778   \t3.06507 \n",
            "187\t795   \t0.860046\t187\t0.905097\t0.516168\t795   \t0.0575595\t260.903\t187\t263\t223\t795   \t1.43664 \n",
            "188\t795   \t0.860061\t188\t0.905107\t0.56617 \t795   \t0.0557901\t261.002\t188\t263\t239\t795   \t1.15216 \n",
            "189\t764   \t0.862856\t189\t0.905135\t0.605062\t764   \t0.0542947\t260.956\t189\t265\t125\t764   \t5.17485 \n",
            "190\t774   \t0.863126\t190\t0.905135\t0.638396\t774   \t0.0531594\t261.644\t190\t265\t217\t774   \t2.85123 \n",
            "191\t758   \t0.859942\t191\t0.905135\t0.571763\t758   \t0.0564014\t262.584\t191\t266\t233\t758   \t1.33111 \n",
            "192\t775   \t0.862446\t192\t0.905135\t0.632875\t775   \t0.0523775\t262.68 \t192\t273\t141\t775   \t4.463   \n",
            "193\t773   \t0.861463\t193\t0.905136\t0.58844 \t773   \t0.0535218\t262.338\t193\t265\t125\t773   \t7.97926 \n",
            "194\t795   \t0.861263\t194\t0.905149\t0.499551\t795   \t0.0554515\t262.787\t194\t265\t143\t795   \t3.87457 \n",
            "195\t757   \t0.863266\t195\t0.905149\t0.555129\t757   \t0.0564001\t262.9  \t195\t265\t241\t757   \t1.19048 \n",
            "196\t744   \t0.863138\t196\t0.905214\t0.571799\t744   \t0.0542923\t263.023\t196\t267\t253\t744   \t0.770571\n",
            "197\t771   \t0.862808\t197\t0.905214\t0.64959 \t771   \t0.0516086\t262.854\t197\t269\t125\t771   \t6.64498 \n",
            "198\t766   \t0.860818\t198\t0.905214\t0.649385\t766   \t0.0544153\t263.522\t198\t269\t145\t766   \t3.92292 \n",
            "199\t782   \t0.8623  \t199\t0.905216\t0.466256\t782   \t0.0554307\t264.128\t199\t279\t225\t782   \t2.10716 \n",
            "200\t776   \t0.859901\t200\t0.905227\t0.57181 \t776   \t0.0567523\t264.444\t200\t271\t229\t776   \t2.25642 \n"
          ]
        }
      ],
      "source": [
        "#  DEBUG : REMOVE THIS !!!\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def xmate(ind1, ind2):\n",
        "    \"\"\" Reproduction operator for multi-tree GP, where trees are represented as a list.\n",
        "\n",
        "    Crossover happens to a subtree that is selected at random.\n",
        "    Crossover operations are limited to parents from the same tree.\n",
        "\n",
        "    FIXME: Have to compile the trees (manually), which is frustrating.\n",
        "\n",
        "    Args:\n",
        "        ind1 (Individual): The first parent.\n",
        "        ind2 (Individual): The second parent\n",
        "\n",
        "    Returns:\n",
        "        ind1, ind2 (Individual, Individual): The children from the parents reproduction.\n",
        "    \"\"\"\n",
        "    n = range(len(ind1))\n",
        "    selected_tree_idx = random.choice(n)\n",
        "    for tree_idx in n:\n",
        "        g1, g2 = gp.PrimitiveTree(ind1[tree_idx]), gp.PrimitiveTree(ind2[tree_idx])\n",
        "        if tree_idx == selected_tree_idx:\n",
        "            ind1[tree_idx], ind2[tree_idx] = gp.cxOnePoint(g1, g2)\n",
        "        else:\n",
        "            ind1[tree_idx], ind2[tree_idx] = g1, g2\n",
        "    return ind1, ind2\n",
        "\n",
        "\n",
        "def xmut(ind, expr):\n",
        "    \"\"\" Mutation operator for multi-tree GP, where trees are represented as a list.\n",
        "\n",
        "    Mutation happens to a tree selected at random, when an individual is selected for crossover.\n",
        "\n",
        "    FIXME: Have to compile the trees (manually), which is frustrating.\n",
        "\n",
        "    Args:\n",
        "        ind: The individual, a list of GP trees.\n",
        "    \"\"\"\n",
        "    n = range(len(ind))\n",
        "    selected_tree_idx = random.choice(n)\n",
        "    for tree_idx in n:\n",
        "        g1 = gp.PrimitiveTree(ind[tree_idx])\n",
        "        if tree_idx == selected_tree_idx:\n",
        "            indx = gp.mutUniform(g1, expr, pset)\n",
        "            ind[tree_idx] = indx[0]\n",
        "        else:\n",
        "            ind[tree_idx] = g1\n",
        "    return ind,\n",
        "\n",
        "\n",
        "def evaluate_classification(individual, alpha = 0.9, verbose=False):\n",
        "    \"\"\"\n",
        "    Evalautes the fitness of an individual for multi-tree GP multi-class classification.\n",
        "\n",
        "    We maxmimize the fitness when we evaluate the accuracy + regularization term.\n",
        "\n",
        "    Args:\n",
        "        individual (Individual): A candidate solution to be evaluated.\n",
        "        alpha (float): A parameter that balances the accuracy and regularization term. Defaults to 0.98.\n",
        "\n",
        "    Returns:\n",
        "        accuracy (tuple): The fitness of the individual.\n",
        "    \"\"\"\n",
        "    features = toolbox.compile(expr=individual, pset=pset)\n",
        "    fitness = wrapper_classification_accuracy(features, verbose=verbose)\n",
        "    return fitness,\n",
        "\n",
        "\n",
        "toolbox.register('evaluate', evaluate_classification)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=7)\n",
        "toolbox.register(\"mate\", xmate)\n",
        "toolbox.register(\"expr_mut\", gp.genFull, min_=0, max_=2)\n",
        "toolbox.register(\"mutate\", xmut, expr=toolbox.expr_mut)\n",
        "\n",
        "\n",
        "def staticLimit(key, max_value):\n",
        "    \"\"\"\n",
        "    A variation of gp.staticLimit that works for Multi-tree representation.\n",
        "    This works for our altered xmut and xmate genetic operators for mutli-tree GP.\n",
        "    If tree depth limit is exceeded, the genetic operator is reverted.\n",
        "\n",
        "    When an invalid (over the limit) child is generated,\n",
        "    it is simply replaced by one of its parents, randomly selected.\n",
        "\n",
        "    Args:\n",
        "        key: The function to use in order the get the wanted value. For\n",
        "             instance, on a GP tree, ``operator.attrgetter('height')`` may\n",
        "             be used to set a depth limit, and ``len`` to set a size limit.\n",
        "        max_value: The maximum value allowed for the given measurement.\n",
        "             Defaults to 17, the suggested value in (Koza 1992)\n",
        "\n",
        "    Returns:\n",
        "        A decorator that can be applied to a GP operator using \\\n",
        "        :func:`~deap.base.Toolbox.decorate`\n",
        "\n",
        "    References:\n",
        "        1. Koza, J. R. G. P. (1992). On the programming of computers by means\n",
        "            of natural selection. Genetic programming.\n",
        "    \"\"\"\n",
        "\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            keep_inds = [[copy.deepcopy(tree) for tree in ind] for ind in args]\n",
        "            new_inds = list(func(*args, **kwargs))\n",
        "            for ind_idx, ind in enumerate(new_inds):\n",
        "                for tree_idx, tree in enumerate(ind):\n",
        "                    if key(tree) > max_value:\n",
        "                        random_parent = random.choice(keep_inds)\n",
        "                        new_inds[ind_idx][tree_idx] = random_parent[tree_idx]\n",
        "            return new_inds\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# See https://groups.google.com/g/deap-users/c/pWzR_q7mKJ0\n",
        "toolbox.decorate(\"mate\", staticLimit(key=operator.attrgetter(\"height\"), max_value=8))\n",
        "toolbox.decorate(\"mutate\", staticLimit(key=operator.attrgetter(\"height\"), max_value=8))\n",
        "\n",
        "\n",
        "def SimpleGPWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
        "             halloffame=None, verbose=__debug__):\n",
        "    \"\"\"\n",
        "    Elitism for Multi-Tree GP for Multi-Class classification.\n",
        "    A variation of the eaSimple method from the DEAP library that supports\n",
        "\n",
        "    Elitism ensures the best individuals (the elite) from each generation are\n",
        "    carried onto the next without alteration. This ensures the quality of the\n",
        "    best solution monotonically increases over time.\n",
        "    \"\"\"\n",
        "    logbook = tools.Logbook()\n",
        "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
        "\n",
        "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
        "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
        "\n",
        "    for ind, fit in zip(invalid_ind, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    if halloffame is None:\n",
        "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
        "\n",
        "    halloffame.update(population)\n",
        "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
        "\n",
        "    record = stats.compile(population) if stats else {}\n",
        "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
        "\n",
        "    if verbose:\n",
        "        print(logbook.stream)\n",
        "\n",
        "    for gen in range(1, ngen + 1):\n",
        "        offspring = toolbox.select(population, len(population) - hof_size)\n",
        "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
        "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
        "\n",
        "        for ind, fit in zip(invalid_ind, fitnesses):\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        offspring.extend(halloffame.items)\n",
        "        halloffame.update(offspring)\n",
        "        population[:] = offspring\n",
        "\n",
        "        record = stats.compile(population) if stats else {}\n",
        "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
        "\n",
        "        if verbose:\n",
        "            print(logbook.stream)\n",
        "\n",
        "    return population, logbook\n",
        "\n",
        "\n",
        "def train(generations=100, population=100, elitism=0.1, crossover_rate=0.5, mutation_rate=0.1):\n",
        "    \"\"\"\n",
        "    This is a Multi-tree GP with Elitism for Multi-class classification.\n",
        "\n",
        "    Args:\n",
        "        generations: The number of generations to evolve the populaiton for.\n",
        "        elitism: The ratio of elites to be kept between generations.\n",
        "        crossover_rate: The probability of a crossover between two individuals.\n",
        "        mutation_rate: The probability of a random mutation within an individual.\n",
        "\n",
        "    Returns:\n",
        "        pop: The final population the algorithm has evolved.\n",
        "        log: The logbook which can record important statistics.\n",
        "        hof: The hall of fame contains the best individual solutions.\n",
        "    \"\"\"\n",
        "    random.seed(420)\n",
        "    pop = toolbox.population(n=population)\n",
        "\n",
        "    mu = round(elitism * population)\n",
        "    if elitism > 0:\n",
        "        # See https://www.programcreek.com/python/example/107757/deap.tools.HallOfFame\n",
        "        hof = tools.HallOfFame(mu)\n",
        "    else:\n",
        "        hof = None\n",
        "\n",
        "    stats_fit = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    length = lambda a: np.max(list(map(len, a)))\n",
        "    stats_size = tools.Statistics(length)\n",
        "\n",
        "    mstats = tools.MultiStatistics(fitness=stats_fit, size=stats_size)\n",
        "    mstats.register(\"avg\", np.mean)\n",
        "    mstats.register(\"std\", np.std)\n",
        "    mstats.register(\"min\", np.min)\n",
        "    mstats.register(\"max\", np.max)\n",
        "\n",
        "    pop, log = SimpleGPWithElitism(pop, toolbox, crossover_rate, mutation_rate,\n",
        "                                   generations, stats=mstats, halloffame=hof,\n",
        "                                   verbose=True)\n",
        "    return pop, log, hof\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DeJong (1975), p=50-100, m=0.001, c=0.6\n",
        "Grefenstette (1986), p=30, m=0.01, c=0.95\n",
        "Schaffer et al., (1989), p=20-30, m=0.005-0.01, c=0.75-0.95\n",
        "\n",
        "References:\n",
        "    1. Patil, V. P., & Pawar, D. D. (2015). The optimal crossover or mutation\n",
        "    rates in genetic algorithm: a review. International Journal of Applied\n",
        "    Engineering and Technology, 5(3), 38-41.\n",
        "\"\"\"\n",
        "\n",
        "beta = 1\n",
        "population = n_features * beta\n",
        "generations = 200\n",
        "elitism = 0.1\n",
        "crossover_rate = 0.8\n",
        "mutation_rate = 0.2\n",
        "\n",
        "assert crossover_rate + mutation_rate == 1, \"Crossover and mutation sums to 1 (to please the Gods!)\"\n",
        "\n",
        "pop, log, hof = train(generations, population, elitism, crossover_rate, mutation_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "VSEgttwuilPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c366218a-ae09-4dad-e9e8-606992a0b56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01969574748582734, Val intra-class: 0.002038145099516528\n",
            "Train inter-class: 0.05422037347170029, Val inter-class: 0.07205800842614218\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019831309410084326, Val intra-class: 0.0024789574263457416\n",
            "Train inter-class: 0.054417628453195484, Val inter-class: 0.07221400054710586\n",
            "Train accuracy: 0.8333333333333334, Validation accuracy: 1.0, Test accuracy: 0.25\n",
            "Train intra-class: 0.019831675509425894, Val intra-class: 0.0024799613781695067\n",
            "Train inter-class: 0.054416095881929576, Val inter-class: 0.07221575898157678\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01983167550942588, Val intra-class: 0.0024799613781695067\n",
            "Train inter-class: 0.054416095881929576, Val inter-class: 0.07221575898157678\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019776742935089825, Val intra-class: 0.002501218722196116\n",
            "Train inter-class: 0.05439307090798232, Val inter-class: 0.07218743384396904\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019799878445624176, Val intra-class: 0.002502926128886336\n",
            "Train inter-class: 0.054396118481682035, Val inter-class: 0.07218911134888589\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019807722899077538, Val intra-class: 0.0025053387309552882\n",
            "Train inter-class: 0.05440634784447887, Val inter-class: 0.07218828163794604\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.019799532876123835, Val intra-class: 0.0025024572729921294\n",
            "Train inter-class: 0.05439644284857852, Val inter-class: 0.07218657905270377\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019805809332854057, Val intra-class: 0.002505560874121788\n",
            "Train inter-class: 0.054402096142565436, Val inter-class: 0.07219025571447026\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.01980618043392422, Val intra-class: 0.0025024926774487683\n",
            "Train inter-class: 0.05440137975356452, Val inter-class: 0.07218768586051212\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01980546141197426, Val intra-class: 0.002505092511349472\n",
            "Train inter-class: 0.054402422478498004, Val inter-class: 0.07218772345296615\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019805461412101925, Val intra-class: 0.002505092511349533\n",
            "Train inter-class: 0.05440242247859043, Val inter-class: 0.07218772345296716\n",
            "Train accuracy: 0.7777777777777777, Validation accuracy: 0.75, Test accuracy: 0.625\n",
            "Train intra-class: 0.019808154844735942, Val intra-class: 0.002505668515662966\n",
            "Train inter-class: 0.054404510366601176, Val inter-class: 0.0721884807296292\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019808154844735942, Val intra-class: 0.002505668515662966\n",
            "Train inter-class: 0.054404510366601176, Val inter-class: 0.0721884807296292\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.25\n",
            "Train intra-class: 0.019803330783589448, Val intra-class: 0.0025029524487475774\n",
            "Train inter-class: 0.05439693422614738, Val inter-class: 0.07218734280786394\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019800798540057944, Val intra-class: 0.0025122142555801755\n",
            "Train inter-class: 0.05440244384366743, Val inter-class: 0.07218799063549389\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01980846277493303, Val intra-class: 0.0025057791590669637\n",
            "Train inter-class: 0.05440330255428638, Val inter-class: 0.07218834109484645\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01980846277493303, Val intra-class: 0.0025057791590669637\n",
            "Train inter-class: 0.05440330255428638, Val inter-class: 0.07218834109484645\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 1.0, Test accuracy: 0.625\n",
            "Train intra-class: 0.01980896118351928, Val intra-class: 0.002505828368442315\n",
            "Train inter-class: 0.0544036056800833, Val inter-class: 0.07218857795987242\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01980857394893983, Val intra-class: 0.002505771919687698\n",
            "Train inter-class: 0.05440281026261601, Val inter-class: 0.07218866361829898\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019807918442320088, Val intra-class: 0.0025084278281814216\n",
            "Train inter-class: 0.05440218963344983, Val inter-class: 0.0721898610010555\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.25\n",
            "Train intra-class: 0.019773783868474063, Val intra-class: 0.002490355290880341\n",
            "Train inter-class: 0.05437188868050417, Val inter-class: 0.07216694357567467\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01981882019744742, Val intra-class: 0.0025031442807675044\n",
            "Train inter-class: 0.0544055384640707, Val inter-class: 0.07218952303817994\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.01981882019744741, Val intra-class: 0.0025031442807675044\n",
            "Train inter-class: 0.05440553846407069, Val inter-class: 0.07218952303817994\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01981882019744741, Val intra-class: 0.0025031442807675044\n",
            "Train inter-class: 0.05440553846407069, Val inter-class: 0.07218952303817994\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01981882019744848, Val intra-class: 0.0025031442807675044\n",
            "Train inter-class: 0.05440553846407147, Val inter-class: 0.07218952303817994\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019798702967261553, Val intra-class: 0.0025043315394597614\n",
            "Train inter-class: 0.05438532957994361, Val inter-class: 0.07219042362250876\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01979545489804531, Val intra-class: 0.0025278188681555616\n",
            "Train inter-class: 0.05440214159383197, Val inter-class: 0.07219172301642499\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01983575094835853, Val intra-class: 0.0024913086593556944\n",
            "Train inter-class: 0.05440828713304821, Val inter-class: 0.0721892358997806\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019795106817399818, Val intra-class: 0.002527354630180048\n",
            "Train inter-class: 0.05440246423357154, Val inter-class: 0.07218919110786585\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.0198024271529777, Val intra-class: 0.002509357420834186\n",
            "Train inter-class: 0.05439159305598621, Val inter-class: 0.07218225424017367\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019803619384772365, Val intra-class: 0.0025042240375783144\n",
            "Train inter-class: 0.05439051663102224, Val inter-class: 0.0721783441336368\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.625\n",
            "Train intra-class: 0.019775396929918847, Val intra-class: 0.002488848097177904\n",
            "Train inter-class: 0.05436160356928461, Val inter-class: 0.07216171122960435\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019813724538821662, Val intra-class: 0.0025270115235276384\n",
            "Train inter-class: 0.05440410466990383, Val inter-class: 0.07218882113451079\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 1.0, Test accuracy: 0.25\n",
            "Train intra-class: 0.019823831955496838, Val intra-class: 0.0025248692868514674\n",
            "Train inter-class: 0.054409734495428456, Val inter-class: 0.07218837256921597\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019816553771013615, Val intra-class: 0.0025259245754519037\n",
            "Train inter-class: 0.05440068202727771, Val inter-class: 0.07218993704973699\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01981141577614514, Val intra-class: 0.002532203480477716\n",
            "Train inter-class: 0.05440554380582803, Val inter-class: 0.07218414667516801\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019828955911932313, Val intra-class: 0.0025263236844080606\n",
            "Train inter-class: 0.05441095320508973, Val inter-class: 0.07218870102904337\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 1.0, Test accuracy: 0.25\n",
            "Train intra-class: 0.019806006651079223, Val intra-class: 0.00254325944426253\n",
            "Train inter-class: 0.05440434602634344, Val inter-class: 0.07218895740472767\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019743500578382336, Val intra-class: 0.002410896880192109\n",
            "Train inter-class: 0.054276792039135636, Val inter-class: 0.07206793626623864\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.625\n",
            "Train intra-class: 0.01983883160548063, Val intra-class: 0.00259027299463845\n",
            "Train inter-class: 0.05441292968237384, Val inter-class: 0.07219727365488336\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01965981967047436, Val intra-class: 0.0022377534286161962\n",
            "Train inter-class: 0.05410563263500602, Val inter-class: 0.07190384570167886\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019847564101002066, Val intra-class: 0.0032134131989499937\n",
            "Train inter-class: 0.05449784095579662, Val inter-class: 0.07233221371434777\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019799972636966815, Val intra-class: 0.0031068475080638334\n",
            "Train inter-class: 0.05438516140929736, Val inter-class: 0.07222340197960563\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01967254448366071, Val intra-class: 0.002505560874121788\n",
            "Train inter-class: 0.05401434650511026, Val inter-class: 0.0716658941276591\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.020058546706471538, Val intra-class: 0.0033236202526281417\n",
            "Train inter-class: 0.0545721454069663, Val inter-class: 0.07213458159240019\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.019747276759036432, Val intra-class: 0.002331978599654695\n",
            "Train inter-class: 0.05395036033078391, Val inter-class: 0.07143782487588926\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.019649526368258202, Val intra-class: 0.0021980833158816424\n",
            "Train inter-class: 0.05378534839792572, Val inter-class: 0.07134033360703894\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.019400831421112208, Val intra-class: 0.001764484278130553\n",
            "Train inter-class: 0.05341657453972464, Val inter-class: 0.07097854014645341\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.01971866741155403, Val intra-class: 0.0030635188009740357\n",
            "Train inter-class: 0.05411327270754776, Val inter-class: 0.07189183757519965\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01956281119880077, Val intra-class: 0.002309415883998274\n",
            "Train inter-class: 0.0537174188163606, Val inter-class: 0.07134135270538829\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 0.75, Test accuracy: 0.25\n",
            "Train intra-class: 0.01956537887271719, Val intra-class: 0.002310455318090448\n",
            "Train inter-class: 0.053719452507276194, Val inter-class: 0.07134127806872895\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019506329479873404, Val intra-class: 0.0021738333572707976\n",
            "Train inter-class: 0.05361754220951755, Val inter-class: 0.07118468568825687\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.0195024922864682, Val intra-class: 0.002177308491021132\n",
            "Train inter-class: 0.05361742682152171, Val inter-class: 0.07118388652124703\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 1.0, Test accuracy: 0.25\n",
            "Train intra-class: 0.019889130722772395, Val intra-class: 0.00302793334382766\n",
            "Train inter-class: 0.05423502226641088, Val inter-class: 0.0717904571859028\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.25\n",
            "Train intra-class: 0.01961121001743024, Val intra-class: 0.002145007379643949\n",
            "Train inter-class: 0.05362313422072781, Val inter-class: 0.07124033931524279\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019890541826709986, Val intra-class: 0.003027384080187372\n",
            "Train inter-class: 0.05423456517370826, Val inter-class: 0.0717905119676658\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019954249628926238, Val intra-class: 0.0029362686120427283\n",
            "Train inter-class: 0.05422559740424601, Val inter-class: 0.071770462778339\n",
            "Train accuracy: 0.875, Validation accuracy: 1.0, Test accuracy: 0.25\n",
            "Train intra-class: 0.0198057270486877, Val intra-class: 0.0029091967380382067\n",
            "Train inter-class: 0.054121106463297465, Val inter-class: 0.0716790753247555\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01979703177974384, Val intra-class: 0.0029066723016889255\n",
            "Train inter-class: 0.054110255452775806, Val inter-class: 0.07167708637870181\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019800820240933777, Val intra-class: 0.0029175786338745224\n",
            "Train inter-class: 0.05411574468497196, Val inter-class: 0.0716778267321525\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01979515500769828, Val intra-class: 0.002919645306293147\n",
            "Train inter-class: 0.05411128637669748, Val inter-class: 0.0716756425998151\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019807456048136537, Val intra-class: 0.0029266924831373715\n",
            "Train inter-class: 0.05412713938266463, Val inter-class: 0.07167880151250725\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 1.0, Test accuracy: 0.25\n",
            "Train intra-class: 0.01980543896901256, Val intra-class: 0.002931879046471545\n",
            "Train inter-class: 0.054123686087824495, Val inter-class: 0.07167890685069528\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01980345666462205, Val intra-class: 0.0029100555783209575\n",
            "Train inter-class: 0.05410974867971589, Val inter-class: 0.07166674265924872\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 0.75, Test accuracy: 0.25\n",
            "Train intra-class: 0.019826530242926944, Val intra-class: 0.0029229645487303087\n",
            "Train inter-class: 0.05412888549409224, Val inter-class: 0.07167911667552611\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01982653024292719, Val intra-class: 0.0029229645487303087\n",
            "Train inter-class: 0.05412888549409242, Val inter-class: 0.07167911667552611\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019828369013892948, Val intra-class: 0.0029285900537115747\n",
            "Train inter-class: 0.05413435279015167, Val inter-class: 0.07168036870203069\n",
            "Train accuracy: 0.875, Validation accuracy: 0.75, Test accuracy: 0.25\n",
            "Train intra-class: 0.019829059316173743, Val intra-class: 0.0029290022848446327\n",
            "Train inter-class: 0.054135131927605586, Val inter-class: 0.07168009644414147\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.625\n",
            "Train intra-class: 0.01982905931617375, Val intra-class: 0.0029290022848446327\n",
            "Train inter-class: 0.0541351319276056, Val inter-class: 0.07168009644414147\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01982501987254031, Val intra-class: 0.0029269109697766557\n",
            "Train inter-class: 0.05412847976713039, Val inter-class: 0.07167943701955624\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.0198250198725403, Val intra-class: 0.0029269109697766557\n",
            "Train inter-class: 0.05412847976713037, Val inter-class: 0.07167943701955624\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.0198250198725403, Val intra-class: 0.0029269109697766557\n",
            "Train inter-class: 0.05412847976713037, Val inter-class: 0.07167943701955624\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.625\n",
            "Train intra-class: 0.019830469032422787, Val intra-class: 0.0029291643709882386\n",
            "Train inter-class: 0.054134220647202747, Val inter-class: 0.07168058927720301\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01983046903242254, Val intra-class: 0.0029291643709882386\n",
            "Train inter-class: 0.05413422064720259, Val inter-class: 0.07168058927720301\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01983046903242255, Val intra-class: 0.0029291643709882386\n",
            "Train inter-class: 0.05413422064720259, Val inter-class: 0.07168058927720301\n",
            "Train accuracy: 0.7916666666666666, Validation accuracy: 1.0, Test accuracy: 0.125\n",
            "Train intra-class: 0.019830469032422346, Val intra-class: 0.0029291643709882386\n",
            "Train inter-class: 0.054134220647202407, Val inter-class: 0.07168058927720301\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01983046903242254, Val intra-class: 0.0029291643709882386\n",
            "Train inter-class: 0.05413422064720259, Val inter-class: 0.07168058927720301\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.019830469032435478, Val intra-class: 0.0029291643709882407\n",
            "Train inter-class: 0.054134220647204065, Val inter-class: 0.07168058927720808\n",
            "Train accuracy: 0.8333333333333334, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.019830469099704734, Val intra-class: 0.002929164371755304\n",
            "Train inter-class: 0.05413422069604878, Val inter-class: 0.07168058929081403\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019830010743846972, Val intra-class: 0.0029291222736838743\n",
            "Train inter-class: 0.054133940902289575, Val inter-class: 0.07168035077951536\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01982698263771412, Val intra-class: 0.0029296683587568323\n",
            "Train inter-class: 0.0541303712682003, Val inter-class: 0.07168135815379331\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019823598281875063, Val intra-class: 0.002921685158412168\n",
            "Train inter-class: 0.05412259600192037, Val inter-class: 0.07167731923352852\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019812602878616695, Val intra-class: 0.002918643392856445\n",
            "Train inter-class: 0.05411680040561603, Val inter-class: 0.07166899094421272\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.019830662233051917, Val intra-class: 0.002929245282338044\n",
            "Train inter-class: 0.054133660682523545, Val inter-class: 0.07168030245871723\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019681278930258377, Val intra-class: 0.0025758868666825156\n",
            "Train inter-class: 0.05386313733876156, Val inter-class: 0.07144782048930434\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01983247626429403, Val intra-class: 0.002932441621969771\n",
            "Train inter-class: 0.05413068620570393, Val inter-class: 0.0716876269558489\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019805959373257895, Val intra-class: 0.0029155224701269875\n",
            "Train inter-class: 0.05410987800709182, Val inter-class: 0.07166381430459873\n",
            "Train accuracy: 0.7777777777777777, Validation accuracy: 0.5, Test accuracy: 0.625\n",
            "Train intra-class: 0.01983504692395287, Val intra-class: 0.0029346369155663888\n",
            "Train inter-class: 0.05413483598599312, Val inter-class: 0.0716845020634715\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.019835046923952687, Val intra-class: 0.0029346369155663888\n",
            "Train inter-class: 0.054134835985993, Val inter-class: 0.0716845020634715\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01983504692395287, Val intra-class: 0.0029346369155663888\n",
            "Train inter-class: 0.05413483598599312, Val inter-class: 0.0716845020634715\n",
            "Train accuracy: 0.875, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01982770850703298, Val intra-class: 0.0029368601689005766\n",
            "Train inter-class: 0.054130690259787714, Val inter-class: 0.07168174534784762\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 1.0, Test accuracy: 0.25\n",
            "Train intra-class: 0.019831558771543936, Val intra-class: 0.0029260616770152774\n",
            "Train inter-class: 0.054126281600527126, Val inter-class: 0.07167768296582479\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.625\n",
            "Train intra-class: 0.019831581904471367, Val intra-class: 0.002926075423307873\n",
            "Train inter-class: 0.0541262959432681, Val inter-class: 0.071677684834888\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.625\n",
            "Train intra-class: 0.019826445682384355, Val intra-class: 0.002934188016547516\n",
            "Train inter-class: 0.054128697498432986, Val inter-class: 0.07167763675345391\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 1.0, Test accuracy: 0.25\n",
            "Train intra-class: 0.019825015464575928, Val intra-class: 0.0029304869227706593\n",
            "Train inter-class: 0.05412348658049943, Val inter-class: 0.07167545958217006\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.01983832561889624, Val intra-class: 0.0029357046709273028\n",
            "Train inter-class: 0.0541350105902749, Val inter-class: 0.07167968940929996\n",
            "Train accuracy: 0.9583333333333334, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.01982917890220149, Val intra-class: 0.002935657464381879\n",
            "Train inter-class: 0.054126800558125396, Val inter-class: 0.07167736655756077\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01984649488904408, Val intra-class: 0.0029291359286127035\n",
            "Train inter-class: 0.05413502766624153, Val inter-class: 0.07167595624416066\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 1.0, Test accuracy: 0.5\n",
            "Train intra-class: 0.01983079627161346, Val intra-class: 0.0029468496763276203\n",
            "Train inter-class: 0.054131592330821626, Val inter-class: 0.07167521115392773\n",
            "Train accuracy: 1.0, Validation accuracy: 0.75, Test accuracy: 0.375\n",
            "Train intra-class: 0.019855514448156156, Val intra-class: 0.0029409732084129886\n",
            "Train inter-class: 0.05413971186863773, Val inter-class: 0.07168172587547432\n",
            "Train accuracy: 0.9166666666666666, Validation accuracy: 1.0, Test accuracy: 0.125\n",
            "Train intra-class: 0.019825097898974787, Val intra-class: 0.0029448051161457323\n",
            "Train inter-class: 0.05411421181244409, Val inter-class: 0.07167835922031847\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(hof)):\n",
        "    evaluate_classification(hof[i], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_classification(hof[0], verbose=True)\n",
        "# Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.625\n",
        "print(f\"Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.625\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf5CtPl1OmD2",
        "outputId": "1aa9789d-ccee-489e-f6ed-5940b050899a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.375\n",
            "Train intra-class: 0.01969574748582734, Val intra-class: 0.002038145099516528\n",
            "Train inter-class: 0.05422037347170029, Val inter-class: 0.07205800842614218\n",
            "Train accuracy: 1.0, Validation accuracy: 1.0, Test accuracy: 0.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = 0.66\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, stratify=y, test_size=(1-train_split), random_state=42)\n",
        "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, stratify=y_temp, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "# print(f\"np.unique(y_train): {np.unique(y_train)}\\n np.unique(y_val): {np.unique(y_val)} \\n np.unique(y_test): {np.unique(y_test)}\")\n",
        "for (name, ds) in [(\"train\", y_train), (\"val\", y_val), (\"test\", y_test)]:\n",
        "    print(f\"{name}: {np.unique(ds)}\")\n",
        "\n",
        "    classes, class_counts = np.unique(ds, axis=0, return_counts=True)\n",
        "    n_features = X.shape[1]\n",
        "    n_instances = X.shape[0]\n",
        "    n_classes = len(np.unique(ds, axis=0))\n",
        "    class_ratios = np.array(class_counts) / n_instances\n",
        "\n",
        "    print(f\"Class Counts: {class_counts}, Class Ratios: {class_ratios}\")\n",
        "    print(f\"Number of features: {n_features}\\nNumber of instances: {n_instances}\\nNumber of classes {n_classes}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZrI3MjywK_K",
        "outputId": "f546212e-0225-4331-b137-97282eb06050"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: [0. 1. 2. 3. 4. 5.]\n",
            "Class Counts: [4 4 2 4 3 2], Class Ratios: [0.13333333 0.13333333 0.06666667 0.13333333 0.1        0.06666667]\n",
            "Number of features: 1023\n",
            "Number of instances: 30\n",
            "Number of classes 6.\n",
            "val: [0. 2. 3. 4.]\n",
            "Class Counts: [2 1 1 1], Class Ratios: [0.06666667 0.03333333 0.03333333 0.03333333]\n",
            "Number of features: 1023\n",
            "Number of instances: 30\n",
            "Number of classes 4.\n",
            "test: [1. 3. 4. 5.]\n",
            "Class Counts: [2 1 2 1], Class Ratios: [0.06666667 0.03333333 0.06666667 0.03333333]\n",
            "Number of features: 1023\n",
            "Number of instances: 30\n",
            "Number of classes 4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkaU6kAGyBl2"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "pEqouLDs4xBJ"
      },
      "outputs": [],
      "source": [
        "from deap import base, creator, gp\n",
        "import pygraphviz as pgv\n",
        "\n",
        "multi_tree = hof[0]\n",
        "for t_idx,tree in enumerate(multi_tree):\n",
        "    nodes, edges, labels = gp.graph(tree)\n",
        "\n",
        "    g = pgv.AGraph()\n",
        "    g.add_nodes_from(nodes)\n",
        "    g.add_edges_from(edges)\n",
        "    g.layout(prog=\"dot\")\n",
        "\n",
        "    for i in nodes:\n",
        "        n = g.get_node(i)\n",
        "        n.attr[\"label\"] = labels[i]\n",
        "\n",
        "    g.draw(f\"tree-{t_idx}.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX_g4MipuQTU"
      },
      "source": [
        "## Changelog\n",
        "\n",
        "| Date | Title | Description | Update |\n",
        "| --- | --- | --- | ---- |\n",
        "| 2024-02-27 16:31 | Fix fitness | Fixed fitness function to penalize intra-class distance <br> and reward inter-class distance. |\n",
        "| 2024-02-26 15:59 | Intra/inter | Updated fitness value to include a normalized sum of <br> intraclass and interclass distance |\n",
        "| 2024-02-14 16:56 | Mass spectra | Applications for MT-GP for rapid mass spectrometry dataset. | |\n",
        "| 2022-08-21 17:30 | Multi-Objective - Onehot Encoding | Change to multi-objective problem, one-vs-all with a tree classifier for each class.<br> Y labels are encoded in onehot encodings, error is absolute difference between $|\\hat{y} - y|$|\n",
        "| 2022-08-22 20:44 | Non-linearity |  Introduce $ round . sigmoid $ to evaluate_classification() method.<br>Previously, we push each tree to predict either a 0 or 1 value with the onehot encoding representation.<br>Now, the non-linearity will map any negative value to a negative class 0, and any positive value to positive class 1.|\n",
        "| 2022-08-22 21:06 | ~~Genetic operators for tree with worst fitness~~ | Only apply the genetic operators, crossover and mutation, to the tree with the worst fitness.<br> This guarantees monotonic improvement for the Multi-tree between generations, the best performing tree remain unaltered.| (Update) This was very slow, and inefficient,<br> basically turned the GP into a single objective,<br>that balances multi-objective fitness functions. |\n",
        "| 2022-08-22 21:15 | Halloffame Equality Operator | Numpy equality function (operators.eq) between two arrays returns the equality element wise,<br>which raises an exception in the if similar() check of the hall of fame. <br> Using a different equality function like numpy.array_equal or numpy.allclose solve this issue.|\n",
        "| 2022-08-22 23:22 | Elitism as aggregate best tree | Perform elitsim by constructing the best tree, as the tree with best fitness from each clas.<br>The goal is to have monotonous improvement across the multiple objective functions.|\n",
        "| 2022-08-22 23:32 | Update fitness for elite | The elitism was not working as intended, as the multi-objectives didn't appear to increase monotnously.<br> This was because the aggregate fitness was not being assigned to the best individual after it was created.<br>Therefore the best invidiual was not passed on to the next generation. |\n",
        "| 2022-08-22 02:28 | staticLimit max height | Rewrite the gp.staticLimit decorator function to handle the Multi-tree representation.<br>Note: size and depth are different values!<br>depth is the maximum length of a root-to-leaf traversal,<br>size is the total number of nodes.|\n",
        "| 2022-08-24 9:37 | Evaluate Mutli-tree train accuracy | Take the classification accuracy as the argmax for the aggregate multitree.<br> 74% training accuracy, which is not ideal, but this shall improve with time.|\n",
        "| 2022-08-25 13:30 | Single-objective fitness | Change the fitness function to a single objective fitness function.<br>This forces the multi-tree GP to find the best tree subset for one-vs-rest classification performance.|\n",
        "| 2022-08-25 20:01 | Fitness = Balanced accuracy + distance measure | Implement the fitness function for MCIFC, but for multi-class classification from (Tran 2019) |\n",
        "| 2022-08-26 21:27 | Sklearn Balanced Accuracy | Changed to the balanced accuracy metric from sklearn.<br>This is much easier to use for now, probably faster than the previous method as well. |\n",
        "| 2022-09-05 17:00 | Reject invalid predictions | Change the fitness function to reject invalid predictioctions outright -<br>e.g. multi-label or zero-label predictions<br>- when computing the balanced accuracy for the fitness function. |\n",
        "| 2022-09-13 19:00 | Mutation + Crossover = 100% | Ensure the mutation and crossover rate sum to 100%,<br>not necessary with deap, but good to avoid conference questions |\n",
        "| 2022-09-13 21:00 | Feature Construction | Changed to Wrapper-based Feature Construction with Multi-tree GP.|\n",
        "| 2022-09-13 21:34 | $m = r \\times c$ | Add more trees, following example from (Tran 2019).<br> With 8 trees for a multi-class classification<br> $m = r \\times c = 8$ trees, where number of classes $c = 4$, and reconstruction ratio $r = 2$/ |\n",
        "| 2022-09-30 19:49 | Quick Evalaute | Manually parse the GP trees, 5x speedup for DEAP in Python (Zhang 2022). |\n",
        "| 2022-10-13 6:02 | Ignore timestamps after 4500 | Ignore timestamps after 4500 did not improve accuracy for SVM classifier.<br>So the bizzare pattern that occurs on GC-MS image there has important information.<br> Should investigate this further, perhaps ask Daniel as he is a domain expert.|\n",
        "| 2022-10-13 22:16 | Cross validation | Evaluate the mean balanced classification accuracy over stratified k-fold cross validation.|\n",
        "| 2023-01-13 20:58 | 2x speedup | Only evaluate test set for verbose alternative of the evaluate_classification method.<br> This results in a 2x speedup in the efficiency of the training regime.|"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1Yg4t38NHSYPAlu_099cQeOR-qSwIaaTl",
      "authorship_tag": "ABX9TyNMK0EXlKWvgGgMGfDhxZry",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}