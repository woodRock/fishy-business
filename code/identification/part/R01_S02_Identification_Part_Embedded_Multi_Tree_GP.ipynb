{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/fishy-business/blob/main/code/identification/part/R01_S02_Identification_Part_Embedded_Multi_Tree_GP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAlW5BfrgjQk"
      },
      "source": [
        "# Multi-tree Genetic Program"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the seed for reproduceability.\n",
        "run = 4 # @param {type: \"integer\"}\n",
        "\n",
        "# The number of features in the dataset.\n",
        "n_features = 1023\n",
        "\n",
        "# Hyperparameters\n",
        "beta = 1 # @param {type: \"integer\"}\n",
        "population = n_features * beta\n",
        "generations = 500 # @param {type: \"integer\"}\n",
        "elitism = 0.1 # @param {type: \"number\"}\n",
        "crossover_rate = 0.8 # @param {type: \"number\"}\n",
        "mutation_rate = 0.2 # @param {type: \"number\"}\n",
        "\n",
        "assert crossover_rate + mutation_rate == 1, \"Crossover and mutation sums to 1 (to please the Gods!)\""
      ],
      "metadata": {
        "id": "1WDTpz7ko3mg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLL18yL7WWRt",
        "outputId": "6d096367-7ecd-40b7-e899-08e239f381fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deap in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.25.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libgraphviz-dev is already the newest version (2.42.2-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Requirement already satisfied: pygraphviz in /usr/local/lib/python3.10/dist-packages (1.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install deap\n",
        "!apt install libgraphviz-dev\n",
        "!pip install pygraphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-0dl8EoqK0T",
        "outputId": "645dcea5-36c2-4055-bc79-3cefc1ed883c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[INFO] Reading the dataset.\n",
            "Class Counts: [6 6 3 6 6 3], Class Ratios: [0.2 0.2 0.1 0.2 0.2 0.1]\n",
            "Number of features: 1023\n",
            "Number of instances: 30\n",
            "Number of classes 6.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "import scipy.io\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from google.colab import drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path_gdrive = '/content/drive/MyDrive/AI/Data'\n",
        "dataset = 'Part' #@param [\"Fish\", \"Part\"]\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.listdir('/content/drive/My Drive')\n",
        "\n",
        "path = ['drive', 'MyDrive', 'AI', 'fish', 'REIMS_data.xlsx']\n",
        "path = os.path.join(*path)\n",
        "\n",
        "# Load the dataset\n",
        "print(\"[INFO] Reading the dataset.\")\n",
        "raw = pd.read_excel(path)\n",
        "\n",
        "data = raw[~raw['m/z'].str.contains('HM')]\n",
        "data = data[~data['m/z'].str.contains('QC')]\n",
        "data = data[~data['m/z'].str.contains('HM')]\n",
        "X = data.drop('m/z', axis=1) # X contains only the features.\n",
        "y = data['m/z'].apply(lambda x:\n",
        "                          0 if 'Fillet' in x\n",
        "                    else  1 if 'Heads' in x\n",
        "                    else (2 if 'Livers' in x\n",
        "                    else (3 if 'Skins' in x\n",
        "                    else (4 if 'Guts' in x\n",
        "                    else (5 if 'Frames' in x\n",
        "                    else None )))))  # For fish parts\n",
        "xs = []\n",
        "ys = []\n",
        "for (x,y) in zip(X.to_numpy(),y):\n",
        "    if y is not None and not np.isnan(y):\n",
        "       xs.append(x)\n",
        "       ys.append(y)\n",
        "X = np.array(xs)\n",
        "y = np.array(ys)\n",
        "\n",
        "classes, class_counts = np.unique(y, axis=0, return_counts=True)\n",
        "n_features = X.shape[1]\n",
        "n_instances = X.shape[0]\n",
        "n_classes = len(np.unique(y, axis=0))\n",
        "class_ratios = np.array(class_counts) / n_instances\n",
        "\n",
        "print(f\"Class Counts: {class_counts}, Class Ratios: {class_ratios}\")\n",
        "print(f\"Number of features: {n_features}\\nNumber of instances: {n_instances}\\nNumber of classes {n_classes}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "8AQBTITsyEzE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import copy\n",
        "import random\n",
        "import operator\n",
        "from re import I\n",
        "from operator import attrgetter\n",
        "from functools import wraps, partial\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from deap import algorithms\n",
        "from deap.algorithms import varAnd\n",
        "from deap import base, creator, tools, gp\n",
        "from deap.gp import PrimitiveTree, Primitive, Terminal\n",
        "\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "pset = gp.PrimitiveSet(\"MAIN\", n_features)\n",
        "pset.addPrimitive(operator.add, 2)\n",
        "pset.addPrimitive(operator.sub, 2)\n",
        "pset.addPrimitive(operator.mul, 2)\n",
        "pset.addPrimitive(operator.neg, 1)\n",
        "# pset.addEphemeralConstant(\"rand101\", lambda: random.randint(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "rQE3P3qLyMDr"
      },
      "outputs": [],
      "source": [
        "toolbox = base.Toolbox()\n",
        "\n",
        "minimized = False\n",
        "if minimized:\n",
        "    weight = -1.0\n",
        "else:\n",
        "    weight = 1.0\n",
        "\n",
        "weights = (weight,)\n",
        "\n",
        "if minimized:\n",
        "    creator.create(\"FitnessMin\", base.Fitness, weights=weights)\n",
        "    creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "else:\n",
        "    creator.create(\"FitnessMax\", base.Fitness, weights=weights)\n",
        "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "def quick_evaluate(expr: PrimitiveTree, pset, data, prefix='ARG'):\n",
        "    \"\"\" Quick evaluate offers a 500% speedup for the evluation of GP trees.\n",
        "\n",
        "    The default implementation of gp.compile provided by the DEAP library is\n",
        "    horrendously inefficient. (Zhang 2022) has shared his code which leads to a\n",
        "    5x speedup in the compilation and evaluation of GP trees when compared to the\n",
        "    standard library approach.\n",
        "\n",
        "    For multi-tree GP, this speedup factor is invaluable! As each individual conists\n",
        "    of m trees. For the fish dataset we have 4 classes, each with 3 constructed features,\n",
        "    which corresponds to 4 classes x 3 features = 12 trees for each individual.\n",
        "    12 trees x 500% speedup = 6,000% overall speedup, or 60 times faster.\n",
        "    The 500% speedup is fundamental, for efficient evaluation of multi-tree GP.\n",
        "\n",
        "    Args:\n",
        "        expr (PrimitiveTree): The uncompiled (gp.PrimitiveTree) GP tree.\n",
        "        pset: The primitive set.\n",
        "        data: The dataset to evaluate the GP tree for.\n",
        "        prefix: Prefix for variable arguments. Defaults to ARG.\n",
        "\n",
        "    Returns:\n",
        "        The (array-like) result of the GP tree evaluate on the dataset .\n",
        "    \"\"\"\n",
        "    result = None\n",
        "    stack = []\n",
        "    for node in expr:\n",
        "        stack.append((node, []))\n",
        "        while len(stack[-1][1]) == stack[-1][0].arity:\n",
        "            prim, args = stack.pop()\n",
        "            if isinstance(prim, Primitive):\n",
        "                result = pset.context[prim.name](*args)\n",
        "            elif isinstance(prim, Terminal):\n",
        "                if prefix in prim.name:\n",
        "                    result = data[:, int(prim.name.replace(prefix, ''))]\n",
        "                else:\n",
        "                    result = prim.value\n",
        "            else:\n",
        "                raise Exception\n",
        "            if len(stack) == 0:\n",
        "                break # If stack is empty, all nodes should have been seen\n",
        "            stack[-1][1].append(result)\n",
        "    return result\n",
        "\n",
        "def compileMultiTree(expr, pset):\n",
        "    \"\"\"Compile the expression represented by a list of trees.\n",
        "\n",
        "    A variation of the gp.compileADF method, that handles Multi-tree GP.\n",
        "\n",
        "    Args:\n",
        "        expr: Expression to compile. It can either be a PrimitiveTree,\n",
        "                 a string of Python code or any object that when\n",
        "                 converted into string produced a valid Python code\n",
        "                 expression.\n",
        "        pset: Primitive Set\n",
        "\n",
        "    Returns:\n",
        "        A set of functions that correspond for each tree in the Multi-tree.\n",
        "    \"\"\"\n",
        "    funcs = []\n",
        "    gp_tree = None\n",
        "    func = None\n",
        "\n",
        "    for subexpr in expr:\n",
        "        gp_tree = gp.PrimitiveTree(subexpr)\n",
        "        # 5x speedup by manually parsing GP tree (Zhang 2022) https://mail.google.com/mail/u/0/#inbox/FMfcgzGqQmQthcqPCCNmstgLZlKGXvbc\n",
        "        func = quick_evaluate(gp_tree, pset, X, prefix='ARG')\n",
        "        funcs.append(func)\n",
        "\n",
        "    # Hengzhe's method returns the features in the wrong rotation for multi-tree\n",
        "    features = np.array(funcs).T\n",
        "    return features\n",
        "\n",
        "# MCIFC constructs 8 feautres for a (c=4) multi-class classification problem (Tran 2019).\n",
        "# c - number of classes, r - construction ratio, m - total number of constructed features.\n",
        "# m = r * c = 2 ratio * 4 classes = 8 features\n",
        "\n",
        "r = 1\n",
        "c = n_classes\n",
        "m = r * c\n",
        "\n",
        "toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=2)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.expr, n=m)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"compile\", compileMultiTree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFj8hToTJ09i"
      },
      "source": [
        "## Infinite speedup!?\n",
        "\n",
        "For multi-tree GP, this speedup factor is invaluable! As each individual conists of m trees. For the fish dataset we have 4 classes, each with 3 constructed features, which corresponds to 4 classes x 3 features = 12 trees for each individual. 12 trees x 500% speedup = 6,000% overall speedup, or 60 times faster. The 500% speedup is fundamental, for efficient evaluation of multi-tree GP.\n",
        "\n",
        "The evaluation below shows my calculations may still be wrong. And perhaps, it is even faster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcGYavg1D-77",
        "outputId": "82d67770-a4fc-48a2-f788-a73d3e34bda1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.26 ms ± 182 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "4.76 µs ± 186 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
          ]
        }
      ],
      "source": [
        "first = toolbox.population(n=1)[0]\n",
        "\n",
        "subtree = first[0]\n",
        "gp_tree = gp.PrimitiveTree(subtree)\n",
        "\n",
        "%timeit gp.compile(gp_tree, pset)\n",
        "%timeit quick_evaluate(gp_tree, pset, X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryKFCvmusAlS",
        "outputId": "2929a23b-695d-4707-8b80-c5cc6d86a2b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.17777777777777778 +- 0.015713484026367734\n",
            "Val accuracy: 0.2333333333333333 +- 0.02357022603955158\n",
            "Test accuracy: 0.0 +- 0.0\n",
            "Train intra-class: 0.03870290454391563, Val Inter-class: 0.027398001816543928\n",
            "Val intra-class: 0.025319031592404975, Val Inter-class: 0.04797359707484997\n",
            "Test inter-class: 0.03870290454391563, Val inter-class: 0.027398001816543928\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24109173194948502"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC as svm\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier as knn\n",
        "from sklearn.ensemble import RandomForestClassifier as rf\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "def normalize(x):\n",
        "    \"\"\"\n",
        "    Normalize a numpy array to interclass/intraclass distance sum to 1.\n",
        "\n",
        "    Args:\n",
        "        x: numpy array to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        numpy array normalized to interclass/intraclass distance sum to 1.\n",
        "    \"\"\"\n",
        "    x_norm = (x-np.min(x))/(np.max(x)-np.min(x))\n",
        "    return x_norm\n",
        "\n",
        "def is_same_class(a,b):\n",
        "    \"\"\"\n",
        "    Return True if a and b are in the same class.\n",
        "\n",
        "    Args:\n",
        "        a: first point\n",
        "        b: second point\n",
        "\n",
        "    Returns:\n",
        "        True if a and b are in the same class.\n",
        "    \"\"\"\n",
        "    return a[1] == b[1]\n",
        "\n",
        "def euclidian_distance(a,b):\n",
        "    \"\"\"\n",
        "    Return the euclidian distance between two points.\n",
        "\n",
        "    Args:\n",
        "        a: first point\n",
        "        b: second point\n",
        "\n",
        "    Returns:\n",
        "        Euclidian distance between a and b.\n",
        "    \"\"\"\n",
        "    a,b = a[0], b[0]\n",
        "    return np.linalg.norm(a-b)\n",
        "\n",
        "def intraclass_distance(_X,_y):\n",
        "    \"\"\"\n",
        "    Return the intra-class distance for a dataset.\n",
        "    The average distance between all pairs of instances that are from the same class.\n",
        "\n",
        "    Args:\n",
        "        _X: numpy array of features.\n",
        "        _y: numpy array of labels.\n",
        "\n",
        "    Returns:\n",
        "        Intra-class distance for a dataset.\n",
        "    \"\"\"\n",
        "    data = list(zip(_X, _y))\n",
        "    pair_length = sum([1 if is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]])\n",
        "    d = sum([euclidian_distance(a,b) if is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]]) / (pair_length * _X.shape[1])\n",
        "    return d\n",
        "\n",
        "def interclass_distance(_X,_y):\n",
        "    \"\"\"\n",
        "    Return the inter-class distance for a dataset.\n",
        "    The average distance between all pairs of instances that are from different classes.\n",
        "\n",
        "    Args:\n",
        "        _X: numpy array of features.\n",
        "        _y: numpy array of labels.\n",
        "\n",
        "    Returns:\n",
        "        Inter-class distance for a dataset.\n",
        "    \"\"\"\n",
        "    data = list(zip(_X, _y))\n",
        "    pair_length = sum([1 if not is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]])\n",
        "    d = sum([euclidian_distance(a,b) if not is_same_class(a,b) else 0 for idx, a in enumerate(data) for b in data[idx + 1:]]) / (pair_length * _X.shape[1])\n",
        "    return d\n",
        "\n",
        "def wrapper_classification_accuracy(X, k=3, verbose=False):\n",
        "    \"\"\" Evaluate balanced classification accuracy over stratified k-fold cross validation.\n",
        "\n",
        "    This method is our fitness measure for an individual. We measure each individual\n",
        "    based on its balanced classification accuracy using 10-fold cross-validation on\n",
        "    the training set.\n",
        "\n",
        "    If verbose, we evaluate performance on the test set as well, and print the results\n",
        "    to the standard output. By default, only the train set is evaluated, which\n",
        "    corresponds to a 2x speedup for training, when compared to the verbose method.\n",
        "\n",
        "    Args:\n",
        "        X: entire dataset, train and test.\n",
        "        k: Number of folds, for cross validation. Defaults to 10.\n",
        "        verbose: If true, prints stuff. Defaults to false.\n",
        "\n",
        "    Returns:\n",
        "        Average balanced classification accuracy with 10-fold CV on training set.\n",
        "    \"\"\"\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    test_accs = []\n",
        "\n",
        "    # Reserve a test set that is not touched by the training algorithm.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Stratified k-fold validation.\n",
        "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X_train,y_train):\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "        # Normalize features to interclass/intraclass distance sum to 1.\n",
        "        X_train = normalize(X_train)\n",
        "        # Class-dependent multi-tree embedded GP (Tran 2019).\n",
        "        y_predict = [np.argmax(x) for x in X_train]\n",
        "        train_acc = balanced_accuracy_score(y_train, y_predict)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # 2x speedup: only evaluate test set in verbose mode.\n",
        "        if verbose:\n",
        "            X_val, X_test = normalize(X_val), normalize(X_test)\n",
        "\n",
        "            y_predict = [np.argmax(x) for x in X_val]\n",
        "            val_acc = balanced_accuracy_score(y_val, y_predict)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            y_predict = [np.argmax(x) for x in X_test]\n",
        "            test_acc = balanced_accuracy_score(y_test, y_predict)\n",
        "            test_accs.append(test_acc)\n",
        "\n",
        "    # Mean and standard deviation for training accuracy.\n",
        "    train_accuracy = np.mean(train_accs)\n",
        "    train_std = np.std(train_accs)\n",
        "    # Distance-based regularization method, intra/inter class distance.\n",
        "    train_intraclass_distance = intraclass_distance(X_train, y_train)\n",
        "    train_interclass_distance = interclass_distance(X_train, y_train)\n",
        "\n",
        "    # 2x speedup: only evaluate test set in verbose mode.\n",
        "    if verbose:\n",
        "        # Mean and standard deviation for val accuracy.\n",
        "        val_accuracy = np.mean(val_accs)\n",
        "        val_std = np.std(val_accs)\n",
        "        # Mean and standard deviation for test accuracy.\n",
        "        test_accuracy = np.mean(test_accs)\n",
        "        test_std = np.std(test_accs)\n",
        "        # Distance-based regularization method, intra/inter class distance.\n",
        "        val_intraclass_distance = intraclass_distance(X_val, y_val)\n",
        "        val_interclass_distance = interclass_distance(X_val, y_val)\n",
        "        # Distance-based regularization method, intra/inter class distance.\n",
        "        test_intrarclass_distance = intraclass_distance(X_train, y_train)\n",
        "        test_interclass_distance = interclass_distance(X_train, y_train)\n",
        "        # When verbose, give a full evaluation for an individual.\n",
        "        print(f\"Train accuracy: {train_accuracy} +- {train_std}\")\n",
        "        print(f\"Val accuracy: {val_accuracy} +- {val_std}\")\n",
        "        print(f\"Test accuracy: {test_accuracy} +- {test_std}\")\n",
        "\n",
        "        print(f\"Train intra-class: {train_intraclass_distance}, Val Inter-class: {train_interclass_distance}\")\n",
        "        print(f\"Val intra-class: {val_intraclass_distance}, Val Inter-class: {val_interclass_distance}\")\n",
        "        print(f\"Test inter-class: {test_intrarclass_distance}, Val inter-class: {test_interclass_distance}\")\n",
        "\n",
        "    # Alpha balances the inter-class/intra-class distance.\n",
        "    alpha = 0.5\n",
        "    #  Beta balances the accuracy and distance regularization term.\n",
        "    beta = 0.8\n",
        "    train_distance = alpha * (1 - train_intraclass_distance) + alpha * train_interclass_distance\n",
        "    fitness = beta * train_accuracy + (1 - beta) * train_distance\n",
        "    # Fitness value must be a tuple.\n",
        "    assert fitness <= 1, f\"fitness {fitness} should be normalized, and cannot exceed 1\"\n",
        "    if fitness > 1:\n",
        "        print(f\"Train intra-class: {train_intraclass_distance}\")\n",
        "        print(f\"Train inter-class: {train_interclass_distance}\")\n",
        "    return fitness\n",
        "\n",
        "# Evaluate the verbose accuracy for the first individual in a generated population.\n",
        "individual = toolbox.population(n=1)[0]\n",
        "features = toolbox.compile(expr=individual, pset=pset)\n",
        "wrapper_classification_accuracy(features, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vOz-1Oislgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c2a3d9-0211-4e05-fb74-87ad20a0f396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   \t      \t                                 fitness                                 \t                      size                     \n",
            "   \t      \t-------------------------------------------------------------------------\t-----------------------------------------------\n",
            "gen\tnevals\tavg     \tgen\tmax     \tmin      \tnevals\tstd      \tavg    \tgen\tmax\tmin\tnevals\tstd    \n",
            "0  \t1023  \t0.235102\t0  \t0.445272\t0.0989214\t1023  \t0.0529308\t5.72043\t0  \t7  \t3  \t1023  \t1.60706\n",
            "1  \t788   \t0.298046\t1  \t0.458484\t0.101342 \t788   \t0.0543395\t5.72727\t1  \t13 \t3  \t788   \t1.78155\n",
            "2  \t739   \t0.354016\t2  \t0.503348\t0.100181 \t739   \t0.054561 \t5.56598\t2  \t12 \t3  \t739   \t1.72308\n",
            "3  \t768   \t0.384729\t3  \t0.527393\t0.146651 \t768   \t0.0564349\t5.71163\t3  \t13 \t3  \t768   \t1.60093\n",
            "4  \t787   \t0.407911\t4  \t0.59221 \t0.159248 \t787   \t0.0670162\t6.0391 \t4  \t13 \t3  \t787   \t1.54136\n",
            "5  \t773   \t0.432442\t5  \t0.598117\t0.130577 \t773   \t0.0691465\t5.70283\t5  \t14 \t3  \t773   \t1.92571\n",
            "6  \t766   \t0.463226\t6  \t0.598509\t0.140962 \t766   \t0.0773738\t4.65494\t6  \t13 \t3  \t766   \t1.97174\n",
            "7  \t793   \t0.494587\t7  \t0.599371\t0.170389 \t793   \t0.084985 \t4.08407\t7  \t11 \t3  \t793   \t1.58585\n",
            "8  \t779   \t0.524356\t8  \t0.621946\t0.188884 \t779   \t0.0817817\t4.16618\t8  \t10 \t3  \t779   \t1.52264\n",
            "9  \t781   \t0.539893\t9  \t0.647162\t0.189112 \t781   \t0.0828368\t4.22092\t9  \t11 \t3  \t781   \t1.37356\n",
            "10 \t775   \t0.550305\t10 \t0.648132\t0.175029 \t775   \t0.0814194\t3.64125\t10 \t11 \t3  \t775   \t1.37899\n",
            "11 \t802   \t0.555153\t11 \t0.648673\t0.188558 \t802   \t0.0831276\t3.50342\t11 \t9  \t3  \t802   \t1.23319\n",
            "12 \t764   \t0.562237\t12 \t0.648673\t0.179038 \t764   \t0.0907303\t3.63343\t12 \t11 \t3  \t764   \t1.38472\n",
            "13 \t767   \t0.580642\t13 \t0.648763\t0.162292 \t767   \t0.0877208\t3.60606\t13 \t12 \t3  \t767   \t1.38919\n",
            "14 \t785   \t0.586111\t14 \t0.649079\t0.158789 \t785   \t0.0902516\t3.63148\t14 \t9  \t3  \t785   \t1.2997 \n",
            "15 \t755   \t0.590263\t15 \t0.649079\t0.160414 \t755   \t0.0942264\t3.75758\t15 \t9  \t3  \t755   \t1.40446\n",
            "16 \t768   \t0.585808\t16 \t0.649079\t0.150621 \t768   \t0.0959176\t3.79863\t16 \t9  \t3  \t768   \t1.40538\n",
            "17 \t763   \t0.593998\t17 \t0.68009 \t0.131049 \t763   \t0.089249 \t3.69013\t17 \t11 \t3  \t763   \t1.42412\n",
            "18 \t781   \t0.596498\t18 \t0.68009 \t0.131811 \t781   \t0.086448 \t3.43793\t18 \t9  \t3  \t781   \t1.16976\n",
            "19 \t775   \t0.59853 \t19 \t0.680698\t0.178914 \t775   \t0.0855514\t3.46139\t19 \t9  \t3  \t775   \t1.23813\n",
            "20 \t762   \t0.598272\t20 \t0.680698\t0.139296 \t762   \t0.0936688\t3.34018\t20 \t9  \t3  \t762   \t1.09724\n",
            "21 \t788   \t0.60873 \t21 \t0.690803\t0.16217  \t788   \t0.088848 \t3.32454\t21 \t9  \t3  \t788   \t1.09841\n",
            "22 \t768   \t0.615365\t22 \t0.691043\t0.178096 \t768   \t0.0978081\t3.28055\t22 \t9  \t3  \t768   \t0.952685\n",
            "23 \t778   \t0.622302\t23 \t0.691172\t0.189361 \t778   \t0.0973569\t3.3089 \t23 \t9  \t3  \t778   \t1.06595 \n",
            "24 \t756   \t0.62991 \t24 \t0.691172\t0.129397 \t756   \t0.0831978\t3.28152\t24 \t9  \t3  \t756   \t1.01452 \n",
            "25 \t772   \t0.630915\t25 \t0.691287\t0.148724 \t772   \t0.0914708\t3.31672\t25 \t9  \t3  \t772   \t1.03194 \n",
            "26 \t791   \t0.641351\t26 \t0.691352\t0.176104 \t791   \t0.0860891\t3.39492\t26 \t9  \t3  \t791   \t1.1498  \n",
            "27 \t788   \t0.638193\t27 \t0.691627\t0.142865 \t788   \t0.0899264\t3.35875\t27 \t11 \t3  \t788   \t1.07642 \n",
            "28 \t774   \t0.640925\t28 \t0.69165 \t0.169574 \t774   \t0.0908107\t3.45161\t28 \t9  \t3  \t774   \t1.20416 \n",
            "29 \t757   \t0.6472  \t29 \t0.695816\t0.183682 \t757   \t0.0829132\t3.49267\t29 \t9  \t3  \t757   \t1.18218 \n",
            "30 \t785   \t0.643262\t30 \t0.695816\t0.181996 \t785   \t0.0920041\t3.66569\t30 \t11 \t3  \t785   \t1.3018  \n",
            "31 \t763   \t0.638792\t31 \t0.695816\t0.180154 \t763   \t0.0926658\t3.68817\t31 \t9  \t3  \t763   \t1.39806 \n",
            "32 \t761   \t0.64153 \t32 \t0.695816\t0.180998 \t761   \t0.0916965\t3.98045\t32 \t9  \t3  \t761   \t1.62508 \n",
            "33 \t772   \t0.641488\t33 \t0.696058\t0.189557 \t772   \t0.0928917\t4.88368\t33 \t13 \t3  \t772   \t1.93773 \n",
            "34 \t751   \t0.636855\t34 \t0.698705\t0.191045 \t751   \t0.0982706\t6.13392\t34 \t15 \t3  \t751   \t1.70637 \n",
            "35 \t773   \t0.634223\t35 \t0.698705\t0.189962 \t773   \t0.102127 \t6.81916\t35 \t15 \t3  \t773   \t1.35979 \n",
            "36 \t745   \t0.632207\t36 \t0.698705\t0.189355 \t745   \t0.104892 \t7.25709\t36 \t15 \t3  \t745   \t1.66306 \n",
            "37 \t756   \t0.627491\t37 \t0.698705\t0.180239 \t756   \t0.107379 \t7.96676\t37 \t18 \t3  \t756   \t2.31593 \n",
            "38 \t807   \t0.621883\t38 \t0.698705\t0.152538 \t807   \t0.115915 \t9.05181\t38 \t21 \t3  \t807   \t2.72927 \n",
            "39 \t758   \t0.633986\t39 \t0.699422\t0.189579 \t758   \t0.104161 \t9.12708\t39 \t19 \t3  \t758   \t2.46886 \n",
            "40 \t766   \t0.631741\t40 \t0.699422\t0.18389  \t766   \t0.106797 \t9.05963\t40 \t19 \t3  \t766   \t1.81052 \n",
            "41 \t770   \t0.636304\t41 \t0.699422\t0.174261 \t770   \t0.0980327\t9.11535\t41 \t19 \t3  \t770   \t1.58018 \n",
            "42 \t773   \t0.639303\t42 \t0.747484\t0.192235 \t773   \t0.0964192\t9.08407\t42 \t17 \t3  \t773   \t1.56663 \n",
            "43 \t790   \t0.629716\t43 \t0.747484\t0.19144  \t790   \t0.109439 \t8.87195\t43 \t17 \t3  \t790   \t1.51958 \n",
            "44 \t762   \t0.63883 \t44 \t0.747484\t0.191114 \t762   \t0.0977569\t8.43304\t44 \t17 \t3  \t762   \t1.69098 \n",
            "45 \t773   \t0.629842\t45 \t0.747484\t0.211726 \t773   \t0.113243 \t7.81525\t45 \t17 \t3  \t773   \t1.6378  \n",
            "46 \t802   \t0.631693\t46 \t0.747484\t0.150746 \t802   \t0.111311 \t7.64321\t46 \t17 \t3  \t802   \t1.59691 \n",
            "47 \t774   \t0.631285\t47 \t0.747672\t0.193702 \t774   \t0.117688 \t7.82111\t47 \t17 \t3  \t774   \t1.66508 \n",
            "48 \t752   \t0.64087 \t48 \t0.747672\t0.120892 \t752   \t0.128334 \t8.49365\t48 \t15 \t5  \t752   \t1.71182 \n",
            "49 \t810   \t0.659674\t49 \t0.76855 \t0.150521 \t810   \t0.123    \t9.39589\t49 \t16 \t3  \t810   \t1.56285 \n",
            "50 \t765   \t0.665748\t50 \t0.76855 \t0.150521 \t765   \t0.11706  \t9.80156\t50 \t16 \t3  \t765   \t1.22083 \n",
            "51 \t792   \t0.666568\t51 \t0.76855 \t0.120814 \t792   \t0.131606 \t9.89247\t51 \t15 \t3  \t792   \t1.14372 \n",
            "52 \t770   \t0.669428\t52 \t0.768681\t0.120814 \t770   \t0.130625 \t9.98338\t52 \t19 \t3  \t770   \t1.23754 \n",
            "53 \t757   \t0.672783\t53 \t0.768681\t0.121934 \t757   \t0.132744 \t10.1056\t53 \t18 \t3  \t757   \t1.18915 \n",
            "54 \t754   \t0.682194\t54 \t0.78855 \t0.121934 \t754   \t0.133279 \t10.2366\t54 \t18 \t5  \t754   \t1.28939 \n",
            "55 \t785   \t0.692488\t55 \t0.78855 \t0.121851 \t785   \t0.125791 \t10.218 \t55 \t19 \t5  \t785   \t1.34418 \n",
            "56 \t769   \t0.692844\t56 \t0.78855 \t0.172004 \t769   \t0.126167 \t10.2796\t56 \t18 \t3  \t769   \t1.33823 \n",
            "57 \t777   \t0.687981\t57 \t0.788681\t0.12184  \t777   \t0.134901 \t10.3871\t57 \t18 \t3  \t777   \t1.54272 \n",
            "58 \t791   \t0.691561\t58 \t0.788681\t0.121934 \t791   \t0.133483 \t10.5093\t58 \t18 \t3  \t791   \t1.50712 \n",
            "59 \t792   \t0.690952\t59 \t0.788681\t0.121934 \t792   \t0.145544 \t10.5787\t59 \t19 \t5  \t792   \t1.7083  \n",
            "60 \t766   \t0.694337\t60 \t0.788681\t0.121934 \t766   \t0.146636 \t10.7312\t60 \t20 \t5  \t766   \t1.99488 \n",
            "61 \t772   \t0.704972\t61 \t0.788771\t0.121918 \t772   \t0.138317 \t11.1955\t61 \t20 \t6  \t772   \t2.48296 \n",
            "62 \t762   \t0.708329\t62 \t0.788771\t0.121873 \t762   \t0.13089  \t11.0196\t62 \t23 \t6  \t762   \t2.46532 \n",
            "63 \t780   \t0.701927\t63 \t0.788852\t0.101379 \t780   \t0.140117 \t10.9169\t63 \t23 \t7  \t780   \t2.10291 \n",
            "64 \t764   \t0.699169\t64 \t0.788852\t0.121713 \t764   \t0.142893 \t11.4233\t64 \t25 \t7  \t764   \t2.2623  \n",
            "65 \t775   \t0.702861\t65 \t0.789604\t0.121713 \t775   \t0.138965 \t11.8553\t65 \t25 \t7  \t775   \t2.50758 \n",
            "66 \t781   \t0.705353\t66 \t0.789604\t0.119949 \t781   \t0.13575  \t12.3363\t66 \t25 \t8  \t781   \t2.82537 \n",
            "67 \t763   \t0.713174\t67 \t0.789604\t0.121713 \t763   \t0.126101 \t12.4418\t67 \t25 \t8  \t763   \t2.89306 \n",
            "68 \t773   \t0.710045\t68 \t0.789756\t0.121713 \t773   \t0.133161 \t13.8485\t68 \t25 \t6  \t773   \t2.82869 \n",
            "69 \t772   \t0.707631\t69 \t0.789756\t0.121713 \t772   \t0.135721 \t14.7947\t69 \t25 \t7  \t772   \t2.44447 \n",
            "70 \t769   \t0.714982\t70 \t0.789756\t0.122859 \t769   \t0.126314 \t15.1437\t70 \t24 \t8  \t769   \t2.13519 \n",
            "71 \t791   \t0.710022\t71 \t0.789756\t0.121584 \t791   \t0.131263 \t15.6725\t71 \t28 \t7  \t791   \t1.90058 \n",
            "72 \t778   \t0.717221\t72 \t0.790118\t0.122709 \t778   \t0.118786 \t15.8309\t72 \t28 \t8  \t778   \t1.61425 \n",
            "73 \t781   \t0.722161\t73 \t0.790118\t0.121611 \t781   \t0.113142 \t15.9384\t73 \t25 \t8  \t781   \t1.44265 \n",
            "74 \t781   \t0.725436\t74 \t0.790366\t0.122474 \t781   \t0.115123 \t16.0684\t74 \t25 \t10 \t781   \t1.48805 \n",
            "75 \t783   \t0.713645\t75 \t0.790366\t0.122474 \t783   \t0.132906 \t16.6569\t75 \t30 \t9  \t783   \t1.89834 \n",
            "76 \t755   \t0.721764\t76 \t0.791411\t0.122709 \t755   \t0.123571 \t17.4027\t76 \t30 \t7  \t755   \t2.42504 \n",
            "77 \t759   \t0.719876\t77 \t0.791426\t0.121934 \t759   \t0.124438 \t17.7586\t77 \t31 \t10 \t759   \t2.55446 \n",
            "78 \t781   \t0.722142\t78 \t0.791426\t0.12145  \t781   \t0.117511 \t17.085 \t78 \t31 \t7  \t781   \t2.25882 \n",
            "79 \t763   \t0.722013\t79 \t0.791426\t0.123248 \t763   \t0.119278 \t17.1134\t79 \t30 \t10 \t763   \t2.10219 \n",
            "80 \t762   \t0.715942\t80 \t0.791426\t0.162597 \t762   \t0.125768 \t17.39  \t80 \t28 \t5  \t762   \t2.06431 \n",
            "81 \t768   \t0.726491\t81 \t0.791426\t0.123277 \t768   \t0.107651 \t17.6344\t81 \t28 \t10 \t768   \t2.08548 \n",
            "82 \t742   \t0.725619\t82 \t0.791641\t0.162765 \t742   \t0.109807 \t17.6256\t82 \t27 \t5  \t742   \t2.09211 \n",
            "83 \t769   \t0.72576 \t83 \t0.791641\t0.123277 \t769   \t0.108967 \t17.1554\t83 \t27 \t8  \t769   \t1.99469 \n",
            "84 \t796   \t0.722637\t84 \t0.791641\t0.162555 \t796   \t0.119933 \t17.2141\t84 \t28 \t6  \t796   \t2.34596 \n",
            "85 \t760   \t0.718627\t85 \t0.791641\t0.150808 \t760   \t0.122915 \t17.741 \t85 \t36 \t7  \t760   \t2.69759 \n",
            "86 \t783   \t0.720332\t86 \t0.791641\t0.172849 \t783   \t0.11944  \t18.0518\t86 \t35 \t10 \t783   \t2.50127 \n",
            "87 \t755   \t0.718432\t87 \t0.791641\t0.122918 \t755   \t0.122395 \t18.6813\t87 \t35 \t8  \t755   \t2.52536 \n",
            "88 \t789   \t0.726192\t88 \t0.791713\t0.12154  \t789   \t0.111246 \t19.2463\t88 \t40 \t10 \t789   \t2.26105 \n",
            "89 \t775   \t0.720371\t89 \t0.791713\t0.122918 \t775   \t0.126396 \t19.6061\t89 \t35 \t10 \t775   \t2.16527 \n",
            "90 \t784   \t0.724929\t90 \t0.791713\t0.170841 \t784   \t0.117204 \t20.0577\t90 \t32 \t10 \t784   \t2.71299 \n",
            "91 \t760   \t0.728764\t91 \t0.791713\t0.122918 \t760   \t0.110388 \t20.9091\t91 \t35 \t10 \t760   \t3.32627 \n",
            "92 \t773   \t0.729004\t92 \t0.791714\t0.122918 \t773   \t0.109568 \t21.7126\t92 \t33 \t10 \t773   \t3.5394  \n",
            "93 \t750   \t0.719245\t93 \t0.791763\t0.122918 \t750   \t0.128569 \t22.2913\t93 \t34 \t10 \t750   \t3.00557 \n",
            "94 \t789   \t0.732049\t94 \t0.791763\t0.170446 \t789   \t0.108359 \t22.3324\t94 \t37 \t10 \t789   \t2.4932  \n",
            "95 \t785   \t0.731426\t95 \t0.791763\t0.123252 \t785   \t0.107452 \t22.7576\t95 \t36 \t10 \t785   \t2.57172 \n",
            "96 \t771   \t0.7305  \t96 \t0.791763\t0.123277 \t771   \t0.109453 \t23.6598\t96 \t43 \t10 \t771   \t3.13833 \n",
            "97 \t787   \t0.726659\t97 \t0.791763\t0.122918 \t787   \t0.114076 \t24.6598\t97 \t43 \t10 \t787   \t3.07126 \n",
            "98 \t756   \t0.73186 \t98 \t0.791808\t0.172291 \t756   \t0.105345 \t24.8886\t98 \t43 \t10 \t756   \t2.4273  \n",
            "99 \t768   \t0.725509\t99 \t0.791808\t0.170519 \t768   \t0.118449 \t25.1153\t99 \t34 \t10 \t768   \t2.05125 \n",
            "100\t773   \t0.725488\t100\t0.791808\t0.170845 \t773   \t0.11267  \t25.8475\t100\t38 \t5  \t773   \t2.20951 \n",
            "101\t764   \t0.732496\t101\t0.791808\t0.122918 \t764   \t0.109178 \t26.7058\t101\t40 \t12 \t764   \t2.45037 \n",
            "102\t766   \t0.731044\t102\t0.800652\t0.123277 \t766   \t0.108181 \t27.7517\t102\t38 \t19 \t766   \t2.23212 \n",
            "103\t753   \t0.733708\t103\t0.800652\t0.171246 \t753   \t0.106603 \t28.1496\t103\t40 \t19 \t753   \t1.92278 \n",
            "104\t770   \t0.735133\t104\t0.800652\t0.123277 \t770   \t0.104311 \t27.9629\t104\t40 \t18 \t770   \t1.74543 \n",
            "105\t784   \t0.730617\t105\t0.800697\t0.123277 \t784   \t0.109721 \t27.9443\t105\t40 \t10 \t784   \t2.09543 \n",
            "106\t751   \t0.73136 \t106\t0.800697\t0.123273 \t751   \t0.10768  \t27.4819\t106\t38 \t10 \t751   \t2.28887 \n",
            "107\t756   \t0.731817\t107\t0.800783\t0.122884 \t756   \t0.110317 \t26.5777\t107\t49 \t10 \t756   \t2.6705  \n",
            "108\t803   \t0.732818\t108\t0.80088 \t0.159425 \t803   \t0.113759 \t25.9091\t108\t36 \t9  \t803   \t2.49585 \n",
            "109\t766   \t0.740971\t109\t0.80088 \t0.171249 \t766   \t0.105351 \t26.2434\t109\t36 \t10 \t766   \t2.6073  \n",
            "110\t755   \t0.740427\t110\t0.80088 \t0.171274 \t755   \t0.102754 \t27.306 \t110\t50 \t10 \t755   \t2.19108 \n",
            "111\t776   \t0.745012\t111\t0.801793\t0.171249 \t776   \t0.101324 \t28.0577\t111\t50 \t17 \t776   \t2.17594 \n",
            "112\t770   \t0.739517\t112\t0.801793\t0.170769 \t770   \t0.104152 \t28.4868\t112\t50 \t17 \t770   \t2.64918 \n",
            "113\t771   \t0.74324 \t113\t0.801793\t0.165857 \t771   \t0.100389 \t29.1193\t113\t50 \t17 \t771   \t3.32552 \n",
            "114\t798   \t0.737812\t114\t0.801889\t0.123273 \t798   \t0.103816 \t30.5679\t114\t50 \t19 \t798   \t3.84817 \n",
            "115\t788   \t0.739722\t115\t0.802076\t0.122918 \t788   \t0.105762 \t31.1153\t115\t43 \t18 \t788   \t3.96248 \n",
            "116\t775   \t0.743728\t116\t0.802076\t0.159523 \t775   \t0.097001 \t30.6139\t116\t56 \t16 \t775   \t4.13677 \n",
            "117\t778   \t0.735727\t117\t0.802503\t0.122918 \t778   \t0.108576 \t29.5024\t117\t56 \t19 \t778   \t3.99288 \n",
            "118\t780   \t0.738027\t118\t0.802503\t0.123368 \t780   \t0.107562 \t29.6891\t118\t64 \t7  \t780   \t4.73245 \n",
            "119\t791   \t0.737546\t119\t0.80269 \t0.170757 \t791   \t0.112818 \t31.0762\t119\t56 \t22 \t791   \t5.87105 \n",
            "120\t754   \t0.739959\t120\t0.80269 \t0.172207 \t754   \t0.107562 \t33.2805\t120\t59 \t22 \t754   \t7.42341 \n",
            "121\t757   \t0.745589\t121\t0.802871\t0.172951 \t757   \t0.0941843\t35.652 \t121\t56 \t19 \t757   \t9.07965 \n",
            "122\t770   \t0.733384\t122\t0.802936\t0.123443 \t770   \t0.120021 \t38.7576\t122\t65 \t19 \t770   \t9.94481 \n",
            "123\t778   \t0.739725\t123\t0.802936\t0.191658 \t778   \t0.105411 \t40.1515\t123\t65 \t24 \t778   \t9.63668 \n",
            "124\t772   \t0.746481\t124\t0.803159\t0.213619 \t772   \t0.0946023\t39.7879\t124\t67 \t25 \t772   \t8.98804 \n",
            "125\t796   \t0.736795\t125\t0.803159\t0.123425 \t796   \t0.110849 \t40.219 \t125\t63 \t26 \t796   \t8.42185 \n",
            "126\t767   \t0.744411\t126\t0.803299\t0.172887 \t767   \t0.101572 \t42.564 \t126\t66 \t17 \t767   \t8.08687 \n",
            "127\t771   \t0.749426\t127\t0.803299\t0.172841 \t771   \t0.0854677\t45.086 \t127\t62 \t26 \t771   \t7.31263 \n",
            "128\t767   \t0.739151\t128\t0.803299\t0.171395 \t767   \t0.108095 \t46.6295\t128\t63 \t27 \t767   \t6.55622 \n",
            "129\t777   \t0.745767\t129\t0.803356\t0.172999 \t777   \t0.0959199\t48.0293\t129\t62 \t27 \t777   \t5.63982 \n",
            "130\t766   \t0.743055\t130\t0.803356\t0.172276 \t766   \t0.107327 \t48.5015\t130\t64 \t32 \t766   \t5.93758 \n",
            "131\t788   \t0.740499\t131\t0.803356\t0.123753 \t788   \t0.106269 \t48.3324\t131\t64 \t30 \t788   \t5.98475 \n",
            "132\t773   \t0.745914\t132\t0.803356\t0.193156 \t773   \t0.102486 \t46.566 \t132\t62 \t32 \t773   \t5.91445 \n",
            "133\t792   \t0.739329\t133\t0.803416\t0.172028 \t792   \t0.0993701\t45.5279\t133\t62 \t32 \t792   \t5.46355 \n",
            "134\t785   \t0.743436\t134\t0.803453\t0.17216  \t785   \t0.102108 \t45.7165\t134\t62 \t32 \t785   \t4.9133  \n",
            "135\t795   \t0.745393\t135\t0.803453\t0.172144 \t795   \t0.104623 \t46.2405\t135\t57 \t28 \t795   \t4.14461 \n",
            "136\t767   \t0.747541\t136\t0.803478\t0.17239  \t767   \t0.101348 \t46.3148\t136\t57 \t30 \t767   \t3.3301  \n",
            "137\t794   \t0.745738\t137\t0.803478\t0.172564 \t794   \t0.0969821\t45.8641\t137\t58 \t30 \t794   \t3.33384 \n",
            "138\t791   \t0.742303\t138\t0.803572\t0.169949 \t791   \t0.105513 \t45     \t138\t62 \t30 \t791   \t4.01074 \n",
            "139\t786   \t0.742442\t139\t0.803575\t0.171877 \t786   \t0.0995672\t43.5601\t139\t65 \t30 \t786   \t4.14074 \n",
            "140\t761   \t0.743845\t140\t0.803575\t0.172659 \t761   \t0.107147 \t42.392 \t140\t61 \t30 \t761   \t3.99558 \n",
            "141\t787   \t0.737121\t141\t0.803575\t0.174258 \t787   \t0.11056  \t41.0899\t141\t57 \t28 \t787   \t4.12746 \n",
            "142\t784   \t0.74216 \t142\t0.803655\t0.174314 \t784   \t0.105917 \t40.3099\t142\t52 \t30 \t784   \t4.15614 \n"
          ]
        }
      ],
      "source": [
        "#  DEBUG : REMOVE THIS !!!\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def xmate(ind1, ind2):\n",
        "    \"\"\" Reproduction operator for multi-tree GP, where trees are represented as a list.\n",
        "\n",
        "    Crossover happens to a subtree that is selected at random.\n",
        "    Crossover operations are limited to parents from the same tree.\n",
        "\n",
        "    FIXME: Have to compile the trees (manually), which is frustrating.\n",
        "\n",
        "    Args:\n",
        "        ind1 (Individual): The first parent.\n",
        "        ind2 (Individual): The second parent\n",
        "\n",
        "    Returns:\n",
        "        ind1, ind2 (Individual, Individual): The children from the parents reproduction.\n",
        "    \"\"\"\n",
        "    n = range(len(ind1))\n",
        "    selected_tree_idx = random.choice(n)\n",
        "    for tree_idx in n:\n",
        "        g1, g2 = gp.PrimitiveTree(ind1[tree_idx]), gp.PrimitiveTree(ind2[tree_idx])\n",
        "        if tree_idx == selected_tree_idx:\n",
        "            ind1[tree_idx], ind2[tree_idx] = gp.cxOnePoint(g1, g2)\n",
        "        else:\n",
        "            ind1[tree_idx], ind2[tree_idx] = g1, g2\n",
        "    return ind1, ind2\n",
        "\n",
        "\n",
        "def xmut(ind, expr):\n",
        "    \"\"\" Mutation operator for multi-tree GP, where trees are represented as a list.\n",
        "\n",
        "    Mutation happens to a tree selected at random, when an individual is selected for crossover.\n",
        "\n",
        "    FIXME: Have to compile the trees (manually), which is frustrating.\n",
        "\n",
        "    Args:\n",
        "        ind: The individual, a list of GP trees.\n",
        "    \"\"\"\n",
        "    n = range(len(ind))\n",
        "    selected_tree_idx = random.choice(n)\n",
        "    for tree_idx in n:\n",
        "        g1 = gp.PrimitiveTree(ind[tree_idx])\n",
        "        if tree_idx == selected_tree_idx:\n",
        "            indx = gp.mutUniform(g1, expr, pset)\n",
        "            ind[tree_idx] = indx[0]\n",
        "        else:\n",
        "            ind[tree_idx] = g1\n",
        "    return ind,\n",
        "\n",
        "\n",
        "def evaluate_classification(individual, alpha = 0.9, verbose=False):\n",
        "    \"\"\"\n",
        "    Evalautes the fitness of an individual for multi-tree GP multi-class classification.\n",
        "\n",
        "    We maxmimize the fitness when we evaluate the accuracy + regularization term.\n",
        "\n",
        "    Args:\n",
        "        individual (Individual): A candidate solution to be evaluated.\n",
        "        alpha (float): A parameter that balances the accuracy and regularization term. Defaults to 0.98.\n",
        "\n",
        "    Returns:\n",
        "        accuracy (tuple): The fitness of the individual.\n",
        "    \"\"\"\n",
        "    features = toolbox.compile(expr=individual, pset=pset)\n",
        "    fitness = wrapper_classification_accuracy(features, verbose=verbose)\n",
        "    return fitness,\n",
        "\n",
        "\n",
        "toolbox.register('evaluate', evaluate_classification)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=7)\n",
        "toolbox.register(\"mate\", xmate)\n",
        "toolbox.register(\"expr_mut\", gp.genFull, min_=0, max_=2)\n",
        "toolbox.register(\"mutate\", xmut, expr=toolbox.expr_mut)\n",
        "\n",
        "\n",
        "def staticLimit(key, max_value):\n",
        "    \"\"\"\n",
        "    A variation of gp.staticLimit that works for Multi-tree representation.\n",
        "    This works for our altered xmut and xmate genetic operators for mutli-tree GP.\n",
        "    If tree depth limit is exceeded, the genetic operator is reverted.\n",
        "\n",
        "    When an invalid (over the limit) child is generated,\n",
        "    it is simply replaced by one of its parents, randomly selected.\n",
        "\n",
        "    Args:\n",
        "        key: The function to use in order the get the wanted value. For\n",
        "             instance, on a GP tree, ``operator.attrgetter('height')`` may\n",
        "             be used to set a depth limit, and ``len`` to set a size limit.\n",
        "        max_value: The maximum value allowed for the given measurement.\n",
        "             Defaults to 17, the suggested value in (Koza 1992)\n",
        "\n",
        "    Returns:\n",
        "        A decorator that can be applied to a GP operator using \\\n",
        "        :func:`~deap.base.Toolbox.decorate`\n",
        "\n",
        "    References:\n",
        "        1. Koza, J. R. G. P. (1992). On the programming of computers by means\n",
        "            of natural selection. Genetic programming.\n",
        "    \"\"\"\n",
        "\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            keep_inds = [[copy.deepcopy(tree) for tree in ind] for ind in args]\n",
        "            new_inds = list(func(*args, **kwargs))\n",
        "            for ind_idx, ind in enumerate(new_inds):\n",
        "                for tree_idx, tree in enumerate(ind):\n",
        "                    if key(tree) > max_value:\n",
        "                        random_parent = random.choice(keep_inds)\n",
        "                        new_inds[ind_idx][tree_idx] = random_parent[tree_idx]\n",
        "            return new_inds\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# See https://groups.google.com/g/deap-users/c/pWzR_q7mKJ0\n",
        "toolbox.decorate(\"mate\", staticLimit(key=operator.attrgetter(\"height\"), max_value=8))\n",
        "toolbox.decorate(\"mutate\", staticLimit(key=operator.attrgetter(\"height\"), max_value=8))\n",
        "\n",
        "\n",
        "def SimpleGPWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
        "             halloffame=None, verbose=__debug__):\n",
        "    \"\"\"\n",
        "    Elitism for Multi-Tree GP for Multi-Class classification.\n",
        "    A variation of the eaSimple method from the DEAP library that supports\n",
        "\n",
        "    Elitism ensures the best individuals (the elite) from each generation are\n",
        "    carried onto the next without alteration. This ensures the quality of the\n",
        "    best solution monotonically increases over time.\n",
        "\n",
        "    Args:\n",
        "        population: The number of individuals to evolve.\n",
        "        toolbox: The toolbox containing the genetic operators.\n",
        "        cxpb: The probability of a crossover between two individuals.\n",
        "        mutpb: The probability of a random mutation within an individual.\n",
        "        ngen: The number of genetations to evolve the population for.\n",
        "        stats: That can be used to collect statistics on the evolution.\n",
        "        halloffame: The hall of fame contains the best individual solutions.\n",
        "        verbose: Whether or not to print the logbook.\n",
        "\n",
        "    Returns:\n",
        "        population: The final population the algorithm has evolved.\n",
        "        logbook: The logbook which can record important statistics.\n",
        "    \"\"\"\n",
        "    logbook = tools.Logbook()\n",
        "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
        "\n",
        "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
        "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
        "\n",
        "    for ind, fit in zip(invalid_ind, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    if halloffame is None:\n",
        "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
        "\n",
        "    halloffame.update(population)\n",
        "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
        "\n",
        "    record = stats.compile(population) if stats else {}\n",
        "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
        "\n",
        "    if verbose:\n",
        "        print(logbook.stream)\n",
        "\n",
        "    for gen in range(1, ngen + 1):\n",
        "        offspring = toolbox.select(population, len(population) - hof_size)\n",
        "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
        "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
        "\n",
        "        for ind, fit in zip(invalid_ind, fitnesses):\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        offspring.extend(halloffame.items)\n",
        "        halloffame.update(offspring)\n",
        "        population[:] = offspring\n",
        "\n",
        "        record = stats.compile(population) if stats else {}\n",
        "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
        "\n",
        "        if verbose:\n",
        "            print(logbook.stream)\n",
        "\n",
        "    return population, logbook\n",
        "\n",
        "\n",
        "def train(generations=100, population=100, elitism=0.1, crossover_rate=0.5, mutation_rate=0.1):\n",
        "    \"\"\"\n",
        "    This is a Multi-tree GP with Elitism for Multi-class classification.\n",
        "\n",
        "    Args:\n",
        "        generations: The number of generations to evolve the populaiton for.\n",
        "        elitism: The ratio of elites to be kept between generations.\n",
        "        crossover_rate: The probability of a crossover between two individuals.\n",
        "        mutation_rate: The probability of a random mutation within an individual.\n",
        "\n",
        "    Returns:\n",
        "        pop: The final population the algorithm has evolved.\n",
        "        log: The logbook which can record important statistics.\n",
        "        hof: The hall of fame contains the best individual solutions.\n",
        "\n",
        "    References:\n",
        "        1. Koza, J. R. (1994). Genetic programming II: automatic discovery of\n",
        "          reusable programs.\n",
        "        2. Tran, B., Xue, B., & Zhang, M. (2019).\n",
        "          Genetic programming for multiple-feature construction on\n",
        "          high-dimensional classification. Pattern Recognition, 93, 404-417.\n",
        "        3. Patil, V. P., & Pawar, D. D. (2015). The optimal crossover or mutation\n",
        "          rates in genetic algorithm: a review. International Journal of Applied\n",
        "          Engineering and Technology, 5(3), 38-41.\n",
        "    \"\"\"\n",
        "    # Reproducuble results for each run.\n",
        "    random.seed(run)\n",
        "\n",
        "    pop = toolbox.population(n=population)\n",
        "\n",
        "    # Elitism (Koza 1994)\n",
        "    mu = round(elitism * population)\n",
        "    if elitism > 0:\n",
        "        # See https://www.programcreek.com/python/example/107757/deap.tools.HallOfFame\n",
        "        hof = tools.HallOfFame(mu)\n",
        "    else:\n",
        "        hof = None\n",
        "\n",
        "    stats_fit = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    length = lambda a: np.max(list(map(len, a)))\n",
        "    stats_size = tools.Statistics(length)\n",
        "\n",
        "    mstats = tools.MultiStatistics(fitness=stats_fit, size=stats_size)\n",
        "    mstats.register(\"avg\", np.mean)\n",
        "    mstats.register(\"std\", np.std)\n",
        "    mstats.register(\"min\", np.min)\n",
        "    mstats.register(\"max\", np.max)\n",
        "\n",
        "    # Run the genetic program.\n",
        "    pop, log = SimpleGPWithElitism(pop, toolbox, crossover_rate, mutation_rate,\n",
        "                                   generations, stats=mstats, halloffame=hof,\n",
        "                                   verbose=True)\n",
        "    return pop, log, hof\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DeJong (1975), p=50-100, m=0.001, c=0.6\n",
        "Grefenstette (1986), p=30, m=0.01, c=0.95\n",
        "Schaffer et al., (1989), p=20-30, m=0.005-0.01, c=0.75-0.95\n",
        "\n",
        "References:\n",
        "\n",
        "\"\"\"\n",
        "pop, log, hof = train(generations, population, elitism, crossover_rate, mutation_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSEgttwuilPy"
      },
      "outputs": [],
      "source": [
        "# for i in range(len(hof)):\n",
        "    # print(f\"i: {i}th index\")\n",
        "    # evaluate_classification(hof[i], verbose=True)\n",
        "\n",
        "evaluate_classification(hof[0], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Perform t-SNE dimensionality reduction\n",
        "perplexity = 10\n",
        "features = toolbox.compile(expr=hof[0], pset=pset)\n",
        "evaluate_classification(hof.items[0], verbose=True)\n",
        "\n",
        "for X_set in [X, features]:\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
        "    X_tsne = tsne.fit_transform(X_set)\n",
        "\n",
        "    # Plot the data\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    labels = ['Fillet','Heads','Livers','Skins','Guts','Frames']\n",
        "\n",
        "    # Plot points belonging to different classes with different colors\n",
        "    for idx, label in enumerate(np.unique(y)):\n",
        "        plt.scatter(X_tsne[y == label, 0], X_tsne[y == label, 1], label=labels[idx])\n",
        "\n",
        "    plt.title('t-SNE Visualization of Fish Parts Dataset')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fRYbQEfvceBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming you have your own dataset with features X and labels y\n",
        "# Replace X and y with your actual data\n",
        "# X should be your feature matrix and y should be your labels\n",
        "\n",
        "for X_set in [X, features]:\n",
        "    # Perform PCA dimensionality reduction\n",
        "    pca = PCA(n_components=3)\n",
        "    X_pca = pca.fit_transform(X_set)\n",
        "\n",
        "    # Plot the data\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    labels = ['Fillet','Heads','Livers','Skins','Guts','Frames']\n",
        "\n",
        "    # Plot points belonging to different classes with different colors\n",
        "    for idx, label in enumerate(np.unique(y)):\n",
        "        ax.scatter(X_pca[y == label, 0], X_pca[y == label, 1], X_pca[y == label, 2], label=labels[idx])\n",
        "\n",
        "    ax.set_title('PCA Visualization of Fish Parts Dataset')\n",
        "    ax.set_xlabel('Principal Component 1')\n",
        "    ax.set_ylabel('Principal Component 2')\n",
        "    ax.set_zlabel('Principal Component 3')\n",
        "    ax.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KizUbrqkTxC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Assuming you have your own dataset with features X and labels y\n",
        "# Replace X and y with your actual data\n",
        "# X should be your feature matrix and y should be your labels\n",
        "\n",
        "for X_set in [X, features]:\n",
        "\n",
        "    # Perform t-SNE dimensionality reduction\n",
        "    tsne = TSNE(n_components=3, perplexity=10, random_state=42)\n",
        "    X_tsne = tsne.fit_transform(X_set)\n",
        "\n",
        "    # Plot the data\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    labels = ['Fillet','Heads','Livers','Skins','Guts','Frames']\n",
        "\n",
        "    # Plot points belonging to different classes with different colors\n",
        "    for idx, label in enumerate(np.unique(y)):\n",
        "        ax.scatter(X_tsne[y == label, 0], X_tsne[y == label, 1], X_tsne[y == label, 2], label=labels[idx])\n",
        "\n",
        "    ax.set_title('t-SNE Visualization of Fish Parts Dataset')\n",
        "    ax.set_xlabel('t-SNE Dimension 1')\n",
        "    ax.set_ylabel('t-SNE Dimension 2')\n",
        "    ax.set_zlabel('t-SNE Dimension 3')\n",
        "    ax.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Y5B7evEZRx33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "feature_no = 4\n",
        "data = pd.DataFrame(features[:30,:feature_no], columns=[f'feature_{i}' for i in range(feature_no)])\n",
        "data['class'] = y[:30]\n",
        "\n",
        "# Add class labels to the DataFrame\n",
        "labels = ['Fillet','Heads','Livers','Skins','Guts','Frames']\n",
        "\n",
        "# Create pairplot\n",
        "plot = sns.pairplot(data, hue='class', palette='viridis')\n",
        "\n",
        "# Modify legend\n",
        "handles = plot._legend_data.values()\n",
        "plt.legend(handles, labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YXvJAx9ARaH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the fitness value from the training output.\n",
        "! cat results.txt | awk '{ print $5 }' | sed 's/$/,/'"
      ],
      "metadata": {
        "id": "dAUtkIVDWKap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitness = [\n",
        " 0.419297,\n",
        "0.48275,\n",
        "0.489763,\n",
        "0.526699,\n",
        "0.530472,\n",
        "0.550908,\n",
        "0.590023,\n",
        "0.624244,\n",
        "0.648323,\n",
        "0.659196,\n",
        "0.659196,\n",
        "0.729868,\n",
        "0.747645,\n",
        "0.749064,\n",
        "0.769127,\n",
        "0.769127,\n",
        "0.809127,\n",
        "0.809127,\n",
        "0.809127,\n",
        "0.80951,\n",
        "0.80951,\n",
        "0.832581,\n",
        "0.832581,\n",
        "0.832581,\n",
        "0.832581,\n",
        "0.833219,\n",
        "0.833219,\n",
        "0.833219,\n",
        "0.860261,\n",
        "0.862216,\n",
        "0.862216,\n",
        "0.862216,\n",
        "0.862216,\n",
        "0.862695,\n",
        "0.862753,\n",
        "0.862753,\n",
        "0.862753,\n",
        "0.86316,\n",
        "0.863465,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.864027,\n",
        "0.901095,\n",
        "0.901095,\n",
        "0.901095,\n",
        "0.901096,\n",
        "0.901106,\n",
        "0.901106,\n",
        "0.901106,\n",
        "0.90111,\n",
        "0.901118,\n",
        "0.901118,\n",
        "0.901126,\n",
        "0.901127,\n",
        "0.902822,\n",
        "0.902822,\n",
        "0.902822,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904569,\n",
        "0.904745,\n",
        "0.904745,\n",
        "0.904745,\n",
        "0.904745,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905098,\n",
        "0.905355,\n",
        "0.905355,\n",
        "0.905355,\n",
        "0.905355,\n",
        "0.905355,\n",
        "0.905355,\n",
        "0.905355,\n",
        "0.905355,\n",
        "0.905355,\n",
        "0.905612,\n",
        "0.905612,\n",
        "0.905612,\n",
        "0.905612,\n",
        "0.905821,\n",
        "0.905821,\n",
        "0.905821,\n",
        "0.905821,\n",
        "0.906142,\n",
        "0.906142,\n",
        "0.906142,\n",
        "0.906142,\n",
        "0.906142,\n",
        "0.906256,\n",
        "0.906857,\n",
        "0.906857,\n",
        "0.906857,\n",
        "0.906857,\n",
        "0.906885,\n",
        "0.906902,\n",
        "0.906902,\n",
        "0.906902,\n",
        "0.906902,\n",
        "0.906902,\n",
        "0.906916,\n",
        "0.906916,\n",
        "0.906916,\n",
        "0.906916,\n",
        "0.906916,\n",
        "0.907324,\n",
        "0.907324,\n",
        "0.907324,\n",
        "0.907324,\n",
        "0.907344,\n",
        "0.907667,\n",
        "0.907667,\n",
        "0.907667,\n",
        "0.907667,\n",
        "0.907892,\n",
        "0.907892,\n",
        "0.90794,\n",
        "0.90794,\n",
        "0.90816,\n",
        "0.90816,\n",
        "0.908205,\n",
        "0.908259,\n",
        "0.908259,\n",
        "0.908259,\n",
        "0.908259,\n",
        "0.90826,\n",
        "0.908279,\n",
        "0.908297,\n",
        "0.908297,\n",
        "0.908297,\n",
        "0.908301,\n",
        "0.908308,\n",
        "0.90834,\n",
        "0.908373,\n",
        "0.908414,\n",
        "0.908477,\n",
        "0.908477,\n",
        "0.908477,\n",
        "0.908477,\n",
        "0.908511,\n",
        "0.908511,\n",
        "0.908511,\n",
        "0.908511,\n",
        "0.908511,\n",
        "0.908543,\n",
        "0.908543,\n",
        "0.908543,\n",
        "0.908543,\n",
        "0.908543,\n",
        "0.908543,\n",
        "0.908566,\n",
        "0.908589,\n",
        "0.908645,\n",
        "0.908786,\n",
        "0.908786,\n",
        "0.908786,\n",
        "0.908835,\n",
        "0.908912,\n",
        "0.908912,\n",
        "0.908912,\n",
        "0.908912,\n",
        "0.908912,\n",
        "0.908956,\n",
        "0.908987,\n",
        "0.908987,\n",
        "0.908987,\n",
        "0.908987,\n",
        "0.908987,\n",
        "0.908987,\n",
        "0.908987,\n",
        "0.908987,\n",
        "0.909046,\n",
        "0.909046,\n",
        "0.909046,\n",
        "0.909046,\n",
        "0.909046,\n",
        "0.909046,\n",
        "0.909046,\n",
        "0.909046,\n",
        "0.909046,\n",
        "0.909047,\n",
        "0.909047,\n",
        "0.909062,\n",
        "0.909067,\n",
        "0.90908,\n",
        "0.90908,\n",
        "0.90909,\n",
        "0.90909,\n",
        "0.909136,\n",
        "0.909136,\n",
        "0.909136,\n",
        "0.909136,\n",
        "0.909169,\n",
        "0.909169,\n",
        "0.909169,\n",
        "0.909183,\n",
        "0.909236,\n",
        "0.909236,\n",
        "0.909236,\n",
        "0.909236,\n",
        "0.909266,\n",
        "0.909266,\n",
        "0.909266,\n",
        "0.909266,\n",
        "0.909266,\n",
        "0.909297,\n",
        "0.909297,\n",
        "0.909297,\n",
        "0.909311,\n",
        "0.909311,\n",
        "0.909311,\n",
        "0.909311,\n",
        "0.909311,\n",
        "0.909311,\n",
        "0.909311,\n",
        "0.909311,\n",
        "0.909317,\n",
        "0.909317,\n",
        "0.909317,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909335,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909352,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909353,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909354,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909355,\n",
        "0.909356,\n",
        "0.909356,\n",
        "0.909356,\n",
        "0.909356,\n",
        "0.909356,\n",
        "0.909356,\n",
        "0.909356,\n",
        "0.909356,\n",
        "0.909356,\n",
        "0.909357,\n",
        "0.909357,\n",
        "0.909357,\n",
        "0.909357,\n",
        "0.909357,\n",
        "0.909357,\n",
        "0.909358,\n",
        "0.909358,\n",
        "0.909358,\n",
        "0.909358,\n",
        "0.909358,\n",
        "0.909359,\n",
        "0.909359,\n",
        "0.90936,\n",
        "0.90936,\n",
        "0.90936,\n",
        "0.909361,\n",
        "0.909361,\n",
        "0.909371,\n",
        "0.909371,\n",
        "0.909371,\n",
        "0.909372,\n",
        "0.909375,\n",
        "0.909375,\n",
        "0.909383,\n",
        "0.909385,\n",
        "0.909385,\n",
        "0.9094,\n",
        "0.9094,\n",
        "0.909405,\n",
        "0.909405,\n",
        "0.909417,\n",
        "0.90942,\n",
        "0.90942,\n",
        "0.909428,\n",
        "0.909428,\n",
        "0.909439,\n",
        "0.909451,\n",
        "0.909451,\n",
        "0.909453,\n",
        "0.909476,\n",
        "0.909477,\n",
        "0.909477,\n",
        "0.909517,\n",
        "0.909517,\n",
        "0.909521,\n",
        "0.909577,\n",
        "0.910173,\n",
        "0.910173,\n",
        "0.910188,\n",
        "0.910188,\n",
        "0.910339,\n",
        "0.910339,\n",
        "0.910341,\n",
        "0.910341,\n",
        "0.910905,\n",
        "0.910905,\n",
        "0.911118,\n",
        "0.911767,\n",
        "0.911767,\n",
        "0.911793,\n",
        "0.9119,\n",
        "0.9119,\n",
        "0.911947,\n",
        "0.911952,\n",
        "0.911952,\n",
        "0.911968,\n",
        "0.911993,\n",
        "0.911993,\n",
        "0.911993,\n",
        "0.912008,\n",
        "0.912099,\n",
        "0.912099,\n",
        "0.912099,\n",
        "0.912102,\n",
        "0.912108,\n",
        "0.912109,\n",
        "0.912109,\n",
        "0.912133,\n",
        "0.912133,\n",
        "0.912133,\n",
        "0.912134,\n",
        "0.912134,\n",
        "0.912134,\n",
        "0.912134,\n",
        "0.912146,\n",
        "0.912148,\n",
        "0.912149,\n",
        "0.912176,\n",
        "0.912176,\n",
        "0.912176,\n",
        "0.912176,\n",
        "0.912176,\n",
        "0.912183,\n",
        "0.912187,\n",
        "0.912204,\n",
        "0.912204,\n",
        "0.912204,\n",
        "0.912209,\n",
        "0.912209,\n",
        "0.912209,\n",
        "0.912217,\n",
        "0.912217,\n",
        "0.912217,\n",
        "0.912217,\n",
        "0.912217,\n",
        "0.912219,\n",
        "0.912223,\n",
        "0.912263,\n",
        "0.912263,\n",
        "0.912263,]\n",
        "\n",
        "plt.plot(fitness)\n",
        "plt.title(\"Fitness: evolutionary process\")\n",
        "plt.xlabel(\"generation\")\n",
        "plt.ylabel(\"fitness\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z7FefL7NWsmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkaU6kAGyBl2"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEqouLDs4xBJ"
      },
      "outputs": [],
      "source": [
        "from deap import base, creator, gp\n",
        "import pygraphviz as pgv\n",
        "\n",
        "multi_tree = hof[0]\n",
        "for t_idx,tree in enumerate(multi_tree):\n",
        "    nodes, edges, labels = gp.graph(tree)\n",
        "\n",
        "    g = pgv.AGraph()\n",
        "    g.add_nodes_from(nodes)\n",
        "    g.add_edges_from(edges)\n",
        "    g.layout(prog=\"dot\")\n",
        "\n",
        "    for i in nodes:\n",
        "        n = g.get_node(i)\n",
        "        n.attr[\"label\"] = labels[i]\n",
        "\n",
        "    g.draw(f\"tree-{t_idx}.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX_g4MipuQTU"
      },
      "source": [
        "## Changelog\n",
        "\n",
        "| Date | Title | Description | Update |\n",
        "| --- | --- | --- | ---- |\n",
        "| 2024-04-11 17:35 | Embedded class-dependent GP | Changed from a wrapper-based multi-tree GP to an embedded class dependent multi-tree GP |\n",
        "| 2024-03-28 17:22 | KNN + cross validation | Changed the wrapper-based classifier to be KNN <br> Implemented cross validation due to low sample volume | |\n",
        "| 2024-03-06 15:25 | SVM random state | Set the random state for the SVM to a fixed value | |\n",
        "| 2024-02-27 16:31 | Fix fitness | Fixed fitness function to penalize intra-class distance <br> and reward inter-class distance. |\n",
        "| 2024-02-26 15:59 | Intra/inter | Updated fitness value to include a normalized sum of <br> intraclass and interclass distance |\n",
        "| 2024-02-14 16:56 | Mass spectra | Applications for MT-GP for rapid mass spectrometry dataset. | |\n",
        "| 2022-08-21 17:30 | Multi-Objective - Onehot Encoding | Change to multi-objective problem, one-vs-all with a tree classifier for each class.<br> Y labels are encoded in onehot encodings, error is absolute difference between $|\\hat{y} - y|$|\n",
        "| 2022-08-22 20:44 | Non-linearity |  Introduce $ round . sigmoid $ to evaluate_classification() method.<br>Previously, we push each tree to predict either a 0 or 1 value with the onehot encoding representation.<br>Now, the non-linearity will map any negative value to a negative class 0, and any positive value to positive class 1.|\n",
        "| 2022-08-22 21:06 | ~~Genetic operators for tree with worst fitness~~ | Only apply the genetic operators, crossover and mutation, to the tree with the worst fitness.<br> This guarantees monotonic improvement for the Multi-tree between generations, the best performing tree remain unaltered.| (Update) This was very slow, and inefficient,<br> basically turned the GP into a single objective,<br>that balances multi-objective fitness functions. |\n",
        "| 2022-08-22 21:15 | Halloffame Equality Operator | Numpy equality function (operators.eq) between two arrays returns the equality element wise,<br>which raises an exception in the if similar() check of the hall of fame. <br> Using a different equality function like numpy.array_equal or numpy.allclose solve this issue.|\n",
        "| 2022-08-22 23:22 | Elitism as aggregate best tree | Perform elitsim by constructing the best tree, as the tree with best fitness from each clas.<br>The goal is to have monotonous improvement across the multiple objective functions.|\n",
        "| 2022-08-22 23:32 | Update fitness for elite | The elitism was not working as intended, as the multi-objectives didn't appear to increase monotnously.<br> This was because the aggregate fitness was not being assigned to the best individual after it was created.<br>Therefore the best invidiual was not passed on to the next generation. |\n",
        "| 2022-08-22 02:28 | staticLimit max height | Rewrite the gp.staticLimit decorator function to handle the Multi-tree representation.<br>Note: size and depth are different values!<br>depth is the maximum length of a root-to-leaf traversal,<br>size is the total number of nodes.|\n",
        "| 2022-08-24 9:37 | Evaluate Mutli-tree train accuracy | Take the classification accuracy as the argmax for the aggregate multitree.<br> 74% training accuracy, which is not ideal, but this shall improve with time.|\n",
        "| 2022-08-25 13:30 | Single-objective fitness | Change the fitness function to a single objective fitness function.<br>This forces the multi-tree GP to find the best tree subset for one-vs-rest classification performance.|\n",
        "| 2022-08-25 20:01 | Fitness = Balanced accuracy + distance measure | Implement the fitness function for MCIFC, but for multi-class classification from (Tran 2019) |\n",
        "| 2022-08-26 21:27 | Sklearn Balanced Accuracy | Changed to the balanced accuracy metric from sklearn.<br>This is much easier to use for now, probably faster than the previous method as well. |\n",
        "| 2022-09-05 17:00 | Reject invalid predictions | Change the fitness function to reject invalid predictioctions outright -<br>e.g. multi-label or zero-label predictions<br>- when computing the balanced accuracy for the fitness function. |\n",
        "| 2022-09-13 19:00 | Mutation + Crossover = 100% | Ensure the mutation and crossover rate sum to 100%,<br>not necessary with deap, but good to avoid conference questions |\n",
        "| 2022-09-13 21:00 | Feature Construction | Changed to Wrapper-based Feature Construction with Multi-tree GP.|\n",
        "| 2022-09-13 21:34 | $m = r \\times c$ | Add more trees, following example from (Tran 2019).<br> With 8 trees for a multi-class classification<br> $m = r \\times c = 8$ trees, where number of classes $c = 4$, and reconstruction ratio $r = 2$/ |\n",
        "| 2022-09-30 19:49 | Quick Evalaute | Manually parse the GP trees, 5x speedup for DEAP in Python (Zhang 2022). |\n",
        "| 2022-10-13 6:02 | Ignore timestamps after 4500 | Ignore timestamps after 4500 did not improve accuracy for SVM classifier.<br>So the bizzare pattern that occurs on GC-MS image there has important information.<br> Should investigate this further, perhaps ask Daniel as he is a domain expert.|\n",
        "| 2022-10-13 22:16 | Cross validation | Evaluate the mean balanced classification accuracy over stratified k-fold cross validation.|\n",
        "| 2023-01-13 20:58 | 2x speedup | Only evaluate test set for verbose alternative of the evaluate_classification method.<br> This results in a 2x speedup in the efficiency of the training regime.|"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1Yg4t38NHSYPAlu_099cQeOR-qSwIaaTl",
      "authorship_tag": "ABX9TyNSafWc39kNBCSWZ1pVFhjS",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}