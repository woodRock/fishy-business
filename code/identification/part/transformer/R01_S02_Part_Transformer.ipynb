{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZBVt0nZSnXe7wYc+Z4hYk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/fishy-business/blob/main/code/identification/part/transformer/R01_S02_Part_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1E-5 #@param {type:\"integer\"}\n",
        "batch_size = 64 # @param {type:\"integer\"}\n",
        "epochs = 100 # @param {type:\"integer\"}\n",
        "is_next_spectra = True # @param {type:\"boolean\"}\n",
        "is_masked_spectra = True # @param {type:\"boolean\"}"
      ],
      "metadata": {
        "id": "5ZOG3XjPWFOh"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.listdir('/content/drive/My Drive')\n",
        "\n",
        "path = ['drive', 'MyDrive', 'AI', 'fish', 'REIMS_data.xlsx']\n",
        "path = os.path.join(*path)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, samples, labels):\n",
        "        self.samples = torch.tensor(samples, dtype=torch.float32)\n",
        "        self.labels = torch.tensor([np.array(ys) for ys in labels], dtype=torch.float32)\n",
        "        # Normalize the features to be between in [0,1]\n",
        "        self.samples = F.normalize(self.samples, dim = 0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "print(\"[INFO] Reading the dataset.\")\n",
        "raw = pd.read_excel(path)\n",
        "\n",
        "data = raw[~raw['m/z'].str.contains('HM')]\n",
        "data = data[~data['m/z'].str.contains('QC')]\n",
        "data = data[~data['m/z'].str.contains('HM')]\n",
        "X = data.drop('m/z', axis=1) # X contains only the features.\n",
        "y = data['m/z'].apply(lambda x:\n",
        "                          [1,0,0,0,0,0] if 'Fillet' in x\n",
        "                    else ([0,1,0,0,0,0] if 'Heads' in x\n",
        "                    else ([0,0,1,0,0,0] if 'Livers' in x\n",
        "                    else ([0,0,0,1,0,0] if 'Skins' in x\n",
        "                    else ([0,0,0,0,1,0] if 'Guts' in x\n",
        "                    else ([0,0,0,0,0,1] if 'Frames' in x\n",
        "                    else None ))))))  # Labels (0 for Hoki, 1 for Moki)\n",
        "xs = []\n",
        "ys = []\n",
        "for (x,y) in zip(X.to_numpy(),y):\n",
        "    if y is not None:\n",
        "       xs.append(x)\n",
        "       ys.append(y)\n",
        "X = np.array(xs)\n",
        "y = np.array(ys)\n",
        "\n",
        "# Evaluation parameters.\n",
        "train_split = 0.8\n",
        "val_split = 0.5 # 1/2 of 20%, validation and test, 10% and 10%, respectively.\n",
        "\n",
        "# Step 2: Split your dataset into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, stratify=y, test_size=(1-train_split))\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_split)\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "val_dataset = CustomDataset(X_val, y_val)\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "\n",
        "assert train_dataset.samples.shape[0] == train_dataset.labels.shape[0] , \"train_dataset samples and labels should have same length.\"\n",
        "assert val_dataset.samples.shape[0] == val_dataset.labels.shape[0] , \"train_dataset samples and labels should have same length.\"\n",
        "assert test_dataset.samples.shape[0] == test_dataset.labels.shape[0] , \"train_dataset samples and labels should have same length.\"\n",
        "\n",
        "# Step 4: Create PyTorch DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# calculate steps per epoch for training and validation set\n",
        "train_steps = len(train_loader.dataset) // batch_size\n",
        "val_steps = len(val_loader.dataset) // batch_size\n",
        "# when batch_size greater than dataset size, avoid division by zero.\n",
        "train_steps = max(1, train_steps)\n",
        "val_steps = max(1, val_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnXm6P13vhb3",
        "outputId": "b9c18501-1f76-47a1-eb0e-5a3de79fea6c"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[INFO] Reading the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert input_dim % num_heads == 0\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = input_dim // num_heads\n",
        "\n",
        "        self.query = nn.Linear(input_dim, input_dim)\n",
        "        self.key = nn.Linear(input_dim, input_dim)\n",
        "        self.value = nn.Linear(input_dim, input_dim)\n",
        "        self.fc_out = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\" Attention mechanism (Vaswani 2017)\"\"\"\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # Split the heads\n",
        "        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Energy-based models\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "\n",
        "        x = torch.matmul(attention, V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.input_dim)\n",
        "\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
        "        # Dropout (Hinton 2012, Srivastava 2014)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # GELU (Hendrycks 2016)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        # Dropout (Hinton 2012, Srivastava 2014)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, hidden_dim, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(input_dim, num_heads)\n",
        "        self.feed_forward = FeedForward(input_dim, hidden_dim)\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Layer normalization (Ba 2016)\n",
        "        # Pre-norm formulation (Xiong 2020, Karpathy 2023)\n",
        "        x_norm = self.norm1(x)\n",
        "        atttention = self.self_attention(x_norm, x_norm, x_norm, mask)\n",
        "        # Residual connections (He 2016)\n",
        "        # Dropout (Srivastava 2014, Hinton 2012)\n",
        "        x = x + self.dropout1(atttention)\n",
        "        x_norm = self.norm2(x)\n",
        "        feed_forward_out = self.feed_forward(x_norm)\n",
        "        x = x + self.dropout2(feed_forward_out)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, num_layers, num_heads, hidden_dim, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(input_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, hidden_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(input_dim, num_heads)\n",
        "        self.cross_attention = MultiHeadAttention(input_dim, num_heads)\n",
        "        self.feed_forward = FeedForward(input_dim, hidden_dim)\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\" Attention mechanism (Vasawin 2017)\"\"\"\n",
        "        # Layer normalization (Ba 2016)\n",
        "        # Pre-norm formulation (Xiong 2020, Karpathy 2023)\n",
        "        x_norm = self.norm1(x)\n",
        "        # Self attention (Vaswani 2017)\n",
        "        attention = self.self_attention(x_norm, x_norm, x_norm, tgt_mask)\n",
        "        # Residual connections (He 2016)\n",
        "        # Dropout (Srivastava 2014, Hinton 2012)\n",
        "        x = x + self.dropout1(attention)\n",
        "        x_norm = self.norm2(x)\n",
        "        # Cross attention (Vaswani 2017)\n",
        "        cross_attention = self.cross_attention(x_norm, encoder_output, encoder_output, src_mask)\n",
        "        x = x + self.dropout2(cross_attention)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, num_layers, num_heads, hidden_dim, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(input_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    References:\n",
        "    1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\n",
        "        A. N., ... & Polosukhin, I. (2017).\n",
        "        Attention is all you need.\n",
        "        Advances in neural information processing systems, 30.\n",
        "    2. He, K., Zhang, X., Ren, S., & Sun, J. (2016).\n",
        "        Deep residual learning for image recognition.\n",
        "        In Proceedings of the IEEE conference on\n",
        "        computer vision and pattern recognition (pp. 770-778).\n",
        "    3. LeCun, Y. (1989). Generalization and network design strategies.\n",
        "        Connectionism in perspective, 19(143-155), 18.\n",
        "    4. LeCun, Y., Boser, B., Denker, J., Henderson, D., Howard,\n",
        "        R., Hubbard, W., & Jackel, L. (1989).\n",
        "        Handwritten digit recognition with a back-propagation network.\n",
        "        Advances in neural information processing systems, 2.\n",
        "    5. LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E.,\n",
        "        Hubbard, W., & Jackel, L. D. (1989).\n",
        "        Backpropagation applied to handwritten zip code recognition.\n",
        "        Neural computation, 1(4), 541-551.\n",
        "    6. Hendrycks, D., & Gimpel, K. (2016).\n",
        "        Gaussian error linear units (gelus).\n",
        "        arXiv preprint arXiv:1606.08415.\n",
        "    7. Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016).\n",
        "        Layer normalization. arXiv preprint arXiv:1607.06450.\n",
        "    8. Srivastava, N., Hinton, G., Krizhevsky, A.,\n",
        "        Sutskever, I., & Salakhutdinov, R. (2014).\n",
        "        Dropout: a simple way to prevent neural networks from overfitting.\n",
        "        The journal of machine learning research, 15(1), 1929-1958.\n",
        "    9. Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever,\n",
        "        I., & Salakhutdinov, R. R. (2012).\n",
        "        Improving neural networks by preventing co-adaptation of feature detectors.\n",
        "        arXiv preprint arXiv:1207.0580.\n",
        "    10. Glorot, X., & Bengio, Y. (2010, March).\n",
        "        Understanding the difficulty of training deep feedforward neural networks.\n",
        "        In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256).\n",
        "        JMLR Workshop and Conference Proceedings.\n",
        "    11. Loshchilov, I., & Hutter, F. (2017).\n",
        "        Decoupled weight decay regularization.\n",
        "        arXiv preprint arXiv:1711.05101.\n",
        "    12. Goodfellow, Ian, Yoshua Bengio, and Aaron Courville.\n",
        "        Deep learning. MIT press, 2016.\n",
        "    13. Morgan, N., & Bourlard, H. (1989).\n",
        "        Generalization and parameter estimation in feedforward nets:\n",
        "        Some experiments. Advances in neural information processing systems, 2.\n",
        "    14. Xiong, R., Yang, Y., He, D., Zheng, K.,\n",
        "        Zheng, S., Xing, C., ... & Liu, T. (2020, November).\n",
        "        On layer normalization in the transformer architecture.\n",
        "        In International Conference on Machine Learning (pp. 10524-10533). PMLR.\n",
        "    14. Karpathy, Andrej (2023)\n",
        "        Let's build GPT: from scratch, in code, spelled out.\n",
        "        YouTube https://youtu.be/kCc8FmEb1nY?si=1vM4DhyqsGKUSAdV\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, num_layers, num_heads, hidden_dim, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, num_layers, num_heads, hidden_dim, dropout)\n",
        "        self.decoder = Decoder(input_dim, num_layers, num_heads, hidden_dim, dropout)\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        x = self.encoder(src, src_mask)\n",
        "        x = self.decoder(tgt, x, src_mask, tgt_mask)\n",
        "        x = self.fc(x[:, 0, :])\n",
        "        return x"
      ],
      "metadata": {
        "id": "bygy82tLE9N5"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "def pre_train_masked_spectra(model, filepath=\"next_spectra_model_weights.pth\", mask_prob=0.2):\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        model.train()\n",
        "\n",
        "        for (x,y) in train_loader:\n",
        "            # Generate batch of data\n",
        "            tgt_x, x = x.to(device), x.to(device)\n",
        "\n",
        "            batch_size = x.shape[0]\n",
        "            mask = torch.rand(batch_size, 1023) < mask_prob\n",
        "            mask = mask.to(device)\n",
        "            x[mask] = 0\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x, x)\n",
        "            loss = criterion(outputs, tgt_x)  # Compare predicted spectra with true spectra\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        total_val_loss = 0.0\n",
        "        model.eval()\n",
        "        for (x,y) in val_loader:\n",
        "            tgt_x, x = x.to(device), x.to(device)\n",
        "\n",
        "            val_batch_size = x.shape[0]\n",
        "            mask = torch.rand(val_batch_size, 1023) < mask_prob\n",
        "            mask = mask.to(device)\n",
        "            x[mask] = 0\n",
        "\n",
        "            outputs = model(x, x)\n",
        "            val_loss = criterion(outputs, tgt_x)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/batch_size:.4f}, Val: {val_loss/val_batch_size:.4f}')\n",
        "\n",
        "    masked_spectra_prediction = model\n",
        "    torch.save(masked_spectra_prediction.state_dict(), filepath)\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters\n",
        "input_dim = 1023\n",
        "output_dim = 1023  # Same as input_dim for masked spectra prediction\n",
        "num_layers = 3\n",
        "num_heads = 3\n",
        "hidden_dim = 128\n",
        "dropout = 0.2\n",
        "learning_rate = 1E-4\n",
        "batch_size = 64\n",
        "num_epochs = 100\n",
        "\n",
        "if is_masked_spectra:\n",
        "    # Load the transformer.\n",
        "    model = Transformer(input_dim, output_dim, num_layers, num_heads, hidden_dim, dropout)\n",
        "    # Specify the device (GPU or CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    # Initialize your model, loss function, and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model = pre_train_masked_spectra(model, filepath=\"next_spectra_model_weights.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd2kNbQLa63y",
        "outputId": "1ce53536-0fe5-46e7-fcfa-4121dac99474"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.0101, Val: 0.1741\n",
            "Epoch [2/100], Loss: 0.0087, Val: 0.1532\n",
            "Epoch [3/100], Loss: 0.0084, Val: 0.1371\n",
            "Epoch [4/100], Loss: 0.0073, Val: 0.1298\n",
            "Epoch [5/100], Loss: 0.0069, Val: 0.1211\n",
            "Epoch [6/100], Loss: 0.0067, Val: 0.1234\n",
            "Epoch [7/100], Loss: 0.0064, Val: 0.1119\n",
            "Epoch [8/100], Loss: 0.0062, Val: 0.1041\n",
            "Epoch [9/100], Loss: 0.0061, Val: 0.1046\n",
            "Epoch [10/100], Loss: 0.0059, Val: 0.0973\n",
            "Epoch [11/100], Loss: 0.0057, Val: 0.1007\n",
            "Epoch [12/100], Loss: 0.0056, Val: 0.0939\n",
            "Epoch [13/100], Loss: 0.0056, Val: 0.0931\n",
            "Epoch [14/100], Loss: 0.0053, Val: 0.0881\n",
            "Epoch [15/100], Loss: 0.0053, Val: 0.0893\n",
            "Epoch [16/100], Loss: 0.0053, Val: 0.0880\n",
            "Epoch [17/100], Loss: 0.0054, Val: 0.0833\n",
            "Epoch [18/100], Loss: 0.0051, Val: 0.0816\n",
            "Epoch [19/100], Loss: 0.0048, Val: 0.0891\n",
            "Epoch [20/100], Loss: 0.0048, Val: 0.0789\n",
            "Epoch [21/100], Loss: 0.0047, Val: 0.0777\n",
            "Epoch [22/100], Loss: 0.0045, Val: 0.0801\n",
            "Epoch [23/100], Loss: 0.0046, Val: 0.0858\n",
            "Epoch [24/100], Loss: 0.0047, Val: 0.0779\n",
            "Epoch [25/100], Loss: 0.0047, Val: 0.0792\n",
            "Epoch [26/100], Loss: 0.0043, Val: 0.0740\n",
            "Epoch [27/100], Loss: 0.0044, Val: 0.0704\n",
            "Epoch [28/100], Loss: 0.0042, Val: 0.0738\n",
            "Epoch [29/100], Loss: 0.0040, Val: 0.0762\n",
            "Epoch [30/100], Loss: 0.0041, Val: 0.0714\n",
            "Epoch [31/100], Loss: 0.0041, Val: 0.0781\n",
            "Epoch [32/100], Loss: 0.0043, Val: 0.0743\n",
            "Epoch [33/100], Loss: 0.0042, Val: 0.0739\n",
            "Epoch [34/100], Loss: 0.0039, Val: 0.0763\n",
            "Epoch [35/100], Loss: 0.0039, Val: 0.0722\n",
            "Epoch [36/100], Loss: 0.0039, Val: 0.0696\n",
            "Epoch [37/100], Loss: 0.0038, Val: 0.0724\n",
            "Epoch [38/100], Loss: 0.0037, Val: 0.0680\n",
            "Epoch [39/100], Loss: 0.0039, Val: 0.0764\n",
            "Epoch [40/100], Loss: 0.0036, Val: 0.0739\n",
            "Epoch [41/100], Loss: 0.0038, Val: 0.0706\n",
            "Epoch [42/100], Loss: 0.0036, Val: 0.0684\n",
            "Epoch [43/100], Loss: 0.0035, Val: 0.0752\n",
            "Epoch [44/100], Loss: 0.0035, Val: 0.0711\n",
            "Epoch [45/100], Loss: 0.0034, Val: 0.0699\n",
            "Epoch [46/100], Loss: 0.0035, Val: 0.0669\n",
            "Epoch [47/100], Loss: 0.0036, Val: 0.0692\n",
            "Epoch [48/100], Loss: 0.0032, Val: 0.0653\n",
            "Epoch [49/100], Loss: 0.0035, Val: 0.0706\n",
            "Epoch [50/100], Loss: 0.0033, Val: 0.0663\n",
            "Epoch [51/100], Loss: 0.0032, Val: 0.0661\n",
            "Epoch [52/100], Loss: 0.0032, Val: 0.0677\n",
            "Epoch [53/100], Loss: 0.0032, Val: 0.0658\n",
            "Epoch [54/100], Loss: 0.0032, Val: 0.0670\n",
            "Epoch [55/100], Loss: 0.0031, Val: 0.0687\n",
            "Epoch [56/100], Loss: 0.0031, Val: 0.0619\n",
            "Epoch [57/100], Loss: 0.0031, Val: 0.0634\n",
            "Epoch [58/100], Loss: 0.0032, Val: 0.0664\n",
            "Epoch [59/100], Loss: 0.0030, Val: 0.0613\n",
            "Epoch [60/100], Loss: 0.0031, Val: 0.0620\n",
            "Epoch [61/100], Loss: 0.0029, Val: 0.0677\n",
            "Epoch [62/100], Loss: 0.0031, Val: 0.0637\n",
            "Epoch [63/100], Loss: 0.0030, Val: 0.0647\n",
            "Epoch [64/100], Loss: 0.0029, Val: 0.0610\n",
            "Epoch [65/100], Loss: 0.0029, Val: 0.0595\n",
            "Epoch [66/100], Loss: 0.0029, Val: 0.0638\n",
            "Epoch [67/100], Loss: 0.0030, Val: 0.0663\n",
            "Epoch [68/100], Loss: 0.0030, Val: 0.0650\n",
            "Epoch [69/100], Loss: 0.0028, Val: 0.0622\n",
            "Epoch [70/100], Loss: 0.0028, Val: 0.0632\n",
            "Epoch [71/100], Loss: 0.0028, Val: 0.0625\n",
            "Epoch [72/100], Loss: 0.0029, Val: 0.0633\n",
            "Epoch [73/100], Loss: 0.0027, Val: 0.0593\n",
            "Epoch [74/100], Loss: 0.0030, Val: 0.0616\n",
            "Epoch [75/100], Loss: 0.0027, Val: 0.0612\n",
            "Epoch [76/100], Loss: 0.0027, Val: 0.0609\n",
            "Epoch [77/100], Loss: 0.0027, Val: 0.0589\n",
            "Epoch [78/100], Loss: 0.0026, Val: 0.0616\n",
            "Epoch [79/100], Loss: 0.0028, Val: 0.0575\n",
            "Epoch [80/100], Loss: 0.0026, Val: 0.0588\n",
            "Epoch [81/100], Loss: 0.0026, Val: 0.0676\n",
            "Epoch [82/100], Loss: 0.0026, Val: 0.0631\n",
            "Epoch [83/100], Loss: 0.0026, Val: 0.0604\n",
            "Epoch [84/100], Loss: 0.0027, Val: 0.0598\n",
            "Epoch [85/100], Loss: 0.0025, Val: 0.0594\n",
            "Epoch [86/100], Loss: 0.0025, Val: 0.0617\n",
            "Epoch [87/100], Loss: 0.0025, Val: 0.0622\n",
            "Epoch [88/100], Loss: 0.0024, Val: 0.0614\n",
            "Epoch [89/100], Loss: 0.0025, Val: 0.0577\n",
            "Epoch [90/100], Loss: 0.0026, Val: 0.0568\n",
            "Epoch [91/100], Loss: 0.0024, Val: 0.0607\n",
            "Epoch [92/100], Loss: 0.0023, Val: 0.0561\n",
            "Epoch [93/100], Loss: 0.0024, Val: 0.0571\n",
            "Epoch [94/100], Loss: 0.0023, Val: 0.0605\n",
            "Epoch [95/100], Loss: 0.0025, Val: 0.0594\n",
            "Epoch [96/100], Loss: 0.0025, Val: 0.0581\n",
            "Epoch [97/100], Loss: 0.0023, Val: 0.0565\n",
            "Epoch [98/100], Loss: 0.0023, Val: 0.0535\n",
            "Epoch [99/100], Loss: 0.0025, Val: 0.0554\n",
            "Epoch [100/100], Loss: 0.0024, Val: 0.0549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def mask_left_side(input_spectra, mask_prob=0.5):\n",
        "    \"\"\"\n",
        "    Masks the left-hand side of the input spectra tensor.\n",
        "\n",
        "    Args:\n",
        "        input_spectra (torch.Tensor): Input spectra tensor of shape (batch_size, 1023).\n",
        "        mask_prob (float): Probability of masking each element of the left-hand side.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Masked input spectra tensor.\n",
        "    \"\"\"\n",
        "    # Calculate the index to split the tensor\n",
        "    split_index = input_spectra.shape[0] // 2\n",
        "    # Mask the left half of the input tensor\n",
        "    input_spectra[:split_index] = 0\n",
        "    return input_spectra\n",
        "\n",
        "def mask_right_side(input_spectra, mask_prob=0.5):\n",
        "    \"\"\"\n",
        "    Masks the right-hand side of the input spectra tensor.\n",
        "\n",
        "    Args:\n",
        "        input_spectra (torch.Tensor): Input spectra tensor of shape (batch_size, 1023).\n",
        "        mask_prob (float): Probability of masking each element of the right-hand side.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Masked input spectra tensor.\n",
        "    \"\"\"\n",
        "    # Calculate the index to split the tensor\n",
        "    split_index = input_spectra.shape[0] // 2\n",
        "    # Mask the left half of the input tensor\n",
        "    input_spectra[split_index:] = 0\n",
        "    return input_spectra\n",
        "\n",
        "def pre_train_model_next_spectra(model, filepath=\"next_spectra_model_weights.pth\"):\n",
        "    # Assume train_loader is your DataLoader containing spectra data\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        num_pairs = 0\n",
        "\n",
        "        # Iterate over batches in the train_loader\n",
        "        for (x,y) in train_loader:\n",
        "            # Randomly choose pairs of adjacent spectra from the same index or different indexes\n",
        "            pairs = []\n",
        "            labels = []\n",
        "            for i in range(len(x)):\n",
        "                if random.random() < 0.5:\n",
        "                    # Choose two adjacent spectra from the same index\n",
        "                    if i < len(x) - 1:\n",
        "                        # Mask the right side of the spectra\n",
        "                        left = mask_left_side(x[i])\n",
        "                        right = mask_right_side(x[i])\n",
        "                        pairs.append((left, right))\n",
        "                        labels.append([0,1])\n",
        "                else:\n",
        "                    # Choose two spectra from different indexes\n",
        "                    j = random.randint(0, len(x) - 1)\n",
        "                    if j != i:\n",
        "                        left = mask_left_side(x[i])\n",
        "                        right = mask_right_side(x[j])\n",
        "                        pairs.append((left, right))\n",
        "                        labels.append([1,0])\n",
        "\n",
        "            for (input_spectra, target_spectra), label in zip(pairs, labels):\n",
        "                # Forward pass\n",
        "                input_spectra = input_spectra.to(device)\n",
        "                target_spectra = target_spectra.to(device)\n",
        "                label = torch.tensor(label).to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output = model(input_spectra.unsqueeze(0), target_spectra.unsqueeze(0))\n",
        "                label = label.float()\n",
        "\n",
        "                loss = criterion(output, label.unsqueeze(0))\n",
        "                total_loss += loss.item()\n",
        "                # Backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                num_pairs += 1\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / num_pairs\n",
        "\n",
        "        model.eval()\n",
        "        val_total_loss = 0.0\n",
        "        num_pairs = 0\n",
        "\n",
        "        for (x,y) in val_loader:# Randomly choose pairs of adjacent spectra from the same index or different indexes\n",
        "            pairs = []\n",
        "            labels = []\n",
        "\n",
        "            for i in range(len(x)):\n",
        "                if random.random() < 0.5:\n",
        "                    # Choose two adjacent spectra from the same index\n",
        "                    if i < len(x) - 1:\n",
        "                        # Mask the right side of the spectra\n",
        "                        left = mask_left_side(x[i])\n",
        "                        right = mask_right_side(x[i])\n",
        "                        pairs.append((left, right))\n",
        "                        labels.append([0,1])\n",
        "                else:\n",
        "                    # Choose two spectra from different indexes\n",
        "                    j = random.randint(0, len(x) - 1)\n",
        "                    if j != i:\n",
        "                        left = mask_left_side(x[i])\n",
        "                        right = mask_right_side(x[j])\n",
        "                        pairs.append((left, right))\n",
        "                        labels.append([1,0])\n",
        "\n",
        "            for (input_spectra, target_spectra), label in zip(pairs, labels):\n",
        "                # Forward pass\n",
        "                input_spectra = input_spectra.to(device)\n",
        "                target_spectra = target_spectra.to(device)\n",
        "                label = torch.tensor(label).to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output = model(input_spectra.unsqueeze(0), target_spectra.unsqueeze(0))\n",
        "                label = label.float()\n",
        "\n",
        "                loss = criterion(output, label.unsqueeze(0))\n",
        "                val_total_loss += loss.item()\n",
        "                num_pairs += 1\n",
        "\n",
        "        val_avg_loss = total_loss / num_pairs\n",
        "        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f} Validation: {val_avg_loss:.4f}\")\n",
        "\n",
        "    next_spectra_model = model\n",
        "    torch.save(next_spectra_model.state_dict(), filepath)\n",
        "    return model\n",
        "\n",
        "def transfer_learning(model, filepath='next_spectra_model_weights.pth', output_dim=2):\n",
        "    # Load the state dictionary from the checkpoint.\n",
        "    checkpoint = torch.load(filepath)\n",
        "    # Modify the 'fc.weight' and 'fc.bias' parameters\n",
        "    checkpoint['fc.weight'] = checkpoint['fc.weight'][:output_dim]  # Keep only the first 2 rows\n",
        "    checkpoint['fc.bias'] = checkpoint['fc.bias'][:output_dim] # Keep only the first 2 elements\n",
        "    # Load the modified state dictionary into the model.\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters\n",
        "input_dim = 1023  # Example: size of input sequence\n",
        "output_dim = 2  # Example: number of output classes\n",
        "num_layers = 3\n",
        "num_heads = 3\n",
        "hidden_dim = 128\n",
        "dropout = 0.2\n",
        "learning_rate = 1e-5\n",
        "num_epochs = 50\n",
        "\n",
        "if is_next_spectra:\n",
        "    # Initialize the model, criterion, and optimizer\n",
        "    model = Transformer(input_dim, output_dim, num_layers, num_heads, hidden_dim, dropout)\n",
        "\n",
        "    is_transfer_learning = False\n",
        "    # Transfer learning\n",
        "    if is_transfer_learning:\n",
        "        model = transfer_learning(model, filepath='next_spectra_model_weights.pth')\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # AdamW (Loshchilov 2017)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Specify the device (GPU or CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    print(\"[INFO] Training the network\")\n",
        "    startTime = time.time()\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    model = pre_train_model_next_spectra(model, filepath='next_spectra_model_weights.pth')\n",
        "\n",
        "    # finish measuring how long training took\n",
        "    endTime = time.time()\n",
        "    print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
      ],
      "metadata": {
        "id": "u59zTkBcbRJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf3676c-22cf-4290-f49a-df4e11e35c61"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training the network\n",
            "Epoch 1, Average Loss: 0.5125 Validation: 5.6376\n",
            "Epoch 2, Average Loss: 0.5588 Validation: 4.4703\n",
            "Epoch 3, Average Loss: 0.5809 Validation: 6.9713\n",
            "Epoch 4, Average Loss: 0.4039 Validation: 3.2315\n",
            "Epoch 5, Average Loss: 0.5253 Validation: 6.0408\n",
            "Epoch 6, Average Loss: 0.3959 Validation: 2.9036\n",
            "Epoch 7, Average Loss: 0.6890 Validation: 7.9232\n",
            "Epoch 8, Average Loss: 0.5557 Validation: 4.0748\n",
            "Epoch 9, Average Loss: 0.2694 Validation: 2.0657\n",
            "Epoch 10, Average Loss: 0.4198 Validation: 4.4077\n",
            "Epoch 11, Average Loss: 0.3764 Validation: 2.7604\n",
            "Epoch 12, Average Loss: 0.7347 Validation: 16.8971\n",
            "Epoch 13, Average Loss: 0.2636 Validation: 2.8995\n",
            "Epoch 14, Average Loss: 0.6553 Validation: 5.0240\n",
            "Epoch 15, Average Loss: 0.5015 Validation: 6.0180\n",
            "Epoch 16, Average Loss: 0.8489 Validation: 20.3745\n",
            "Epoch 17, Average Loss: 0.3349 Validation: 3.6836\n",
            "Epoch 18, Average Loss: 0.2134 Validation: 2.5602\n",
            "Epoch 19, Average Loss: 0.2039 Validation: 2.3454\n",
            "Epoch 20, Average Loss: 0.4761 Validation: 3.6503\n",
            "Epoch 21, Average Loss: 0.3883 Validation: 2.8475\n",
            "Epoch 22, Average Loss: 0.5077 Validation: 12.1856\n",
            "Epoch 23, Average Loss: 0.6678 Validation: 5.1196\n",
            "Epoch 24, Average Loss: 0.4324 Validation: 4.9721\n",
            "Epoch 25, Average Loss: 0.4532 Validation: 3.4747\n",
            "Epoch 26, Average Loss: 0.7148 Validation: 5.4800\n",
            "Epoch 27, Average Loss: 0.5920 Validation: 6.8084\n",
            "Epoch 28, Average Loss: 0.4151 Validation: 9.5472\n",
            "Epoch 29, Average Loss: 0.5298 Validation: 5.8279\n",
            "Epoch 30, Average Loss: 0.3911 Validation: 4.4980\n",
            "Epoch 31, Average Loss: 0.3881 Validation: 2.8463\n",
            "Epoch 32, Average Loss: 0.6392 Validation: 7.0307\n",
            "Epoch 33, Average Loss: 0.4173 Validation: 3.0604\n",
            "Epoch 34, Average Loss: 0.4026 Validation: 2.9525\n",
            "Epoch 35, Average Loss: 0.5063 Validation: 11.6445\n",
            "Epoch 36, Average Loss: 0.4951 Validation: 5.9412\n",
            "Epoch 37, Average Loss: 0.4086 Validation: 4.6985\n",
            "Epoch 38, Average Loss: 0.4512 Validation: 4.9630\n",
            "Epoch 39, Average Loss: 0.6810 Validation: 8.1725\n",
            "Epoch 40, Average Loss: 0.1718 Validation: 1.9761\n",
            "Epoch 41, Average Loss: 0.2002 Validation: 2.2027\n",
            "Epoch 42, Average Loss: 0.7037 Validation: 8.0926\n",
            "Epoch 43, Average Loss: 0.5248 Validation: 3.6736\n",
            "Epoch 44, Average Loss: 0.4014 Validation: 3.0776\n",
            "Epoch 45, Average Loss: 0.6982 Validation: 8.0288\n",
            "Epoch 46, Average Loss: 0.6076 Validation: 6.9875\n",
            "Epoch 47, Average Loss: 0.5642 Validation: 6.4880\n",
            "Epoch 48, Average Loss: 0.3174 Validation: 3.6496\n",
            "Epoch 49, Average Loss: 0.7441 Validation: 8.5570\n",
            "Epoch 50, Average Loss: 0.5097 Validation: 11.7241\n",
            "[INFO] total time taken to train the model: 37.70s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, inputs, src_mask=None, tgt_mask=None)  # Assuming no masking is needed for now\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == targets.argmax(1)).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs, inputs, src_mask=None, tgt_mask=None)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == targets.argmax(1)).sum().item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs, device):\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_accuracy = train(model, train_dataloader, criterion, optimizer, device)\n",
        "        val_loss, val_accuracy = evaluate(model, val_dataloader, criterion, device)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f'[INFO] Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
        "\n",
        "        # Early stopping (Morgan 1989)\n",
        "        # if train_accuracy == 1 and val_accuracy == 1:\n",
        "        #     break\n",
        "\n",
        "    # plot the training loss and accuracy\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label=\"train_loss\")\n",
        "    plt.plot(val_losses, label=\"val_loss\")\n",
        "    plt.plot(train_accuracies, label=\"train_acc\")\n",
        "    plt.plot(val_accuracies, label=\"val_acc\")\n",
        "    plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss/Accuracy\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.legend(bbox_to_anchor=(1.0, 1.0))\n",
        "    plt.savefig(\"model_accuracy.png\")\n",
        "    plt.show()\n",
        "\n",
        "    return train_losses, train_accuracies, val_losses, val_accuracies\n",
        "\n",
        "def transfer_learning(model, filepath='next_spectra_model_weights.pth', output_dim=6):\n",
        "    # Load the state dictionary from the checkpoint.\n",
        "    checkpoint = torch.load(filepath)\n",
        "    # Modify the 'fc.weight' and 'fc.bias' parameters\n",
        "    # Set `fc.weight` to be an array of zeroes\n",
        "    checkpoint['fc.weight'] = torch.zeros(output_dim, checkpoint['fc.weight'].shape[1])\n",
        "    # Set `fc.bias` to be an array of zeros.\n",
        "    checkpoint['fc.bias'] = torch.zeros(output_dim)\n",
        "    # Load the modified state dictionary into the model.\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters\n",
        "input_dim = 1023  # Example: size of input sequence\n",
        "output_dim = 6  # Example: number of output classes\n",
        "num_layers = 3\n",
        "num_heads = 3\n",
        "hidden_dim = 128\n",
        "dropout = 0.2\n",
        "learning_rate = 1e-5\n",
        "num_epochs = 200\n",
        "\n",
        "# Initialize the model, criterion, and optimizer\n",
        "model = Transformer(input_dim, output_dim, num_layers, num_heads, hidden_dim, dropout)\n",
        "\n",
        "is_transfer_learning = True\n",
        "# Transfer learning\n",
        "if is_transfer_learning:\n",
        "    model = transfer_learning(model, filepath='next_spectra_model_weights.pth')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# AdamW (Loshchilov 2017)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Specify the device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"[INFO] Training the network\")\n",
        "startTime = time.time()\n",
        "\n",
        "# Train the model\n",
        "train_losses, train_accuracies, val_losses, val_accuracies = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
        "\n",
        "# finish measuring how long training took\n",
        "endTime = time.time()\n",
        "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xhhI4d1jW5d3",
        "outputId": "60cef22a-41c5-4891-a56e-ba024eb113a7"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training the network\n",
            "[INFO] Epoch 1/200, Train Loss: 1.7918, Train Acc: 0.2083, Val Loss: 1.7899, Val Acc: 0.3333\n",
            "[INFO] Epoch 2/200, Train Loss: 1.7890, Train Acc: 0.8333, Val Loss: 1.7880, Val Acc: 0.3333\n",
            "[INFO] Epoch 3/200, Train Loss: 1.7859, Train Acc: 0.7917, Val Loss: 1.7859, Val Acc: 0.3333\n",
            "[INFO] Epoch 4/200, Train Loss: 1.7825, Train Acc: 0.7917, Val Loss: 1.7835, Val Acc: 0.3333\n",
            "[INFO] Epoch 5/200, Train Loss: 1.7787, Train Acc: 0.7917, Val Loss: 1.7806, Val Acc: 0.3333\n",
            "[INFO] Epoch 6/200, Train Loss: 1.7744, Train Acc: 0.7917, Val Loss: 1.7773, Val Acc: 0.3333\n",
            "[INFO] Epoch 7/200, Train Loss: 1.7697, Train Acc: 0.7917, Val Loss: 1.7735, Val Acc: 0.3333\n",
            "[INFO] Epoch 8/200, Train Loss: 1.7643, Train Acc: 0.7500, Val Loss: 1.7693, Val Acc: 0.3333\n",
            "[INFO] Epoch 9/200, Train Loss: 1.7591, Train Acc: 0.7500, Val Loss: 1.7645, Val Acc: 0.3333\n",
            "[INFO] Epoch 10/200, Train Loss: 1.7524, Train Acc: 0.7917, Val Loss: 1.7592, Val Acc: 0.3333\n",
            "[INFO] Epoch 11/200, Train Loss: 1.7446, Train Acc: 0.7500, Val Loss: 1.7532, Val Acc: 0.3333\n",
            "[INFO] Epoch 12/200, Train Loss: 1.7380, Train Acc: 0.7917, Val Loss: 1.7466, Val Acc: 0.3333\n",
            "[INFO] Epoch 13/200, Train Loss: 1.7303, Train Acc: 0.7500, Val Loss: 1.7393, Val Acc: 0.3333\n",
            "[INFO] Epoch 14/200, Train Loss: 1.7205, Train Acc: 0.7917, Val Loss: 1.7313, Val Acc: 0.3333\n",
            "[INFO] Epoch 15/200, Train Loss: 1.7108, Train Acc: 0.7917, Val Loss: 1.7226, Val Acc: 0.3333\n",
            "[INFO] Epoch 16/200, Train Loss: 1.6992, Train Acc: 0.7917, Val Loss: 1.7132, Val Acc: 0.3333\n",
            "[INFO] Epoch 17/200, Train Loss: 1.6900, Train Acc: 0.7917, Val Loss: 1.7031, Val Acc: 0.3333\n",
            "[INFO] Epoch 18/200, Train Loss: 1.6772, Train Acc: 0.7500, Val Loss: 1.6922, Val Acc: 0.3333\n",
            "[INFO] Epoch 19/200, Train Loss: 1.6646, Train Acc: 0.7917, Val Loss: 1.6806, Val Acc: 0.3333\n",
            "[INFO] Epoch 20/200, Train Loss: 1.6499, Train Acc: 0.7917, Val Loss: 1.6682, Val Acc: 0.3333\n",
            "[INFO] Epoch 21/200, Train Loss: 1.6351, Train Acc: 0.7917, Val Loss: 1.6551, Val Acc: 0.3333\n",
            "[INFO] Epoch 22/200, Train Loss: 1.6203, Train Acc: 0.7500, Val Loss: 1.6413, Val Acc: 0.3333\n",
            "[INFO] Epoch 23/200, Train Loss: 1.6043, Train Acc: 0.7917, Val Loss: 1.6269, Val Acc: 0.3333\n",
            "[INFO] Epoch 24/200, Train Loss: 1.5879, Train Acc: 0.7917, Val Loss: 1.6119, Val Acc: 0.3333\n",
            "[INFO] Epoch 25/200, Train Loss: 1.5680, Train Acc: 0.7500, Val Loss: 1.5965, Val Acc: 0.3333\n",
            "[INFO] Epoch 26/200, Train Loss: 1.5501, Train Acc: 0.7500, Val Loss: 1.5808, Val Acc: 0.3333\n",
            "[INFO] Epoch 27/200, Train Loss: 1.5280, Train Acc: 0.7083, Val Loss: 1.5647, Val Acc: 0.3333\n",
            "[INFO] Epoch 28/200, Train Loss: 1.5094, Train Acc: 0.6250, Val Loss: 1.5486, Val Acc: 0.3333\n",
            "[INFO] Epoch 29/200, Train Loss: 1.4841, Train Acc: 0.7083, Val Loss: 1.5324, Val Acc: 0.3333\n",
            "[INFO] Epoch 30/200, Train Loss: 1.4648, Train Acc: 0.6667, Val Loss: 1.5164, Val Acc: 0.3333\n",
            "[INFO] Epoch 31/200, Train Loss: 1.4406, Train Acc: 0.6667, Val Loss: 1.5010, Val Acc: 0.3333\n",
            "[INFO] Epoch 32/200, Train Loss: 1.4206, Train Acc: 0.6667, Val Loss: 1.4862, Val Acc: 0.3333\n",
            "[INFO] Epoch 33/200, Train Loss: 1.3991, Train Acc: 0.6250, Val Loss: 1.4725, Val Acc: 0.3333\n",
            "[INFO] Epoch 34/200, Train Loss: 1.3762, Train Acc: 0.6667, Val Loss: 1.4598, Val Acc: 0.3333\n",
            "[INFO] Epoch 35/200, Train Loss: 1.3559, Train Acc: 0.6667, Val Loss: 1.4485, Val Acc: 0.3333\n",
            "[INFO] Epoch 36/200, Train Loss: 1.3307, Train Acc: 0.6667, Val Loss: 1.4387, Val Acc: 0.3333\n",
            "[INFO] Epoch 37/200, Train Loss: 1.3087, Train Acc: 0.6667, Val Loss: 1.4309, Val Acc: 0.3333\n",
            "[INFO] Epoch 38/200, Train Loss: 1.2882, Train Acc: 0.6667, Val Loss: 1.4249, Val Acc: 0.0000\n",
            "[INFO] Epoch 39/200, Train Loss: 1.2659, Train Acc: 0.6667, Val Loss: 1.4212, Val Acc: 0.0000\n",
            "[INFO] Epoch 40/200, Train Loss: 1.2526, Train Acc: 0.6667, Val Loss: 1.4197, Val Acc: 0.0000\n",
            "[INFO] Epoch 41/200, Train Loss: 1.2291, Train Acc: 0.6250, Val Loss: 1.4203, Val Acc: 0.0000\n",
            "[INFO] Epoch 42/200, Train Loss: 1.2117, Train Acc: 0.7083, Val Loss: 1.4229, Val Acc: 0.0000\n",
            "[INFO] Epoch 43/200, Train Loss: 1.1976, Train Acc: 0.7083, Val Loss: 1.4275, Val Acc: 0.0000\n",
            "[INFO] Epoch 44/200, Train Loss: 1.1814, Train Acc: 0.6667, Val Loss: 1.4336, Val Acc: 0.0000\n",
            "[INFO] Epoch 45/200, Train Loss: 1.1659, Train Acc: 0.6250, Val Loss: 1.4409, Val Acc: 0.0000\n",
            "[INFO] Epoch 46/200, Train Loss: 1.1514, Train Acc: 0.6667, Val Loss: 1.4488, Val Acc: 0.0000\n",
            "[INFO] Epoch 47/200, Train Loss: 1.1311, Train Acc: 0.6667, Val Loss: 1.4573, Val Acc: 0.0000\n",
            "[INFO] Epoch 48/200, Train Loss: 1.1186, Train Acc: 0.7083, Val Loss: 1.4656, Val Acc: 0.3333\n",
            "[INFO] Epoch 49/200, Train Loss: 1.1031, Train Acc: 0.6667, Val Loss: 1.4734, Val Acc: 0.3333\n",
            "[INFO] Epoch 50/200, Train Loss: 1.0925, Train Acc: 0.7083, Val Loss: 1.4803, Val Acc: 0.3333\n",
            "[INFO] Epoch 51/200, Train Loss: 1.0754, Train Acc: 0.7500, Val Loss: 1.4858, Val Acc: 0.3333\n",
            "[INFO] Epoch 52/200, Train Loss: 1.0625, Train Acc: 0.7083, Val Loss: 1.4899, Val Acc: 0.3333\n",
            "[INFO] Epoch 53/200, Train Loss: 1.0480, Train Acc: 0.7500, Val Loss: 1.4924, Val Acc: 0.3333\n",
            "[INFO] Epoch 54/200, Train Loss: 1.0403, Train Acc: 0.7500, Val Loss: 1.4931, Val Acc: 0.3333\n",
            "[INFO] Epoch 55/200, Train Loss: 1.0245, Train Acc: 0.7083, Val Loss: 1.4916, Val Acc: 0.3333\n",
            "[INFO] Epoch 56/200, Train Loss: 1.0137, Train Acc: 0.7083, Val Loss: 1.4879, Val Acc: 0.3333\n",
            "[INFO] Epoch 57/200, Train Loss: 1.0018, Train Acc: 0.7083, Val Loss: 1.4822, Val Acc: 0.3333\n",
            "[INFO] Epoch 58/200, Train Loss: 0.9880, Train Acc: 0.7083, Val Loss: 1.4750, Val Acc: 0.3333\n",
            "[INFO] Epoch 59/200, Train Loss: 0.9765, Train Acc: 0.7083, Val Loss: 1.4669, Val Acc: 0.3333\n",
            "[INFO] Epoch 60/200, Train Loss: 0.9682, Train Acc: 0.7500, Val Loss: 1.4585, Val Acc: 0.3333\n",
            "[INFO] Epoch 61/200, Train Loss: 0.9596, Train Acc: 0.7500, Val Loss: 1.4494, Val Acc: 0.3333\n",
            "[INFO] Epoch 62/200, Train Loss: 0.9451, Train Acc: 0.7500, Val Loss: 1.4403, Val Acc: 0.3333\n",
            "[INFO] Epoch 63/200, Train Loss: 0.9366, Train Acc: 0.7500, Val Loss: 1.4320, Val Acc: 0.0000\n",
            "[INFO] Epoch 64/200, Train Loss: 0.9260, Train Acc: 0.7500, Val Loss: 1.4239, Val Acc: 0.0000\n",
            "[INFO] Epoch 65/200, Train Loss: 0.9167, Train Acc: 0.7500, Val Loss: 1.4173, Val Acc: 0.0000\n",
            "[INFO] Epoch 66/200, Train Loss: 0.8998, Train Acc: 0.7500, Val Loss: 1.4123, Val Acc: 0.0000\n",
            "[INFO] Epoch 67/200, Train Loss: 0.8877, Train Acc: 0.7500, Val Loss: 1.4090, Val Acc: 0.0000\n",
            "[INFO] Epoch 68/200, Train Loss: 0.8738, Train Acc: 0.7500, Val Loss: 1.4074, Val Acc: 0.0000\n",
            "[INFO] Epoch 69/200, Train Loss: 0.8631, Train Acc: 0.7500, Val Loss: 1.4078, Val Acc: 0.0000\n",
            "[INFO] Epoch 70/200, Train Loss: 0.8506, Train Acc: 0.7500, Val Loss: 1.4105, Val Acc: 0.0000\n",
            "[INFO] Epoch 71/200, Train Loss: 0.8363, Train Acc: 0.7917, Val Loss: 1.4156, Val Acc: 0.0000\n",
            "[INFO] Epoch 72/200, Train Loss: 0.8220, Train Acc: 0.7500, Val Loss: 1.4219, Val Acc: 0.0000\n",
            "[INFO] Epoch 73/200, Train Loss: 0.8144, Train Acc: 0.7917, Val Loss: 1.4299, Val Acc: 0.0000\n",
            "[INFO] Epoch 74/200, Train Loss: 0.7964, Train Acc: 0.7917, Val Loss: 1.4393, Val Acc: 0.0000\n",
            "[INFO] Epoch 75/200, Train Loss: 0.7796, Train Acc: 0.8333, Val Loss: 1.4501, Val Acc: 0.0000\n",
            "[INFO] Epoch 76/200, Train Loss: 0.7519, Train Acc: 0.8333, Val Loss: 1.4619, Val Acc: 0.0000\n",
            "[INFO] Epoch 77/200, Train Loss: 0.7439, Train Acc: 0.8333, Val Loss: 1.4739, Val Acc: 0.0000\n",
            "[INFO] Epoch 78/200, Train Loss: 0.7300, Train Acc: 0.8333, Val Loss: 1.4857, Val Acc: 0.0000\n",
            "[INFO] Epoch 79/200, Train Loss: 0.7104, Train Acc: 0.8333, Val Loss: 1.4971, Val Acc: 0.0000\n",
            "[INFO] Epoch 80/200, Train Loss: 0.6946, Train Acc: 0.8333, Val Loss: 1.5079, Val Acc: 0.0000\n",
            "[INFO] Epoch 81/200, Train Loss: 0.6769, Train Acc: 0.8333, Val Loss: 1.5171, Val Acc: 0.0000\n",
            "[INFO] Epoch 82/200, Train Loss: 0.6573, Train Acc: 0.8333, Val Loss: 1.5246, Val Acc: 0.0000\n",
            "[INFO] Epoch 83/200, Train Loss: 0.6443, Train Acc: 0.8333, Val Loss: 1.5304, Val Acc: 0.3333\n",
            "[INFO] Epoch 84/200, Train Loss: 0.6159, Train Acc: 0.8333, Val Loss: 1.5349, Val Acc: 0.3333\n",
            "[INFO] Epoch 85/200, Train Loss: 0.5958, Train Acc: 0.8333, Val Loss: 1.5369, Val Acc: 0.3333\n",
            "[INFO] Epoch 86/200, Train Loss: 0.5729, Train Acc: 0.8750, Val Loss: 1.5371, Val Acc: 0.3333\n",
            "[INFO] Epoch 87/200, Train Loss: 0.5535, Train Acc: 0.8750, Val Loss: 1.5351, Val Acc: 0.3333\n",
            "[INFO] Epoch 88/200, Train Loss: 0.5340, Train Acc: 0.8750, Val Loss: 1.5306, Val Acc: 0.3333\n",
            "[INFO] Epoch 89/200, Train Loss: 0.5216, Train Acc: 0.9167, Val Loss: 1.5239, Val Acc: 0.3333\n",
            "[INFO] Epoch 90/200, Train Loss: 0.4924, Train Acc: 0.9167, Val Loss: 1.5149, Val Acc: 0.3333\n",
            "[INFO] Epoch 91/200, Train Loss: 0.4764, Train Acc: 0.9167, Val Loss: 1.5035, Val Acc: 0.3333\n",
            "[INFO] Epoch 92/200, Train Loss: 0.4590, Train Acc: 0.9167, Val Loss: 1.4908, Val Acc: 0.3333\n",
            "[INFO] Epoch 93/200, Train Loss: 0.4433, Train Acc: 0.9167, Val Loss: 1.4751, Val Acc: 0.3333\n",
            "[INFO] Epoch 94/200, Train Loss: 0.4193, Train Acc: 0.9167, Val Loss: 1.4573, Val Acc: 0.3333\n",
            "[INFO] Epoch 95/200, Train Loss: 0.3992, Train Acc: 0.9167, Val Loss: 1.4408, Val Acc: 0.3333\n",
            "[INFO] Epoch 96/200, Train Loss: 0.3856, Train Acc: 0.9583, Val Loss: 1.4243, Val Acc: 0.3333\n",
            "[INFO] Epoch 97/200, Train Loss: 0.3715, Train Acc: 0.9167, Val Loss: 1.4084, Val Acc: 0.3333\n",
            "[INFO] Epoch 98/200, Train Loss: 0.3449, Train Acc: 0.9167, Val Loss: 1.3938, Val Acc: 0.3333\n",
            "[INFO] Epoch 99/200, Train Loss: 0.3321, Train Acc: 0.9167, Val Loss: 1.3817, Val Acc: 0.3333\n",
            "[INFO] Epoch 100/200, Train Loss: 0.3138, Train Acc: 0.9583, Val Loss: 1.3717, Val Acc: 0.3333\n",
            "[INFO] Epoch 101/200, Train Loss: 0.2977, Train Acc: 0.9583, Val Loss: 1.3658, Val Acc: 0.3333\n",
            "[INFO] Epoch 102/200, Train Loss: 0.2869, Train Acc: 0.9583, Val Loss: 1.3652, Val Acc: 0.3333\n",
            "[INFO] Epoch 103/200, Train Loss: 0.2635, Train Acc: 0.9583, Val Loss: 1.3730, Val Acc: 0.3333\n",
            "[INFO] Epoch 104/200, Train Loss: 0.2447, Train Acc: 1.0000, Val Loss: 1.3838, Val Acc: 0.3333\n",
            "[INFO] Epoch 105/200, Train Loss: 0.2389, Train Acc: 0.9583, Val Loss: 1.4018, Val Acc: 0.3333\n",
            "[INFO] Epoch 106/200, Train Loss: 0.2213, Train Acc: 1.0000, Val Loss: 1.4247, Val Acc: 0.3333\n",
            "[INFO] Epoch 107/200, Train Loss: 0.2046, Train Acc: 1.0000, Val Loss: 1.4529, Val Acc: 0.3333\n",
            "[INFO] Epoch 108/200, Train Loss: 0.2033, Train Acc: 1.0000, Val Loss: 1.4783, Val Acc: 0.3333\n",
            "[INFO] Epoch 109/200, Train Loss: 0.1746, Train Acc: 1.0000, Val Loss: 1.5038, Val Acc: 0.3333\n",
            "[INFO] Epoch 110/200, Train Loss: 0.1664, Train Acc: 1.0000, Val Loss: 1.5296, Val Acc: 0.3333\n",
            "[INFO] Epoch 111/200, Train Loss: 0.1629, Train Acc: 1.0000, Val Loss: 1.5602, Val Acc: 0.3333\n",
            "[INFO] Epoch 112/200, Train Loss: 0.1406, Train Acc: 1.0000, Val Loss: 1.5978, Val Acc: 0.3333\n",
            "[INFO] Epoch 113/200, Train Loss: 0.1279, Train Acc: 1.0000, Val Loss: 1.6415, Val Acc: 0.3333\n",
            "[INFO] Epoch 114/200, Train Loss: 0.1277, Train Acc: 1.0000, Val Loss: 1.6927, Val Acc: 0.3333\n",
            "[INFO] Epoch 115/200, Train Loss: 0.1143, Train Acc: 1.0000, Val Loss: 1.7452, Val Acc: 0.3333\n",
            "[INFO] Epoch 116/200, Train Loss: 0.1116, Train Acc: 1.0000, Val Loss: 1.8060, Val Acc: 0.3333\n",
            "[INFO] Epoch 117/200, Train Loss: 0.0995, Train Acc: 1.0000, Val Loss: 1.8619, Val Acc: 0.3333\n",
            "[INFO] Epoch 118/200, Train Loss: 0.0936, Train Acc: 1.0000, Val Loss: 1.9128, Val Acc: 0.3333\n",
            "[INFO] Epoch 119/200, Train Loss: 0.0874, Train Acc: 1.0000, Val Loss: 1.9621, Val Acc: 0.3333\n",
            "[INFO] Epoch 120/200, Train Loss: 0.0833, Train Acc: 1.0000, Val Loss: 2.0137, Val Acc: 0.3333\n",
            "[INFO] Epoch 121/200, Train Loss: 0.0763, Train Acc: 1.0000, Val Loss: 2.0588, Val Acc: 0.3333\n",
            "[INFO] Epoch 122/200, Train Loss: 0.0716, Train Acc: 1.0000, Val Loss: 2.1070, Val Acc: 0.3333\n",
            "[INFO] Epoch 123/200, Train Loss: 0.0652, Train Acc: 1.0000, Val Loss: 2.1500, Val Acc: 0.3333\n",
            "[INFO] Epoch 124/200, Train Loss: 0.0645, Train Acc: 1.0000, Val Loss: 2.1873, Val Acc: 0.3333\n",
            "[INFO] Epoch 125/200, Train Loss: 0.0595, Train Acc: 1.0000, Val Loss: 2.2154, Val Acc: 0.3333\n",
            "[INFO] Epoch 126/200, Train Loss: 0.0546, Train Acc: 1.0000, Val Loss: 2.2394, Val Acc: 0.3333\n",
            "[INFO] Epoch 127/200, Train Loss: 0.0514, Train Acc: 1.0000, Val Loss: 2.2644, Val Acc: 0.3333\n",
            "[INFO] Epoch 128/200, Train Loss: 0.0463, Train Acc: 1.0000, Val Loss: 2.2904, Val Acc: 0.3333\n",
            "[INFO] Epoch 129/200, Train Loss: 0.0436, Train Acc: 1.0000, Val Loss: 2.3268, Val Acc: 0.3333\n",
            "[INFO] Epoch 130/200, Train Loss: 0.0409, Train Acc: 1.0000, Val Loss: 2.3669, Val Acc: 0.3333\n",
            "[INFO] Epoch 131/200, Train Loss: 0.0384, Train Acc: 1.0000, Val Loss: 2.4081, Val Acc: 0.3333\n",
            "[INFO] Epoch 132/200, Train Loss: 0.0342, Train Acc: 1.0000, Val Loss: 2.4507, Val Acc: 0.3333\n",
            "[INFO] Epoch 133/200, Train Loss: 0.0332, Train Acc: 1.0000, Val Loss: 2.5016, Val Acc: 0.3333\n",
            "[INFO] Epoch 134/200, Train Loss: 0.0308, Train Acc: 1.0000, Val Loss: 2.5514, Val Acc: 0.3333\n",
            "[INFO] Epoch 135/200, Train Loss: 0.0295, Train Acc: 1.0000, Val Loss: 2.5944, Val Acc: 0.3333\n",
            "[INFO] Epoch 136/200, Train Loss: 0.0294, Train Acc: 1.0000, Val Loss: 2.6278, Val Acc: 0.3333\n",
            "[INFO] Epoch 137/200, Train Loss: 0.0248, Train Acc: 1.0000, Val Loss: 2.6631, Val Acc: 0.3333\n",
            "[INFO] Epoch 138/200, Train Loss: 0.0243, Train Acc: 1.0000, Val Loss: 2.6986, Val Acc: 0.3333\n",
            "[INFO] Epoch 139/200, Train Loss: 0.0240, Train Acc: 1.0000, Val Loss: 2.7284, Val Acc: 0.3333\n",
            "[INFO] Epoch 140/200, Train Loss: 0.0215, Train Acc: 1.0000, Val Loss: 2.7587, Val Acc: 0.3333\n",
            "[INFO] Epoch 141/200, Train Loss: 0.0211, Train Acc: 1.0000, Val Loss: 2.7929, Val Acc: 0.3333\n",
            "[INFO] Epoch 142/200, Train Loss: 0.0190, Train Acc: 1.0000, Val Loss: 2.8286, Val Acc: 0.3333\n",
            "[INFO] Epoch 143/200, Train Loss: 0.0180, Train Acc: 1.0000, Val Loss: 2.8611, Val Acc: 0.3333\n",
            "[INFO] Epoch 144/200, Train Loss: 0.0176, Train Acc: 1.0000, Val Loss: 2.8877, Val Acc: 0.3333\n",
            "[INFO] Epoch 145/200, Train Loss: 0.0162, Train Acc: 1.0000, Val Loss: 2.9116, Val Acc: 0.3333\n",
            "[INFO] Epoch 146/200, Train Loss: 0.0162, Train Acc: 1.0000, Val Loss: 2.9363, Val Acc: 0.3333\n",
            "[INFO] Epoch 147/200, Train Loss: 0.0149, Train Acc: 1.0000, Val Loss: 2.9621, Val Acc: 0.3333\n",
            "[INFO] Epoch 148/200, Train Loss: 0.0147, Train Acc: 1.0000, Val Loss: 2.9856, Val Acc: 0.3333\n",
            "[INFO] Epoch 149/200, Train Loss: 0.0144, Train Acc: 1.0000, Val Loss: 3.0069, Val Acc: 0.3333\n",
            "[INFO] Epoch 150/200, Train Loss: 0.0132, Train Acc: 1.0000, Val Loss: 3.0310, Val Acc: 0.3333\n",
            "[INFO] Epoch 151/200, Train Loss: 0.0131, Train Acc: 1.0000, Val Loss: 3.0581, Val Acc: 0.3333\n",
            "[INFO] Epoch 152/200, Train Loss: 0.0129, Train Acc: 1.0000, Val Loss: 3.0817, Val Acc: 0.3333\n",
            "[INFO] Epoch 153/200, Train Loss: 0.0123, Train Acc: 1.0000, Val Loss: 3.1085, Val Acc: 0.3333\n",
            "[INFO] Epoch 154/200, Train Loss: 0.0116, Train Acc: 1.0000, Val Loss: 3.1356, Val Acc: 0.3333\n",
            "[INFO] Epoch 155/200, Train Loss: 0.0110, Train Acc: 1.0000, Val Loss: 3.1599, Val Acc: 0.3333\n",
            "[INFO] Epoch 156/200, Train Loss: 0.0107, Train Acc: 1.0000, Val Loss: 3.1836, Val Acc: 0.3333\n",
            "[INFO] Epoch 157/200, Train Loss: 0.0113, Train Acc: 1.0000, Val Loss: 3.2075, Val Acc: 0.3333\n",
            "[INFO] Epoch 158/200, Train Loss: 0.0102, Train Acc: 1.0000, Val Loss: 3.2285, Val Acc: 0.3333\n",
            "[INFO] Epoch 159/200, Train Loss: 0.0098, Train Acc: 1.0000, Val Loss: 3.2516, Val Acc: 0.3333\n",
            "[INFO] Epoch 160/200, Train Loss: 0.0095, Train Acc: 1.0000, Val Loss: 3.2751, Val Acc: 0.3333\n",
            "[INFO] Epoch 161/200, Train Loss: 0.0092, Train Acc: 1.0000, Val Loss: 3.2991, Val Acc: 0.3333\n",
            "[INFO] Epoch 162/200, Train Loss: 0.0090, Train Acc: 1.0000, Val Loss: 3.3225, Val Acc: 0.3333\n",
            "[INFO] Epoch 163/200, Train Loss: 0.0083, Train Acc: 1.0000, Val Loss: 3.3448, Val Acc: 0.3333\n",
            "[INFO] Epoch 164/200, Train Loss: 0.0083, Train Acc: 1.0000, Val Loss: 3.3663, Val Acc: 0.3333\n",
            "[INFO] Epoch 165/200, Train Loss: 0.0083, Train Acc: 1.0000, Val Loss: 3.3829, Val Acc: 0.3333\n",
            "[INFO] Epoch 166/200, Train Loss: 0.0076, Train Acc: 1.0000, Val Loss: 3.3996, Val Acc: 0.3333\n",
            "[INFO] Epoch 167/200, Train Loss: 0.0075, Train Acc: 1.0000, Val Loss: 3.4157, Val Acc: 0.3333\n",
            "[INFO] Epoch 168/200, Train Loss: 0.0075, Train Acc: 1.0000, Val Loss: 3.4303, Val Acc: 0.3333\n",
            "[INFO] Epoch 169/200, Train Loss: 0.0069, Train Acc: 1.0000, Val Loss: 3.4439, Val Acc: 0.3333\n",
            "[INFO] Epoch 170/200, Train Loss: 0.0070, Train Acc: 1.0000, Val Loss: 3.4590, Val Acc: 0.3333\n",
            "[INFO] Epoch 171/200, Train Loss: 0.0069, Train Acc: 1.0000, Val Loss: 3.4725, Val Acc: 0.3333\n",
            "[INFO] Epoch 172/200, Train Loss: 0.0064, Train Acc: 1.0000, Val Loss: 3.4863, Val Acc: 0.3333\n",
            "[INFO] Epoch 173/200, Train Loss: 0.0064, Train Acc: 1.0000, Val Loss: 3.4980, Val Acc: 0.3333\n",
            "[INFO] Epoch 174/200, Train Loss: 0.0064, Train Acc: 1.0000, Val Loss: 3.5097, Val Acc: 0.3333\n",
            "[INFO] Epoch 175/200, Train Loss: 0.0061, Train Acc: 1.0000, Val Loss: 3.5210, Val Acc: 0.3333\n",
            "[INFO] Epoch 176/200, Train Loss: 0.0058, Train Acc: 1.0000, Val Loss: 3.5299, Val Acc: 0.3333\n",
            "[INFO] Epoch 177/200, Train Loss: 0.0058, Train Acc: 1.0000, Val Loss: 3.5385, Val Acc: 0.3333\n",
            "[INFO] Epoch 178/200, Train Loss: 0.0055, Train Acc: 1.0000, Val Loss: 3.5463, Val Acc: 0.3333\n",
            "[INFO] Epoch 179/200, Train Loss: 0.0055, Train Acc: 1.0000, Val Loss: 3.5508, Val Acc: 0.3333\n",
            "[INFO] Epoch 180/200, Train Loss: 0.0055, Train Acc: 1.0000, Val Loss: 3.5557, Val Acc: 0.3333\n",
            "[INFO] Epoch 181/200, Train Loss: 0.0052, Train Acc: 1.0000, Val Loss: 3.5595, Val Acc: 0.3333\n",
            "[INFO] Epoch 182/200, Train Loss: 0.0051, Train Acc: 1.0000, Val Loss: 3.5647, Val Acc: 0.3333\n",
            "[INFO] Epoch 183/200, Train Loss: 0.0051, Train Acc: 1.0000, Val Loss: 3.5721, Val Acc: 0.3333\n",
            "[INFO] Epoch 184/200, Train Loss: 0.0051, Train Acc: 1.0000, Val Loss: 3.5803, Val Acc: 0.3333\n",
            "[INFO] Epoch 185/200, Train Loss: 0.0050, Train Acc: 1.0000, Val Loss: 3.5894, Val Acc: 0.3333\n",
            "[INFO] Epoch 186/200, Train Loss: 0.0049, Train Acc: 1.0000, Val Loss: 3.5983, Val Acc: 0.3333\n",
            "[INFO] Epoch 187/200, Train Loss: 0.0051, Train Acc: 1.0000, Val Loss: 3.6096, Val Acc: 0.3333\n",
            "[INFO] Epoch 188/200, Train Loss: 0.0047, Train Acc: 1.0000, Val Loss: 3.6199, Val Acc: 0.3333\n",
            "[INFO] Epoch 189/200, Train Loss: 0.0049, Train Acc: 1.0000, Val Loss: 3.6320, Val Acc: 0.3333\n",
            "[INFO] Epoch 190/200, Train Loss: 0.0044, Train Acc: 1.0000, Val Loss: 3.6440, Val Acc: 0.3333\n",
            "[INFO] Epoch 191/200, Train Loss: 0.0046, Train Acc: 1.0000, Val Loss: 3.6574, Val Acc: 0.3333\n",
            "[INFO] Epoch 192/200, Train Loss: 0.0043, Train Acc: 1.0000, Val Loss: 3.6717, Val Acc: 0.3333\n",
            "[INFO] Epoch 193/200, Train Loss: 0.0043, Train Acc: 1.0000, Val Loss: 3.6851, Val Acc: 0.3333\n",
            "[INFO] Epoch 194/200, Train Loss: 0.0041, Train Acc: 1.0000, Val Loss: 3.6976, Val Acc: 0.3333\n",
            "[INFO] Epoch 195/200, Train Loss: 0.0043, Train Acc: 1.0000, Val Loss: 3.7100, Val Acc: 0.3333\n",
            "[INFO] Epoch 196/200, Train Loss: 0.0040, Train Acc: 1.0000, Val Loss: 3.7209, Val Acc: 0.3333\n",
            "[INFO] Epoch 197/200, Train Loss: 0.0041, Train Acc: 1.0000, Val Loss: 3.7300, Val Acc: 0.3333\n",
            "[INFO] Epoch 198/200, Train Loss: 0.0039, Train Acc: 1.0000, Val Loss: 3.7383, Val Acc: 0.3333\n",
            "[INFO] Epoch 199/200, Train Loss: 0.0038, Train Acc: 1.0000, Val Loss: 3.7451, Val Acc: 0.3333\n",
            "[INFO] Epoch 200/200, Train Loss: 0.0035, Train Acc: 1.0000, Val Loss: 3.7517, Val Acc: 0.3333\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] total time taken to train the model: 11.64s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "# switch off autograd\n",
        "with torch.no_grad():\n",
        "    # loop over the test set\n",
        "    datasets = [(\"train\", train_loader), (\"validation\", val_loader), (\"test\", test_loader)]\n",
        "    for name, dataset in datasets:\n",
        "        for (x,y) in dataset:\n",
        "            (x,y) = (x.to(device), y.to(device))\n",
        "            y_true = y\n",
        "            pred = model(x, x, src_mask=None)\n",
        "            # print(f\"pred: {pred}, y: {y}\")\n",
        "            test_correct = (pred.argmax(1) == y.argmax(1)).sum().item()\n",
        "            print(f\"[INFO] {name} accuracy {test_correct} / {len(x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in6o5PQFXsLW",
        "outputId": "5e88e0d3-b57b-4ae0-8da2-f046e448b361"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] train accuracy 24 / 24\n",
            "[INFO] validation accuracy 1 / 3\n",
            "[INFO] test accuracy 1 / 3\n"
          ]
        }
      ]
    }
  ]
}